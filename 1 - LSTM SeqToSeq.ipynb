{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 24 00:30:34 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 26%   24C    P8     6W / 180W |      1MiB / 16278MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Example, Field, Dataset\n",
    "from torchtext.data.iterator import BucketIterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import warnings\n",
    "\n",
    "import utils\n",
    "import torch_utils\n",
    "from optim_utils import LRFinder\n",
    "from beam_utils import Node, find_best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & clean data\n",
    "\n",
    "There's about 2 millions of examples. If we use all examples, the training will take much time. So, I sampled 200 000 examples to train the model in a reasonable time.\n",
    "\n",
    "The cleaning step consists of normalizing unicode characters and removing multiple spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2,007,723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:03<00:00, 64487.08it/s]\n",
      "100%|██████████| 200000/200000 [00:02<00:00, 85348.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples after sampling: 200,000\n",
      "Example:\n",
      "\tFR => par écrit. - (EN) Les conservateurs saluent l'objectif général visé par l'amélioration des droits des passagers et de l'accès pour les personnes handicapées et par la création de règles équitables pour les utilisateurs de bus internationaux, c'est pourquoi j'ai voté en faveur du rapport.\n",
      "\tEN => in writing. - Conservatives welcome the overall aim of improving passenger rights, access for the disabled and creating a level playing field for international bus users, and for this reason voted in favour of the report.\n",
      "CPU times: user 7.27 s, sys: 929 ms, total: 8.2 s\n",
      "Wall time: 8.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_fr = utils.read_file('./data/europarl-v7.fr-en.fr')\n",
    "data_en = utils.read_file('./data/europarl-v7.fr-en.en')\n",
    "\n",
    "assert len(data_fr) == len(data_en)\n",
    "print(f'Number of examples: {len(data_fr):,}')\n",
    "\n",
    "indexes = np.random.choice(range(len(data_fr)), size=200_000, replace=False)\n",
    "\n",
    "pairs = [*zip(\n",
    "    utils.clean_lines([data_fr[index] for index in indexes]),\n",
    "    utils.clean_lines([data_en[index] for index in indexes])\n",
    ")]\n",
    "pairs = [*map(lambda x: {'fr': x[0], 'en': x[1]}, pairs)]\n",
    "print(f'Number of examples after sampling: {len(pairs):,}')\n",
    "print(f'Example:\\n\\tFR => {pairs[0][\"fr\"]}\\n\\tEN => {pairs[0][\"en\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_fr = [*map(lambda pair: len(pair['fr'].split()), pairs)]\n",
    "len_en = [*map(lambda pair: len(pair['en'].split()), pairs)]\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].hist([*filter(lambda x: x < 50, len_fr)])\n",
    "axes[0].grid(True)\n",
    "axes[1].hist([*filter(lambda x: x < 50, len_en)])\n",
    "axes[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples after filtering: 89,752\n",
      "CPU times: user 596 ms, sys: 0 ns, total: 596 ms\n",
      "Wall time: 596 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MIN_LENGTH, MAX_LENGTH = 10, 25\n",
    "pairs = [*filter(lambda pair: MIN_LENGTH <= len(pair['fr'].split()) <= MAX_LENGTH and MIN_LENGTH <= len(pair['fr'].split()) <= MAX_LENGTH, pairs)]\n",
    "print(f'Number of examples after filtering: {len(pairs):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build *TorchText Fields* that will handle processing text. It consists of:\n",
    "\n",
    "- Tokenizing using Spacy language models (french and english)\n",
    "- Add special tokens (init token, pad token, unknown token, end token)\n",
    "- Numericalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89752/89752 [00:29<00:00, 3074.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 89,752\n",
      "CPU times: user 46.4 s, sys: 531 ms, total: 46.9 s\n",
      "Wall time: 46.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FR = Field(lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='fr',\n",
    "           include_lengths=True)\n",
    "EN = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='en',\n",
    "           include_lengths=True)\n",
    "\n",
    "examples = [Example.fromdict(\n",
    "    data=pair,\n",
    "    fields={\n",
    "        'fr': ('src', FR),\n",
    "        'en': ('dest', EN)\n",
    "    }\n",
    ") for pair in tqdm.tqdm(pairs)]\n",
    "print(f'Number of examples: {len(examples):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split data into train, validation and test sets with 90% for train set and 5% for validation and test sets each. 5% is about 5000 examples which enough to explains variance in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 80,777\n",
      "valid set size: 4,487\n",
      "test set size: 4,488\n",
      "{'src': ['par', 'ailleurs', ',', 'de', 'nombreux', 'experts', 'militaires', 'soulignent', 'le', 'danger', 'd’', 'une', 'intervention', 'en', 'l’', 'absence', 'd’', 'un', 'mandat', 'précis', 'et', 'sans', 'équivoque', '.'], 'dest': ['furthermore', ',', 'many', 'military', 'experts', 'point', 'to', 'the', 'dangers', 'of', 'intervening', 'without', 'a', 'mandate', 'that', 'has', 'been', 'accurately', 'and', 'properly', 'drawn', 'up', '.']}\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(examples, fields={'src': FR, 'dest': EN})\n",
    "train_data, valid_data, test_data = data.split(split_ratio=[0.9, 0.05, 0.05])\n",
    "print(f'train set size: {len(train_data.examples):,}')\n",
    "print(f'valid set size: {len(valid_data.examples):,}')\n",
    "print(f'test set size: {len(test_data.examples):,}')\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also built a vocabulary for each language in other to map each unique token with an index (numeric). We only allow tokens that appears 5 times or more. Tokens that appear less than 5 times are converted into unknown token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of FR vocabulary: 12,037\n",
      "Length of EN vocabulary: 9,417\n",
      "CPU times: user 1.28 s, sys: 4.06 ms, total: 1.29 s\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MIN_COUNT = 5\n",
    "FR.build_vocab(train_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=['<unk>', '<pad>'])\n",
    "EN.build_vocab(train_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "print(f'Length of FR vocabulary: {len(FR.vocab):,}')\n",
    "print(f'Length of EN vocabulary: {len(EN.vocab):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "<!-- \n",
    "<img src=\"./img/seq2seq-details.svg\" alt=\"sequence-to-sequence details\" />\n",
    "\n",
    "<img src=\"./img/seq2seq.png\" alt=\"seq2seq\" /> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder model\n",
    "\n",
    "<!-- <img src=\"./img/lstm_3.svg\" alt=\"LSTM cell\" />\n",
    "\n",
    "$$\n",
    "\\begin{split}\\begin{aligned}\n",
    "\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n",
    "\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\\n",
    "\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o), \\\\\n",
    "\\tilde{\\mathbf{C}}_t &= \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c), \\\\\n",
    "\\mathbf{C}_t &= \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t\n",
    "\\end{aligned}\\end{split}\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        vocab_size,\n",
    "        hidden_size,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "        recurrent_dropout\n",
    "    ):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            bidirectional=True,\n",
    "                            dropout=(recurrent_dropout if n_layers > 1 else 0))\n",
    "        \n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "    def fine_tuning_embeddings(self, fine_tune=True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "    \n",
    "    def forward(self, input_sequences, sequence_lengths):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_sequences: Tensor[seq_len, batch_size]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            \n",
    "        :return\n",
    "            outputs: Tensor[seq_len, batch_size, 2 * hidden_size]\n",
    "            hn: Tensor[n_layers * 2, batch_size, hidden_size]\n",
    "            cn: Tensor[n_layers * 2, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_sequences)\n",
    "        embedded = F.dropout(embedded, p=self.dropout)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, sequence_lengths)\n",
    "        outputs, (hn, cn) = self.lstm(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        return outputs, hn, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.11 s, sys: 82.4 ms, total: 6.2 s\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def test_encoder():\n",
    "    batch_size = 128\n",
    "    train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                              batch_size=batch_size,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True)\n",
    "    encoder = EncoderLSTM(\n",
    "        embedding_size=300,\n",
    "        vocab_size=len(FR.vocab),\n",
    "        hidden_size=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.35,\n",
    "        recurrent_dropout=0.25\n",
    "    )\n",
    "    for data in train_iterator:\n",
    "        outputs, hn, cn = encoder(\n",
    "            input_sequences=data.src[0],\n",
    "            sequence_lengths=data.src[1]\n",
    "        )\n",
    "        seq_len = data.src[0].size(0)\n",
    "        assert outputs.size() == torch.Size([seq_len, batch_size, 2 * 256]), outputs.size()\n",
    "        assert hn.size() == torch.Size([4 * 2, batch_size, 256]), hn.size()\n",
    "        assert cn.size() == torch.Size([4 * 2, batch_size, 256]), cn.size()\n",
    "        break\n",
    "    \n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        vocab_size,\n",
    "        hidden_size,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "        recurrent_dropout\n",
    "    ):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=(recurrent_dropout if n_layers > 1 else 0))\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "    def fine_tuning_embeddings(self, fine_tune=True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "        \n",
    "    def forward(self, input_word_index, h_state, c_state):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_word_index: Tensor[batch_size,]\n",
    "            h_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            c_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            \n",
    "        :return\n",
    "            logit: Tensor[batch_size, vocab_size]\n",
    "            h_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            c_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_word_index.unsqueeze(0))\n",
    "        embedded = F.dropout(embedded, p=self.dropout)\n",
    "        outputs, (h_state, c_state) = self.lstm(embedded, (h_state, c_state))\n",
    "        logit = self.fc(outputs).squeeze(0)\n",
    "        return logit, h_state, c_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 490 ms, sys: 0 ns, total: 490 ms\n",
      "Wall time: 246 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def test_decoder():\n",
    "    batch_size = 128\n",
    "    train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                              batch_size=batch_size,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True)\n",
    "    decoder = DecoderLSTM(\n",
    "        embedding_size=300,\n",
    "        vocab_size=len(EN.vocab),\n",
    "        hidden_size=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.25,\n",
    "        recurrent_dropout=0.25\n",
    "    )\n",
    "    for data in train_iterator:\n",
    "        logit, h_state, c_state = decoder(\n",
    "            input_word_index=data.dest[0][0],\n",
    "            h_state=torch.rand(4, batch_size, 256),\n",
    "            c_state=torch.rand(4, batch_size, 256)\n",
    "        )\n",
    "        assert logit.size() == torch.Size([batch_size, len(EN.vocab)]), logit.size()\n",
    "        assert h_state.size() == torch.Size([4, batch_size, 256]), h_state.size()\n",
    "        assert c_state.size() == torch.Size([4, batch_size, 256]), c_state.size()\n",
    "        break\n",
    "        \n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToSeqLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            'Encoder and Decoder must have the same number of reccurent layers'\n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            'Encoder and Decoder must have the same number of reccurrent hidden units'\n",
    "        \n",
    "        super(SeqToSeqLSTM, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.init_h0 = nn.Linear(decoder.n_layers * 2, decoder.n_layers) \n",
    "        self.init_c0 = nn.Linear(decoder.n_layers * 2, decoder.n_layers)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src_sequences, src_lengths, dest_sequences, dest_lengths, tf_ratio):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            src_sequences: Tensor[seq_len, batch_size]\n",
    "            src_lengths: Tensor[batch_size,]\n",
    "            dest_sequences: Tensor[seq_len, batch_size]\n",
    "            dest_lengths: Tensor[batch_size,]\n",
    "            tf_ratio: float\n",
    "            \n",
    "        :return\n",
    "            logits: Tensor[max(decode_lengths), batch_size, vocab_size]\n",
    "            sorted_dest_sequences: Tensor[seq_len, batch_size]\n",
    "            sorted_decode_lengths: Tensor[batch_size,]\n",
    "            sorted_indices: Tensor[batch_size,]\n",
    "        \"\"\"\n",
    "        # Encoding\n",
    "        _, h_state, c_state = self.encoder(\n",
    "            input_sequences=src_sequences,\n",
    "            sequence_lengths=src_lengths\n",
    "        )\n",
    "        # h_state: [n_layers * 2, batch_size, hidden_size]\n",
    "        # c_state: [n_layers * 2, batch_size, hidden_size]\n",
    "        \n",
    "        # Sort the batch (dest) by decreasing lengths\n",
    "        sorted_dest_lengths, sorted_indices = torch.sort(dest_lengths, dim=0, descending=True)\n",
    "        sorted_dest_sequences = dest_sequences[:, sorted_indices]\n",
    "        h_state = h_state[:, sorted_indices, :]\n",
    "        c_state = c_state[:, sorted_indices, :]\n",
    "        \n",
    "        # Init hidden and memory states\n",
    "        h_state = self.init_h0(h_state.permute(1, 2, 0)) # [batch_size, hidden_size, n_layers]\n",
    "        c_state = self.init_c0(c_state.permute(1, 2, 0)) # [batch_size, hidden_size, n_layers]\n",
    "        h_state = h_state.permute(2, 0, 1) # [n_layers, batch_size, hidden_size]\n",
    "        c_state = c_state.permute(2, 0, 1) # [n_layers, batch_size, hidden_size]\n",
    "        \n",
    "        # We won't decode at the <eos> position, since we've finished generating as soon as we generate <eos>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        sorted_decode_lengths = (sorted_dest_lengths - 1).tolist()\n",
    "        \n",
    "        # Decoding\n",
    "        batch_size, last = dest_sequences.size(1), None\n",
    "        logits = torch.zeros(max(sorted_decode_lengths), batch_size, self.decoder.vocab_size).to(self.device)\n",
    "        for t in range(max(sorted_decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in sorted_decode_lengths])\n",
    "            if last is not None:\n",
    "                if random.random() < tf_ratio:\n",
    "                    in_ = last[:batch_size_t]\n",
    "                else:\n",
    "                    in_ = sorted_dest_sequences[t, :batch_size_t]\n",
    "            else:\n",
    "                in_ = sorted_dest_sequences[t, :batch_size_t]\n",
    "            # in_ [batch_size,]\n",
    "            logit, h_state, c_state = self.decoder(\n",
    "                in_, \n",
    "                h_state[:, :batch_size_t, :].contiguous(),\n",
    "                c_state[:, :batch_size_t, :].contiguous()\n",
    "            )\n",
    "            # logit: [batch_size, vocab_size]\n",
    "            # h_state: [num_layers, batch_size, hidden_size]\n",
    "            # c_state: [num_layers, batch_size, hidden_size]\n",
    "            logits[t, :batch_size_t, :] = logit\n",
    "            last = torch.argmax(F.softmax(logit, dim=1), dim=1) # [batch_size,]\n",
    "        \n",
    "        return logits, sorted_dest_sequences, sorted_decode_lengths, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 224 ms, total: 14.3 s\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def test_seq2seq():\n",
    "    batch_size = 128\n",
    "    train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                              batch_size=batch_size,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True)\n",
    "    encoder = EncoderLSTM(\n",
    "        embedding_size=300,\n",
    "        vocab_size=len(FR.vocab),\n",
    "        hidden_size=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.25,\n",
    "        recurrent_dropout=0.25\n",
    "    )\n",
    "    decoder = DecoderLSTM(\n",
    "        embedding_size=300,\n",
    "        vocab_size=len(EN.vocab),\n",
    "        hidden_size=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.25,\n",
    "        recurrent_dropout=0.25\n",
    "    )\n",
    "    model = SeqToSeqLSTM(encoder, decoder, device='cpu')\n",
    "    for data in train_iterator:\n",
    "        logits, sorted_dest_sequences, sorted_decode_lengths, sorted_indices = \\\n",
    "            model(\n",
    "                src_sequences=data.src[0], \n",
    "                src_lengths=data.src[1],\n",
    "                dest_sequences=data.dest[0],\n",
    "                dest_lengths=data.dest[1],\n",
    "                tf_ratio=0.\n",
    "            )\n",
    "        assert logits.size() == torch.Size([\n",
    "            max(sorted_decode_lengths),\n",
    "            batch_size,\n",
    "            len(EN.vocab)\n",
    "        ]), logits.size()\n",
    "        assert sorted_dest_sequences.size() == torch.Size([\n",
    "            data.dest[0].shape[0],\n",
    "            batch_size\n",
    "        ]), sorted_dest_sequences.size()\n",
    "        assert len(sorted_decode_lengths) == batch_size, len(sorted_decode_lengths)\n",
    "        assert sorted_indices.size() == torch.Size([\n",
    "            batch_size,\n",
    "        ]), sorted_indices.size()\n",
    "        break\n",
    "        \n",
    "test_seq2seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, criterion, loader, epoch, grad_clip, tf_ratio, device):\n",
    "    loss_tracker, acc_tracker = utils.AverageMeter(), utils.AverageMeter()\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "    for i, data in pbar:\n",
    "        # Forward prop.\n",
    "        logits, sorted_dest_sequences, sorted_decode_lengths, sorted_indices = \\\n",
    "            model(*data.src, *data.dest, tf_ratio=tf_ratio)\n",
    "        # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "        sorted_dest_sequences = sorted_dest_sequences[1:, :]\n",
    "        # Remove paddings\n",
    "        logits = nn.utils.rnn.pack_padded_sequence(\n",
    "            logits,\n",
    "            sorted_decode_lengths\n",
    "        ).data\n",
    "        sorted_dest_sequences = nn.utils.rnn.pack_padded_sequence(\n",
    "            sorted_dest_sequences,\n",
    "            sorted_decode_lengths\n",
    "        ).data\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, sorted_dest_sequences)\n",
    "        # Back prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if grad_clip is not None:\n",
    "            torch_utils.clip_gradient(optimizer, grad_clip)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Track metrics\n",
    "        loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "        acc_tracker.update(\n",
    "            torch_utils.accuracy(logits, sorted_dest_sequences, 5),\n",
    "            sum(sorted_decode_lengths)\n",
    "        )\n",
    "        # Update progressbar description\n",
    "        pbar.set_description(f'Epoch: {epoch + 1:03d} - loss: {loss_tracker.average:.3f} - acc: {acc_tracker.average:.3f}%')\n",
    "    return loss_tracker.average, acc_tracker.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, loader, field, epoch, device):\n",
    "    references, hypotheses = [], []\n",
    "    loss_tracker, acc_tracker = utils.AverageMeter(), utils.AverageMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, data in pbar: \n",
    "            # Forward prop.\n",
    "            logits, sorted_dest_sequences, sorted_decode_lengths, sorted_indices = \\\n",
    "                model(*data.src, *data.dest, tf_ratio=0.)\n",
    "            # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "            sorted_dest_sequences = sorted_dest_sequences[1:, :]\n",
    "            # Remove paddings\n",
    "            logits_copy = logits.clone()\n",
    "            logits = nn.utils.rnn.pack_padded_sequence(\n",
    "                logits,\n",
    "                sorted_decode_lengths\n",
    "            ).data\n",
    "            sorted_dest_sequences = nn.utils.rnn.pack_padded_sequence(\n",
    "                sorted_dest_sequences,\n",
    "                sorted_decode_lengths\n",
    "            ).data\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, sorted_dest_sequences)\n",
    "            # Track metrics\n",
    "            loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "            acc_tracker.update(\n",
    "                torch_utils.accuracy(logits, sorted_dest_sequences, 5),\n",
    "                sum(sorted_decode_lengths)\n",
    "            )\n",
    "            # Update references\n",
    "            target_sequences = data.dest[0].t()[sorted_indices]\n",
    "            for j in range(target_sequences.size(0)):\n",
    "                target_sequence = target_sequences[j].tolist()\n",
    "                reference = [\n",
    "                    field.vocab.itos[indice] \n",
    "                    for indice in target_sequence \n",
    "                    if indice not in (\n",
    "                        field.vocab.stoi[field.init_token],\n",
    "                        field.vocab.stoi[field.pad_token]\n",
    "                    )\n",
    "                ]\n",
    "                references.append([reference])\n",
    "            # Update hypotheses\n",
    "            _, preds = torch.max(logits_copy, dim=2)\n",
    "            preds = preds.t().tolist()\n",
    "            for j, p in enumerate(preds):\n",
    "                hypotheses.append([\n",
    "                    field.vocab.itos[indice] \n",
    "                    for indice in preds[j][:sorted_decode_lengths[j]] # Remove padding\n",
    "                    if indice not in (\n",
    "                        field.vocab.stoi[field.init_token],\n",
    "                        field.vocab.stoi[field.pad_token]\n",
    "                    )\n",
    "                ])\n",
    "            assert len(references) == len(hypotheses)\n",
    "            # Update progressbar description\n",
    "            pbar.set_description(f'Epoch: {epoch + 1:03d} - val_loss: {loss_tracker.average:.3f} - val_acc: {acc_tracker.average:.3f}%')\n",
    "        # Calculate BLEU-4 score\n",
    "        bleu4 = bleu_score(hypotheses, references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "        # Display some examples\n",
    "        for i in np.random.choice(len(loader), size=3, replace=False):\n",
    "            src, dest = ' '.join(references[i][0]), ' '.join(hypotheses[i])\n",
    "            display(HTML(f'<span style=\"color:blue\"><b>Ground truth translation:</b> {src}</span>'))\n",
    "            display(HTML(f'<span style=\"color:red\"><b>Predicted translation:</b> {dest}</span>'))\n",
    "            print('='*100)\n",
    "    return loss_tracker.average, acc_tracker.average, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, valid_loader, field, n_epochs, grad_clip, tf_ratio, last_improv, model_name, device):\n",
    "    history, best_bleu = {\n",
    "        'acc': [],\n",
    "        'loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_loss': [],\n",
    "        'bleu4': []\n",
    "    }, 0.\n",
    "    for epoch in range(n_epochs):\n",
    "         # Stop training if no improvment since last 4 epochs\n",
    "        if last_improv == 4:\n",
    "            print('Training Finished - The model has stopped improving since last 4 epochs')\n",
    "            break\n",
    "        # Decay LR if no improvment\n",
    "        if last_improv > 0:\n",
    "            torch_utils.adjust_lr(optimizer=optimizer,\n",
    "                                  shrink_factor=0.9,\n",
    "                                  verbose=True)\n",
    "        # Train step\n",
    "        loss, acc = train_step(model=model,\n",
    "                               optimizer=optimizer,\n",
    "                               criterion=criterion,\n",
    "                               loader=train_loader,\n",
    "                               epoch=epoch,\n",
    "                               grad_clip=grad_clip, \n",
    "                               tf_ratio=tf_ratio,\n",
    "                               device=device)\n",
    "        # Validation step\n",
    "        val_loss, val_acc, bleu4 = validate(model=model,\n",
    "                                            criterion=criterion,\n",
    "                                            loader=valid_loader,\n",
    "                                            field=field,\n",
    "                                            epoch=epoch,\n",
    "                                            device=device)\n",
    "        # Update history dict\n",
    "        history['acc'].append(acc)\n",
    "        history['loss'].append(loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['bleu4'].append(bleu4)\n",
    "        # Print BLEU score\n",
    "        text = f'BLEU-4: {bleu4*100:.3f}%'\n",
    "        if bleu4 > best_bleu:\n",
    "            best_bleu, last_improv = bleu4, 0\n",
    "        else:\n",
    "            last_improv += 1\n",
    "            text += f' - Last improvement since {last_improv} epoch(s)'\n",
    "        print(text)\n",
    "        # Decrease teacher forcing rate\n",
    "        tf_ratio = torch_utils.adjust_tf(tf_ratio,\n",
    "                                         shrink_factor=0.8,\n",
    "                                         verbose=False)\n",
    "        # Save checkpoint\n",
    "        torch_utils.save_checkpoint(model=model,\n",
    "                                    optimizer=optimizer,\n",
    "                                    data_name=model_name,\n",
    "                                    epoch=epoch,\n",
    "                                    last_improv=last_improv,\n",
    "                                    bleu4=bleu4,\n",
    "                                    is_best=bleu4 >= best_bleu)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.65 s, sys: 1.33 s, total: 8.98 s\n",
      "Wall time: 8.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load word vectors\n",
    "spacy_fr = spacy.load('fr_core_news_lg') # CBOW trained word vectors\n",
    "spacy_en = spacy.load('en_core_web_lg') # GloVe trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12077/12077 [01:56<00:00, 103.72it/s]\n",
      "100%|██████████| 9442/9442 [01:38<00:00, 95.42it/s] \n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "fr_embeddings = torch_utils.load_embeddings(nlp=spacy_fr, field=FR)\n",
    "en_embeddings = torch_utils.load_embeddings(nlp=spacy_en, field=EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'seq2seq-lstm'\n",
    "N_LAYERS = 4\n",
    "HIDDEN_SIZE = 512\n",
    "EMBEDDING_SIZE = 300\n",
    "ENC_DROPOUT = 0.3\n",
    "ENC_RECURRENT_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.15\n",
    "DEC_RECURRENT_DROPOUT = 0.2\n",
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "INIT_LR = 1e-7\n",
    "GRAD_CLIP = 1.0\n",
    "TF_RATIO = 1.0\n",
    "END_LR = 10\n",
    "N_ITERS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model: 41,471,097\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderLSTM(\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    vocab_size=len(FR.vocab),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=ENC_DROPOUT,\n",
    "    recurrent_dropout=ENC_RECURRENT_DROPOUT\n",
    ")\n",
    "encoder.load_pretrained_embeddings(fr_embeddings)\n",
    "encoder.fine_tuning_embeddings(fine_tune=True)\n",
    "decoder = DecoderLSTM(\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    vocab_size=len(EN.vocab),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DEC_DROPOUT,\n",
    "    recurrent_dropout=DEC_RECURRENT_DROPOUT\n",
    ")\n",
    "decoder.load_pretrained_embeddings(en_embeddings)\n",
    "decoder.fine_tuning_embeddings(fine_tune=True)\n",
    "seq2seq = SeqToSeqLSTM(encoder=encoder, decoder=decoder, device=DEVICE)\n",
    "seq2seq.apply(torch_utils.xavier_init_weights)\n",
    "seq2seq.to(DEVICE)\n",
    "optimizer = optim.RMSprop(params=seq2seq.parameters(), lr=INIT_LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f'Number of parameters of the model: {torch_utils.count_parameters(seq2seq):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True,\n",
    "                              device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from optim_utils import LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [00:18<00:02,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate search finished. See the graph with {finder_name}.plot()\n"
     ]
    }
   ],
   "source": [
    "lr_finder = LRFinder(model=seq2seq,\n",
    "                     optimizer=optimizer,\n",
    "                     criterion=criterion,\n",
    "                     grad_clip=GRAD_CLIP)\n",
    "\n",
    "lr_finder.range_test(data_loader=train_iterator,\n",
    "                     end_lr=END_LR, n_iters=N_ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 2.29E-04\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3UAAAFBCAYAAADQeoayAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeXyU5b3///c12feQlSUEkrAjCBJAQDTgjlZ61NbaeupSxfaco/a0tbXtabU97ff0nPrr6fZoKz11aaueKtYed6nK4sIWNhVBEgiEEGCSQPZ1Zq7fHxMiSxISyMydmbyej8c8mLnva+77M8NlzJvrvq/LWGsFAAAAAAhNLqcLAAAAAACcPUIdAAAAAIQwQh0AAAAAhDBCHQAAAACEMEIdAAAAAIQwQh0AAAAAhLBIpwvoi4yMDDt27Ng+tW1qalJCQkK/jt+f9/Sl7Zna9La/p31n87mCKdD1nevx6RfOoF/QL7pDv6BfdId+Qb/oDv0icP3ibPrLYDFU+8XmzZurrbWZ3e601g76x6xZs2xfrVq1qs9tz+Y9fWl7pja97e9p39l8rmAKdH3nenz6hTPoF2duS78YfMenXziDfnHmtvSLwXf8UO4XZ9NfBouh2i8kFdse8hKXXwIAAABACCPUAQAAAEAII9QBAAAAQAgLiYlSutPR0aGKigq1traetD0lJUU7d+7s17H6856+tD1Tm97297TvbD5XMAWyvtjYWBljAnJsAAAAINSFbKirqKhQUlKSxo4de9Iv/A0NDUpKSurXsfrznr60PVOb3vb3tO9sPlcwBao+a61qamoG9QxMAAAAgJNC9vLL1tZWpaenM4IT5owxSk9PV0REhNOlAAAAAINSyIY6SQS6IYK/ZwAAAKBnAQt1xphHjTFuY8yHJ2ybYYxZb4zZZowpNsbMCdT5nfLzn/9czc3NjtZQW1ur3/zmN0E739ixY1VTUyNJmj9//lkf5/HHH1dlZeVAlQUAAAAMCYEcqXtc0lWnbPsvST+w1s6Q9P3O18FhrbR+vfT88/4/rQ3IaX7+85+rpaUlIMfuq4EIdR6P56ze99577531OQl1AAAAQP8FLNRZa9dKOnrqZknJnc9TJAXnN/hXXpFyc6XLL5duu83/Z26uf/tZampq0jXXXKPzzz9f5513nv7yl7/ol7/8pSorK3XNNddo0aJFkqSVK1dq3rx5uuCCC/SZz3xGjY2NkqTNmzfrkksu0axZs3TllVfq0KFDkqQlS5bovvvu04wZM3Teeedp48aNXee74447NGfOHM2cOVP/93//J0nasWOH5syZoxkzZmj69OkqKSnRAw88oD179mjGjBm6//77T6v93//93zVx4kRddNFFuvnmm/Xwww9LkoqKivTVr35VhYWF+sUvfqEXX3xRc+fO1cyZM3XZZZfpyJEjkqSamhpdccUVmjp1qu68807ZEwJyYmJi1/Of/vSnmj17tqZPn64HH3xQkrRv3z5NnjxZd911l6ZOnaorrrhCLS0tWrFihYqLi/WFL3xBM2bMcDwYAwAAYOjZUVmnY60+p8vot2DfU/dVST81xhyQ9LCkbwf8jK+8It14o1RRITU2SvX1/j8rKvzbzzLYvfbaaxo5cqS2b9+uDz/8UFdddZXuvfdejRw5Ui+//LJWrVql6upq/ehHP9Ibb7yhLVu2qLCwUD/72c/U0dGhe+65RytWrNDmzZt1xx136Lvf/W7XsZubm7Vt2zb95je/0R133CFJevjhh7V48WJt3LhRq1at0v3336+mpib97ne/03333adt27apuLhYOTk5+slPfqKCggJt27ZNP/3pT0+qe9OmTXruuee0fft2vfrqqyouLj5pf3t7u4qLi/X1r39dF110kdavX6+tW7fqc5/7nP7rv/wDqz/4wQ900UUXaceOHfqHf/gHlZeXn/b9rFy5UiUlJdq4caO2bdumzZs3a+3atZKkkpIS/fM//7N27Nih1NRUPffcc7rxxhtVWFioJ598Utu2bVNcXNxZ/b0AAAAAZ+uep7bqyZ3tTpfRb8Fe0uArkv7VWvucMeazkv4g6bLuGhpjlklaJkm5ublndzZrpWXLpJ5GfVpapLvvlnbs6Pehp02bpq9//ev61re+pWuvvVYLFy48rc369ev10UcfacGCBZL8gWnevHkqKSnRhx9+qMsvv1yS5PV6NWLEiK733XzzzZKkiy++WPX19aqtrdVbb72l1157rWtUrbW1VeXl5Zo3b55+/OMfq6KiQtdff73Gjx/fa93vvvuuli5dqtjYWMXGxupTn/rUSftvuummrucVFRW66aabdOjQIbW3tysvL0+StHbtWv31r3+VJF1zzTUaNmzYaedZuXKlVq5cqZkzZ0qSGhsbVVJSotzcXOXl5WnGjBmSpFmzZmnfvn291gwAAAAEWpvHq301TTovP8rpUvot2KHuVkn3dT5/VtL/9NTQWrtc0nJJKiwsPLsb4DZskOrqem9TWytXcbG0eHG/Dj1hwgRt2bJFr7zyiv7t3/5Nl156qb7//e+f1MZaq8svv1xPP/30SdvXr1+vqVOnat26dd0e+9TZHo0xstbqueee08SJE0/aN3nyZM2dO1cvv/yylixZokceeUT5+fn9+iwnOnE9uHvuuUdf+9rXdN1112n16tV66KGH+nwca62+/e1v6+677z5p+759+xQTE9P1OiIigkstAQAA4Liy6ib5rDQyMfQWCAh2xZWSLul8vlhSSUDPduiQ5DrDR3S55Dp8uN+HrqysVHx8vG655Rbdf//92rJliyQpKSlJDQ0NkqQLL7xQ7777rkpLSyX574vbvXu3xo8fr6qqqq5Q19HRoR0njBb+5S9/kSS98847SklJUUpKii699FL96le/6rp/bevWrZKkvXv3Kj8/X/fee6+WLl2q999//6QaTrVgwQK9+OKLam1tVWNjo1566aUeP2NdXZ1GjRolSXriiSe6tl988cV66qmnJEmvvvqqjh07dtp7r7zySj366KNd9xAePHhQbre71++0t7oBAACAQCo54v+9dWRC6C2nFbCROmPM05KKJGUYYyokPSjpLkm/MMZESmpV5+WVATNihOQ7w42OPp98w4f3+9AffPCB7r//frlcLkVFRem3v/2tJGnZsmW6/vrrlZOTo1WrVunxxx/XzTffrLa2NknSj370Iy1atEgrVqzQvffeq7q6Onk8Hn31q1/V1KlTJUmxsbGaOXOmOjo69Oijj0qSvvnNb+p73/uepk+fLp/Pp7y8PL300kt65pln9Kc//UlRUVEaPny4vvOd7ygtLU0LFizQeeedp6uvvvqk++pmz56t6667TtOnT1d2dramTZumlJSUbj/jQw89pM985jMaNmyYFi9erLKyMknSgw8+qJtvvllTp07V/Pnzu7089oorrtDOnTs1b948Sf4JVP785z/3uoj4bbfdpi9/+cuKi4vTunXruK8OAAAAQVPqbpTLSMMTQm+kLmChzlp7cw+7ZgXqnKeZO1dKSfFPjNKT1FT5Cgv7fegrr7xSV1555Wnb77nnHt12221KSkqSJC1evFibNm06qU1DQ4NmzJjRNXHIqW655Rb9/Oc/P2lbXFycHnnkkdPaPvDAA3rggQdO2358JK073/jGN/TQQw+publZF198sWbN8v+VrF69+qR2S5cu1dKlS097f3p6ulauXHnaZ5LUNTInSffdd5/uu+8+nerDD7uWLtQ3vvGNruc33HCDbrjhhh7rBgAAAAKl1N2o3LR4RUeE3khd6MXQ/jBGWr5c6mnEJy5OeuQRf7shZNmyZZoxY4YuuOAC3XDDDbrgggucLgkAAABwVIm7QeOykpwu46wEe6KU4FuyRFqxwj/LZW2t/x47n09KTfUHuiVLpEF0H9crr7zSNcoXKL2N4gEAAABDTYfXp7LqJl06OVtSL1f5DVLhH+okf3ArL5c2bpQqK6WRI6U5c4bcCB0AAACA0+2vaVaH12pcZqI0eMZ7+iykQ5219rTp/3tkjP8eO4Sc4zN+AgAAAIFQ6vaPzo3PTtTREAx1IXtPXWxsrGpqaviFP8xZa1VTUyOv1+t0KQAAAAhTpW5/kivITHS4krMTsiN1OTk5qqioUFVV1UnbW1tbFRsb269j9ec9fWl7pja97e9p39l8rmAKZH2xsbFqamoKyLEBAACAEnejRqXGKSEmNONRaFYtKSoqSnl5eadtX716tWbOnNmvY/XnPX1pe6Y2ve3vad/ZfK5gCnR9+/fvD9ixAQAAMLSVHGnU+OzQHKWTQvjySwAAAAA4V16f1Z6qRv8kKSGKUAcAAABgyDp4rEVtHh8jdQAAAAAQiko6J0kJ1YXHJUIdAAAAgCGspHM5g3FZjNQBAAAAQMgpOdKo7OQYpcRFOV3KWSPUAQAAABiySt0NIT1KJxHqAAAAAAxR1lqVuhs1PoTvp5MIdQAAAACGqEN1rWpq9zJSBwAAAACh6PgkKeMJdQAAAAAQekqO+JczGJ/N5ZcAAAAAEHJK3Y1KS4hWWkK006WcE0IdAAAAgCGp1N0Y8vfTSYQ6AAAAAEOQtVYl7saQv59OItQBAAAAGIKqGttU19JBqAMAAACAUFR6xD/z5bgQX6NOItQBAAAAGIJKqzqXM8hmpA4AAAAAQk7JkUYlxUYqKynG6VLOGaEOAAAAwJBT4m7Q+KxEGWOcLuWcEeoAAAAADDml7kaND4P76SRCHQAAAIAh5lhTu6ob28NijTqJUAcAAABgiDk+Scq4MJgkRSLUAQAAABhiSjqXMwiHNeokQh0AAACAIabE3aD46AiNTIlzupQBQagDAAAAMKSUuhs1LitRLlfoz3wpEeoAAAAADDElRxo1LjM8Lr2UCHUAAAAAhpCG1g4drm8Nm0lSJEIdAAAAgCGk1H18kpTwWKNOItQBAAAAGEJK3OE186VEqAMAAAAwhJS6GxUd6dLotHinSxkwhDoAAAAAQ0bJkQblZyQoIkxmvpQIdQAAAACGkNKqRo3PDp/76aQAhjpjzKPGGLcx5sNTtt9jjNlljNlhjPmvQJ0fAAAAAE7U3O5RxbGWsLqfTgrsSN3jkq46cYMxZpGkpZLOt9ZOlfRwAM8PAAAAAF32VjXJ2vCaJEUKYKiz1q6VdPSUzV+R9BNrbVtnG3egzg8AAAAAJypxN0iSxofRGnVS8O+pmyBpoTFmgzFmjTFmdpDPDwAAAGCIKjnSqEiX0Zj0BKdLGVCRDpwvTdKFkmZLesYYk2+ttac2NMYsk7RMknJzc4NaJAAAAIDwU+pu1NiMBEVFhNd8kcH+NBWS/mr9NkryScrorqG1drm1ttBaW5iZmRnUIgEAAACEn1J3Y9jdTycFP9T9TdIiSTLGTJAULak6yDUAAAAAGGLaPF7tq2kKy1AXsMsvjTFPSyqSlGGMqZD0oKRHJT3aucxBu6Rbu7v0EgAAAAAGUll1k3xWGhdma9RJAQx11tqbe9h1S6DOCQAAAADdKTnSKEkalxl+I3XhdYcgAAAAAHSj1N0ol5HyM8Nr5kuJUAcAAABgCCh1Nyo3LV6xURFOlzLgCHUAAAAAwl6Ju0HjssLvfjqJUAcAAAAgzHV4fSqrbtK4MJz5UiLUAQAAAAhz+2ua1eG1YbmcgUSoAwAAABDmSt3+mS/HZxPqAAAAACDklLobJEkFYbicgUSoAwAAABDmStyNGpUap4SYgC3T7ShCHQAAAICwVnKkMWwnSZEIdQAAAADCmNdntaeqMWwnSZEIdQAAAADC2MFjLWrz+MJ2khSJUAcAAAAgjJV0TpISrguPS4Q6AAAAAGGspHM5A+6pAwAAAIAQtGX/MY1KjVNKXJTTpQQMoQ4AAABAWGrzePVOabWKJmY6XUpAEeoAAAAAhKWNZUfV3O7V4klZTpcSUIQ6AAAAAGFp1a4qRUe6NL8gw+lSAopQBwAAACAsrfrYrXn56YqLjnC6lIAi1AEAAAAIO2XVTSqrbgr7Sy8lQh0AAACAMPTWLrckEeoAAAAAIBSt2uXWuKxEjU6Ld7qUgCPUAQAAAAgrTW0ebSirGRKjdBKhDgAAAECYeae0Wh1eG/br0x1HqAMAAAAQVlbtcispJlKzx6Y5XUpQEOoAAAAAhA1rrVZ97NbCCRmKihgacWdofEoAAAAAQ8JHh+p1pL5NiyYOjfvpJEIdAAAAgDCyqnMpg0uGyP10EqEOAAAAQBh5a5db03NSlJUU63QpQUOoAwAAABAWjja1a+uB2iF16aVEqAMAAAAQJtbsdstaDZn16Y4j1AEAAAAIC6t2VSkjMVrTRqU4XUpQEeoAAAAAhDyP16c1u6t0yYQsuVzG6XKCilAHAAAAIORtPVCrupaOIXfppUSoAwAAABAG3trlVqTLaOGEDKdLCTpCHQAAAICQt2qXW4Vjhyk5NsrpUoKOUAcAAAAgpFXWtmjX4YYht5TBcYQ6AAAAACFt1cduSUNvKYPjCHUAAAAAQtqqXW7lDIvTuKxEp0txBKEOAAAAQMhq7fDq3dIaLZ6UJWOG1lIGxwUs1BljHjXGuI0xH3az7+vGGGuMGXpT0wAAAAAYMBvKjqqlw6tFQ/TSSymwI3WPS7rq1I3GmNGSrpBUHsBzAwAAABgCVu1yKzbKpXn56U6X4piAhTpr7VpJR7vZ9d+SvinJBurcAAAAAMKftVZv7XJrfkGGYqMinC7HMUG9p84Ys1TSQWvt9mCeFwAAAED42VPVpPKjzUP60ktJigzWiYwx8ZK+I/+ll31pv0zSMknKzc0NYGUAAAAAQtHqIb6UwXHBHKkrkJQnabsxZp+kHElbjDHDu2tsrV1urS201hZmZmYGsUwAAAAAoeCtXW5NzE7SqNQ4p0txVNBCnbX2A2ttlrV2rLV2rKQKSRdYaw8HqwYAAAAA4aGhtUMby46qaBIDQIFc0uBpSeskTTTGVBhjvhSocwEAAAAYWt4pqZbHZ7V44tC+9FIK4D111tqbz7B/bKDODQAAACC8vbXLreTYSM0aM8zpUhwX1NkvAQAAAOBc+XxWq3dX6eIJmYqMINLwDQAAAAAIKTsq61XV0KZFXHopiVAHAAAAIMSs2e1fyuCSiUySIhHqAAAAAISYNburNG1UijISY5wuZVAg1AEAAAAIGU0dVlvKa3XJBEbpjiPUAQAAAAgZH9V45fVZLr08AaEOAAAAQMj4oNqrpNhIzRyd6nQpgwahDgAAAEBIsNbqw2qvLhqXwVIGJ+CbAAAAABASSt2NOtpqdTH3052EUAcAAAAgJKzZXSVJhLpTEOoAAAAAhIQ1u6s0MtFoVGqc06UMKoQ6AAAAAINec7tHG/Ye1bSMCKdLGXQIdQAAAAAGvQ17j6rd69O0jEinSxl0CHUAAAAABr01u6sUG+XShGFEmFP16RsxxhQYY2I6nxcZY+41xrAwBAAAAICgWLu7Shfmpys6wjhdyqDT15j7nCSvMWacpOWSRkt6KmBVAQAAAECn8ppm7a1u0iXMetmtvoY6n7XWI+kfJP3KWnu/pBGBKwsAAAAA/NaU+JcyINR1r6+hrsMYc7OkWyW91LktKjAlAQAAAMAn1nxcpdFpccrLSHC6lEGpr6HudknzJP3YWltmjMmT9KfAlQUAAAAAUrvHp/f2VOuSCZkyhvvputOn+UCttR9JuleSjDHDJCVZa/8zkIUBAAAAQPH+o2pu9+qSCVlOlzJo9XX2y9XGmGRjTJqkLZJ+b4z5WWBLAwAAADDUrd1dragIo3kF6U6XMmj19fLLFGttvaTrJf3RWjtX0mWBKwsAAAAA/OvTzRozTIkxLDrek76GukhjzAhJn9UnE6UAAAAAQMAcqW/VzkP1XHp5Bn0NdT+U9LqkPdbaTcaYfEklgSsLAAAAwFC3djdLGfRFXydKeVbSsye83ivphkAVBQAAAABrdlcpMylGk0ckOV3KoNbXiVJyjDHPG2PcnY/njDE5gS4OAAAAwNDk9Vm9XcJSBn3R18svH5P0gqSRnY8XO7cBAAAAwIB7v6JWdS0dXHrZB30NdZnW2sestZ7Ox+OS+HYBAAAABMSa3VUyRrpoXIbTpQx6fQ11NcaYW4wxEZ2PWyTVBLIwAAAAAEPXmt1VOj8nVcMSop0uZdDra6i7Q/7lDA5LOiTpRkm3BagmAAAAAEPYsaZ2bT9Qy6WXfdSnUGet3W+tvc5am2mtzbLWflrMfgkAAAAgAN4prZbPSpdMJNT1RV9H6rrztQGrAgAAAAA6rdldpZS4KJ2fk+p0KSHhXEId84oCAAAAGFDWWq3dXaWF4zMU4SJy9MW5hDo7YFUAAAAAgKRdhxvkbmjjfrp+iOxtpzGmQd2HNyMpLiAVAQAAABiy1uyukiRdTKjrs15DnbU2KViFAAAAAMCaj6s0aXiSspNjnS4lZJzL5ZcAAAAAMGAa2zwq3n+UWS/7iVAHAAAAYFBYt6dGHV7L/XT9RKgDAAAAMCis2e1WfHSECsekOV1KSAlYqDPGPGqMcRtjPjxh20+NMbuMMe8bY543xrDwBAAAAAD5fFZrdldpfkGGoiMZe+qPQH5bj0u66pRtf5d0nrV2uqTdkr4dwPMDAAAACBF/23ZQB4626NrpI5wuJeQELNRZa9dKOnrKtpXWWk/ny/WScgJ1fgAAAAChobHNo/94dZfOH52q684f6XQ5IcfJcc07JL3q4PkBAAAADAK/fqtUVQ1teuhTU+RyGafLCTmOhDpjzHcleSQ92UubZcaYYmNMcVVVVfCKAwAAABA0ZdVNevSdMt1wQY5m5g5zupyQFPRQZ4y5TdK1kr5grbU9tbPWLrfWFlprCzMzmdIUAAAACEc/eukjRUe69K2rJjpdSsgKaqgzxlwl6ZuSrrPWNgfz3AAAAAAGl9Ufu/XmLrfuWTxOWcmxTpcTsgK5pMHTktZJmmiMqTDGfEnSryUlSfq7MWabMeZ3gTo/AAAAgMGr3ePTD1/6SHkZCbp9QZ7T5YS0yEAd2Fp7czeb/xCo8wEAAAAIHU+8t097q5r02G2zWZfuHPHtAQAAAAiqqoY2/fLNEi2amKlFk7KcLifkEeoAAAAABNVPX9+lVo9X37t2itOlhAVCHQAAAICg2X6gVs8UV+iOBXnKz0x0upywQKgDAAAAEBQ+n9VDL+5QRmKM/mXxOKfLCRuEOgAAAABB8fzWg9paXqsHrp6kpNgop8sJG4Q6AAAAAAHX2ObRT17bpfNHp+r6maOcLiesBGxJAwAAAAA47tdvlaqqoU3L/3GWXC7jdDlhhZE6AAAAAAFVVt2kP7yzVzfOytHM3GFOlxN2CHUAAAAAAupHL32kmMgIffOqiU6XEpYIdQAAAAACZtXHbr25y617Fo9TVlKs0+WEJUIdAAAAgIA4Ut+qbz/3gfIzEnT7gjynywlbTJQCAAAAYMC1tHt15xPFamjt0GO3z1Z0JONJgUKoAwAAADCgfD6rrz+7TR9W1un3/1ioySOSnS4prBGXAQAAAAyo/35jt1754LC+c/VkXTYl2+lywh6hDgAAAMCA+b9tB/Wrt0p1U+Fo3bmQ++iCgVAHAAAAYEBs3n9M9694X3Pz0vTvnz5PxrDIeDAQ6gAAAACcs4pjzbr7T8UakRKr390yi4lRgoiJUgAAAACck8Y2j+58olhtHp/+d9lsDUuIdrqkIYVQBwAAAOCseX1W9z29VSXuRj1++2yNy0p0uqQhhzFRAAAAAGftJ6/u1Ju73HroU1O0cHym0+UMSYQ6AAAAAGflL5vK9fu3y3TrvDH6x3ljnS5nyCLUAQAAAOi3dXtq9N3nP9TC8Rn63rVTnC5nSCPUAQAAAOiXfdVN+sqTmzUmPV6//vwFiowgVjiJbx8AAABAnzW0dujOPxZLkh69bbZS4qIcrgjMfgkAAACgT3w+q3/9yzaVVTfpT3fM0Zj0BKdLghipAwAAANBHP/v7br2x063vXTNZ88dlOF0OOhHqAAAAAJzRS+9X6terSnVT4WjdOn+s0+XgBIQ6AAAAAL3aUVmnbzy7XbPGDNMPPz1VxhinS8IJCHUAAAAAelTd2KZlf9ysYfHR+u0tFygmMsLpknAKJkoBAAAA0K12j0//9Octqm5s04ovz1dWUqzTJaEbhDoAAAAA3frBizu0cd9R/eJzMzQtJ8XpctADLr8EAAAAcJo/r9+vJzeU6+5L8rV0xiiny0EvCHUAAAAATrJhb40eemGHiiZm6ptXTnK6HJwBoQ4AAABAl+oWn77y5BblpsfrF5+bqQgXM10OdoQ6AAAAAJKk5naPfrGlTR1en37/xUKlxEU5XRL6gIlSAAAAAMjns7r/2fdV0eDTo7fPUkFmotMloY8YqQMAAACGOI/Xp2+s2K6XPzikz0yI0qKJWU6XhH5gpA4AAAAYwto8Xt379Fa9vuOIvnb5BE1zVThdEvopYCN1xphHjTFuY8yHJ2xLM8b83RhT0vnnsECdHwAAAEDvmts9uvOJYr2+44i+f+0U3XvpeBnDxCihJpCXXz4u6apTtj0g6U1r7XhJb3a+BgAAABBkdS0d+uIfNurd0mr9143TdcdFeU6XhLMUsFBnrV0r6egpm5dKeqLz+ROSPh2o8wMAAADoXn2b1c3L12t7Ra1+/fkL9NnC0U6XhHMQ7Hvqsq21hzqfH5aUHeTzAwAAAEPaoboW/b+NLaptN/r9FwtVxKQoIc+x2S+ttVaS7Wm/MWaZMabYGFNcVVUVxMoAAACA8LSvukk3/nad6tqs/njHXAJdmAh2qDtijBkhSZ1/untqaK1dbq0ttNYWZmZmBq1AAAAAIBx9fLhBn3lknVo6vPrW7FjNyUtzuiQMkGCHuhck3dr5/FZJ/xfk8wMAAABDzrYDtbpp+Tq5jPTM3RdqbEqE0yVhAAVySYOnJa2TNNEYU2GM+ZKkn0i63BhTIumyztcAAAAAAmTdnhp94ffrlRwbpRVfnq9xWUlOl4QBFrCJUqy1N/ew69JAnRMAAADAJzYe8uh//r5RY9Lj9ec75yo7OdbpkhAAwZ79EgAAAECAWWv1h3fK9JvtbZo9dph+/8VCpcZHO10WAoRQBwAAAIQRr8/q31/6SI+/t0+zh0foT1+aq9go7qELZ4Q6AAAAIEy0dnh13+Tle7kAAB2qSURBVP9u1es7jujOi/I0P+EIgW4IcGydOgAAAAAD52hTuz7/+/Va+dERff/aKfq3a6fIZYzTZSEIGKkDAAAAQlx5TbNufWyjKmtb9JvPX6Crp41wuiQEEaEOAAAACGHbD9TqS09sksdn9dRdczVrDIuKDzWEOgAAACBEbXN79Mib65WRFK3Hb5+jgsxEp0uCAwh1AAAAQAh6csN+/WJLm6blpOgPt85WZlKM0yXBIYQ6AAAAIER4fVZrS6r0zKYDevXDwzo/M0JPL7tQ8dH8Wj+U8bcPAAAADHKl7kat2Fyh57dW6Eh9m4bFR+lfFo3TzKhKAh0IdQAAAMBg1Nxh9eSG/VqxuUJby2sV4TJaNDFTP7guR4snZSs60qXVqw85XSYGAUIdAAAAMEh4fVbv7anWs8UVevWDZnX4PtSE7ER9d8lkLZ05UllJsU6XiEGIUAcAAAA4rKG1Q09tKNcT7+1TZV2rkmMjtTAnUl+9bq6mjUqRYRFx9IJQBwAAADikqqFNj79Xpj+u26+GVo/mF6TrO9dM1mWTs7X+3bc1PSfV6RIRAgh1Z6m53SOPz3a/r8OqvrWj5/f2sr+5w6qhm30tnu63B0Jf/yXI2k8+f6Dr6+n4fa21xWPV2ObpU9vjR2z1WDX18T19adtdmxPLb/NYNbd7Oms4+XO1ea1a2r2nHbOn7d0513/g6+79p9Z5Ko/Pqt3j6+X9PZ3r9D3dtfVZe1I/5F8xAQChorymWcvf3qNniivU4fXp6vOG68uXFBDicFYIdWfptsc2aWPZ0Z4bvLmy9wP0tr+nfW+c4ZhOC3R953r8N14P7Hv60vZMbXrb//fX+rd9sFj5amCP//orfWp2at4zkqyVzOsvnxYGzSnv6QqvRvL5fHK98epJbY7v9z8/vt3I6/UoctXrUud2Y0xXG1fn8/b2DsW8+4ZcnccxRopwGUVFuBThMmptblbqB28rwuVSlMuctK+htlWv1byvtIRopSVEKz0xWukJMdpf79WhuhalJUQrJjKiH18mACDQ9td7dc/TW/Xy+5WKdLl0w6xRumthvvJZNBzngFB3lr44b4yumJLd7b49e/aooKCgx/f2tr+0dI/GjTt9X0/bB5rtfvCxR8d/6Q10fd0dvz+1lu7Zo3G9/J10HVOfHHTPnr0qKMjv0/H70vbUNqfWv2fvHhXkF6i7j7V3zx7ld1N/T9tP1Z/vynZTQX/7xXF79+5Vfn7+SaNpZzpmd5t7alu2r0xjx47tts1JL0/ZefzVvv37NSZ3zCn77ElvOd72+Ovy8nKNzh3dtfOT/fak91grVVQc0KicnE+2W9u1z8rKZ6XKg5UaMTJL1h4fefTfJO/xWXl8Ph0+0qLUpFj/a69PHp9/RNfrs6pp9unALreONbWfduXAg++9JUlKiolUWmK0hifHKj8zQXkZCcrLSFReRoJy0+IVHenq/ssFAAwYa602lB3Vb1fv0ZrdrUqMceuuhfm646I8ZScz8QnOHaHuLF07fWSP+1Z7y1W0sOdf8Hvb39O+Mx3TaYGu71yPv9pXrqKL+/f+1b4DKrq4b0G1L23P1Ga1PaCiS7rf39O+3t4zGKw2FSoqGhe4468+qKKiCefw/kMqKprYz/ccVlHR5D62dauoaOoZ2tSoqGh6L/tXq6hodi/7imStVX2LR9VNbTra1K61G7ZoxNgJOtrUppqmdtU0tutQXYv+/tERVTe2d73fZaTRafGdQS9B+Z2Bb8rIZKUlRPfpMwIAeuduaNW/PLVVG8uOKiMxWjeOj9L3Pr9IKXFRTpeGMEKoA4AQZ4xRSnyUUuKjVJApNe2LVNHc3G7b1jV3qKymSWXVjSqratLe6iaVVTdpY9lRNZ9wf2ZuWrxmjE71P3JTNWVEsmKjuJQTAPqjrLpJX3x0g6ob2vXDpVP12cLRWv/u2wQ6DDhCHQAMISnxUZoR7w9rJ7LWyt3Qpj3uRn1wsE7bDtRq076jemF7pSQpKsJoyohknX886I1OVV5GApPTAEAP3q+o1e2PbZKV9PSyC0/7uQsMJEIdAEDGGGUnxyo7OVbzx2V0bT9S36qt5bXadqBW2w4c04rNFfrjuv2SpNT4KM3NS9P8ggwtGJeugsxEQh4ASFq7u0pf/vNmpSVE6493zGESFAQcoQ4A0KPs5Fhddd5wXXXecEn+SVxK3A3aVl6rzfuP6b09NXp9xxFJUlZSjOYXpGt+QYbmFaRrdFp8/05mrbRhg3TokDRihDR37rmvxQEAQfa3rQf1jWe3a3x2kp64fbaymAgFQUCoAwD0WYTLaNLwZE0anqzPzfHft1de06z39lTrvT01eqe0Rn/b5r9kMzct3h/yxmVoXn66MpNiej7wK69Id98t1dZKLpfk80mpqdIjj0hLlgTjowHAOfuft/fqRy/v1IX5aVr+xUIlx3LvHIKDUAcAOCe56fHKTc/V5+bkylqrEnej3i31h7yXPzik/910QJI0aXiSFo7PUFKzR3PbvYqL7px45ZVXpBtvlFpaTj5wY6N/+4oVBDsAg5rPZ/WT13Zp+dq9WjJtuP77phmsE4qgItQBAAaMMUYTspM0ITtJty/Ik9dntaOyTu+UVuudkmo98d5+tXt9+vW2lbpgTKoWjsvQXV+6S9GnBrrjWlr8I3jl5VyKCWBQ6vD69M0V7+v5rQf1xXlj9OCnpirCxc8rBBehDgAQMBEuo+k5qZqek6p/Khqnlnav/vDCKjXEj9LbJdV647EXdGvNUfW6Kl5trbRxo/8eOwAYRFo9Vl96olhrd1fp/isn6p+KCpgwCo4g1AEAgiYuOkLnZUSqqGiyvi2p/qlDink+Uuro+T3WuGQqK4NWIwCcibVWB2tb9J+bWrW/vln/ecM03TS7+/VBgWAg1AEAHJOcnyud4R+1m1rb9cP3qjQ8cbcWjs/QjNGpiopwBadAAENSc7tHlbWtqqxt8T/q/M8P1bV0bW/z+BTlkpb/Y6Eum5LtdMkY4gh1AADnzJ0rpaT4J0XpgU1JVcnYKVrxVol++WaJEmMidWF+ui6ekKH5BRkqyGQRdABnx1qrA0ebtb2iVu9X1Gn7gVp9fKRBtc0nXz7gMlJWUqxGpsZqyshkXT4lWyNSYhV9tIxAh0GBUAcAcI4x0vLl3c9+KUlxcUp64g96fslFqmvu0Ht7qvV2abXeLqnSGzv96+NlJ8dofkFG1/IJANATd0Or3j9Qp/crarW9ok5byprV8PoqSVJ0hEuTRyZrybQRyhkWp1GpcRqREqeRqbHKTo7t9gqB1av3B/sjAN0i1AEAnLVkiX/ZgjOsU5cSH6Wrp43Q1dNGSJL21zTp3dIavbenWmt3V+n5rQclSVnxRpce/UDzC9I1ryBdGYm9rI8HIKxZa/VuaY2e3liuLeXHdKiuVZJ/5G1CdpJmZEXqytmTdH5OqiYOT1J0JJd2IzQR6gAAzluyxL9swcaNUmWlNHKkNGdOr8sYjElP0Jj0BH1+bq58Pqvd7ga9W1qjFzd8rJe2V+rpjeWS/OvjzStI19y8dM3JS1NaQq9zbQIIAx1en156v1LL15Zp56F6pSdEa/64DJ2fk6LzR6dq6shkxUdHavXq1Sq6cIzT5QLnjFAHABgcjDnrZQtcLqNJw5M1aXiyCjz7ddHCi/XBwTq9t8c/kvfUhnI99u4+SdKE7ETNyUvT3Lx0zc1LU1Zy7AB+CABOqm/t0NOd/70frm/VuKxE/ecN07R0xijFRrEYOMIXoQ4AEHYiI1yamTtMM3OH6Z8XjVObx6sPKuq0oeyoNpQd1fNbDurP6/0jeXkZCZozNs0f9PLTHK4cwNmoONasx97dp//dWK6mdq/mF6TrP66fpksmZMrFQuAYAgh1AICwFxMZocKxaSocm6Z/XiR5vD7tqKzXxs6Q99qOw/pL8QFJUlqs0byDWzQzN1Uzc1M1dWQK/8IPDFIfVNTp92/v1csfHJIkXTt9hO5amK/zRqU4XBkQXIQ6AMCQExnh0vmjU3X+6FTddXG+fD6rj480aMPeGr266WNtr6jt+iUx0mU0ZWSyZo5O7Rz9S1VuWjzLKAAOaO3wamt5rX+CpJJqbT9Qq8SYSN2xYKxuW5CnUalxTpcIOIJQBwAY8lwuo8kjkjV5RLLGduxXUVGR3A2t2lZeq20HarW1vFbPbq7QE+v805enJURr5uhUzRo7THPGpqnDZx3+BEB48vqstpYf67o/tnjfMbV5fIpwGU3PSdF3l0zWTXNGKzk2yulSAUcR6gAA6EZWUqyumDpcV0wdLsn/y+XuIw3aWl6rreXHtKX8mN7c5ZYkRbmkWaXrNKdz8pWZuamKj+Z/sUB/nTiT7bo91Xq3pFktK9+T5J/J9gtzx2jBuHTNzksjyAEn4P84AAD0QcQJo3mfn5srSappbNOmfcf0/NvbVdnm1a/fKtEvrf+SzfNGpWhunn8ClsIxaUqJ5xdQ4DhrrQ7Xt+rjww0qOdKoj480aPcR//OWDq8kaWx6vOaOiNSNC6dpXn660llzEuiRI6HOGPOvku6UZCV9IOl2a22rE7UAAHC20hNjdNV5wxVbvUtFRRepobVDm/cf08ayo9q076gee3efHlm7V5I0Oi1Ok4YnK66tXU1phzRpRJLGpicogpn54LAj9a1dfXbTvmOqa25XQkykEmMjlRjjfyTEfPI8Mfb46whFRbgUYYyMMYpwGbmM/3JmlzGKMJ+87vD6VOpu1O4jDdp9xP9nQ6unq4bMpBhNzE7S5+aM1tSRKZpXkK5RqXH+deSmj3Tw2wFCQ9BDnTFmlKR7JU2x1rYYY56R9DlJjwe7FgAABlJSbJSKJmapaGKWJP+kDtsO1Grz/mPaeaheuw43aI+7Qy/s2SJJio1yaUJ2kiYNT9LE4cmaPDxJk0YkO/kREOastdpT1dQZ4I7q7Z3NqnrtTUlSfHSELsgdpqkjk9XU5lFj5+NwXaua2jxqaPOoqc2jc7mFNDU+ShOyk/TpGaM0YXiSJmQlakJ2koYlRA/QJwSGJqcuv4yUFGeM6ZAUL6nSoToAAAiY2KgIXZifrgvz07u2rXxzlUZOukA7D9Xr48MN2nW4QW/udOuZ4oquNsNijGbu26QpI5I1ZWSypoxIVm5aPOttod9aO7zaUVmv18o69FR5sYr3H9PRpnZJUnpCtMYmu3T34gmak5emySOSFRXh6vV41lq1dHjV2OoPfB6flc9aeX1W1vrvPfXZ449PXkcYo7zMBGUmxjBzLBAAQQ911tqDxpiHJZVLapG00lq7Mth1AADghOgI//12p66jVdXQpl2H67XzUL1WbSvRwWMtWrO7St7OYZGE6AhN7gx5rvoODTtQqwnZSYqLZg09+FlrVVbdpG0Hjmlr58ytOw/Vq8Pr70O5aQ1aNDFLs8cO0+y8NOVnJGjNmjUqWpjf53MYYxQfHan46EhlBeqDAOg3Jy6/HCZpqaQ8SbWSnjXG3GKt/fMp7ZZJWiZJubm5wS4TAICgykyKUWZSphaOz9QE3wEVFV2s1g6vSo406qNDdfqosl4fHarXX7ccVGObR4/veFeSNDIlVvmZicrPTFB+RoIaqz0qONqsUalxjOyFuWNN7dpWUdu19EZxWbOaXl8tyX8p5fScFH3ponzNGJ2qloqP9A9XLXK2YAAB48Tll5dJKrPWVkmSMeavkuZLOinUWWuXS1ouSYWFhSwABAAYcmKjIjQtJ0XTcj4Z1fP5rFa8tkpJoyerxN2osuom7a1q1PNbDqqhzT/xxMPFqxQT6VJeRkJn2EtUe41HaRW1ys9MVGIMk1+HEmutKo61aPMRj7b8fbc+qvSP6B6sbZEkGSNNyErSrOxIXT1nsmaMTtWE7KSTJuFZXb3LqfIBBIETP9XLJV1ojImX//LLSyUVO1AHAAAhx+Uyyop3qWjaCF19wnZrraoa2/TXv7+rlJzx2lvVqL1VTdp5qEGv7zgir89q+fv+0b0RKbEqyExUQWaCCrISNS4zUQVZicpK4n4np7V5jo/O1neNzu6srO8K7C5TovzMRM0aM0y3XDhG549O0fScVCXGRPpnipzD1U3AUOTEPXUbjDErJG2R5JG0VZ0jcgAA4OwYY5SVFKtJaRGn/WLf7vFpxWurlTZ2ivZUNfof7kY913kp53GJMZEqyExQfmbiSaN8eRkJ3LsXQNZa7an16vW/fqAXt1d2/Z3Ed95HuXTmSE0ZkaLWQyW6eUkRfxcATuPI9RfW2gclPejEuQEAGGqiI10amehS0XnDT9purZW7oU2l7k+CXmlVozbsrdHzWw+e1PbEe/f8gS9Ro4fFaVh8tFLiorh/7yxUN7bpb1sP6pniA9p9pFWxURVaMm2EFk/K0tSRKRpzyoynq1fvJdAB6BYX1QMAMEQZY5SdHKvs5FgtGJdx0r7mdo/2VTdrb7X/Ms69Vf779068d++T40gpcVFKi49WanyUhsVHKzU+WsPiozQsIVrD4qOVnRyjkalxGpkap+TYyCF7mafH69Oa3VV6pviA3tzplsdnNTM3VbdNjdbXP1OkpNgop0sEEIIIdQAA4DTx0ZH+NfJGnrwY+vF798qqmlRZ16JjTR2qbW7X0eZ2HWv2Pz9U16qdh+p1tLldrR2+046dGBOpkamxXSFvVGqc6g55FF92VMOTYxUT5ZIxUoQxchkjl8sowmXkMpLLHH/ufx2scGitVXVjm/ZVN6msukn7a5p1uL5VCdERSomLUvLxR2xU5+tIVbf4VN/aocToSO2radKzmyv03OYKuRvalJEYrTsuytNnZuVofHaSVq9eTaADcNYIdQAAoM+O37uXlRTbp/atHV4dbWrXkfpWVda2qrK2RQdrW1RZ26LKuha9X1HXtRj28vfX9asWl5ESYiKVGBPZ9WfiSa8jlBjrf35wf4cObSxXTKRLMZER/j+jTn8e6TI6Ut+qfTXN/gBX06T9NU0qPdys1tff6Dp3hMsoIzFaLe1eNbR5ZHuap3vNSrmM5LP+9yyamKXPFuZo0aSsMy70DQB9RagDAAABExsV0TUiN7OHiRlb2r3628o1ypk4TYfqWuXxWnmtlc9n5bNWXp+VtZK367mV1yd1eH1qbPOoqc2jpnaPGlr9z6sa2tTY5ul6HF/AXTs/6FftES6jnGFxGpueoOGjIrXg/Akam5GgsekJyhkW1xXKfD6rhjaP6ls6VNfSofrWDtW3dGjjtg81IrdA9a0dSomL0nUzRvY5DANAfxDqAACAo+KiIzQi0aWF4zMH/NjWWrV5fHpj1VoVzp2nNo9XbR6f2jp8nzz3eDtf+9Tu8SkzOea04LZ69WoVLcjr9hwul1FKnP+yy9EnbI+t/lhFF+cP+GcCgFMR6gAAQNgyxig2KkKJ0UbDUxglAxCeuJgbAAAAAEIYoQ4AAAAAQhihDgAAAABCGKEOAAAAAEIYoQ4AAAAAQhihDgAAAABCGKEOAAAAAEIYoQ4AAAAAQhihDgAAAABCGKEOAAAAAEKYsdY6XcMZGWOqJO3vY/MUSXX9PEV/3tOXtmdq09v+nvZlSKo+Y3XOOZvvPZjHp184g35Bv+gO/YJ+0R36Bf2iO/SLwPWL3t5Hvxic/WKMtTaz2z3W2rB6SFoeyPf0pe2Z2vS2v6d9koqd/m4H+nsP5vHpF/QL+sXgedAv6Bf0C/oF/cL5fnGG/kK/GMT9ortHOF5++WKA39OXtmdq09v+s6l/MAh03ed6fPqFM+gX596GfhH849MvnEG/OPc29IvgHz+U+0Wo9gmJfnGakLj8EpIxpthaW+h0HRhc6BfoDv0C3aFfoDv0C3SHfhF6wnGkLlwtd7oADEr0C3SHfoHu0C/QHfoFukO/CDGM1AEAAABACGOkDgAAAABCGKEOAAAAAEIYoQ4AAAAAQhihLsQZY1zGmB8bY35ljLnV6XoweBhjiowxbxtjfmeMKXK6HgwOxpgEY0yxMeZap2vB4GCMmdz5c2KFMeYrTteDwcEY82ljzO+NMX8xxlzhdD0YHIwx+caYPxhjVjhdC05GqHOQMeZRY4zbGPPhKduvMsZ8bIwpNcY8cIbDLJWUI6lDUkWgakVwDVDfsJIaJcWKvhHyBqhPSNK3JD0TmCoRbAPRL6y1O621X5b0WUkLAlkvgmOA+sXfrLV3SfqypJsCWS+CY4D6xV5r7ZcCWynOBrNfOsgYc7H8v3T/0Vp7Xue2CEm7JV0u/y/imyTdLClC0n+ccog7Oh/HrLWPGGNWWGtvDFb9CJwB6hvV1lqfMSZb0s+stV8IVv0YeAPUJ86XlC5/0K+21r4UnOoRKAPRL6y1bmPMdZK+IulP1tqnglU/AmOg+kXn+/4/SU9aa7cEqXwEyAD3C37nHGQinS5gKLPWrjXGjD1l8xxJpdbavZJkjPlfSUuttf8h6bTLpYwxFZLaO196A1ctgmkg+sYJjkmKCUSdCJ4B+nlRJClB0hRJLcaYV6y1vkDWjcAaqJ8V1toXJL1gjHlZEqEuxA3Qzwsj6SeSXiXQhYcB/t0CgwyhbvAZJenACa8rJM3tpf1fJf3KGLNQ0tpAFgbH9atvGGOul3SlpFRJvw5saXBIv/qEtfa7kmSMuU2dI7kBrQ5O6e/PiiJJ18v/jz+vBLQyOKm/v1/cI+kySSnGmHHW2t8Fsjg4pr8/L9Il/VjSTGPMtzvDHwYBQl2Is9Y2S+LaZpzGWvtX+UM/cBJr7eNO14DBw1q7WtJqh8vAIGOt/aWkXzpdBwYXa22N/PdZYpBhopTB56Ck0Se8zuncBtA3cCr6BLpDv0B36BfoDv0iTBDqBp9NksYbY/KMMdGSPifpBYdrwuBA38Cp6BPoDv0C3aFfoDv0izBBqHOQMeZpSeskTTTGVBhjvmSt9Uj6F0mvS9op6Rlr7Q4n60Tw0TdwKvoEukO/QHfoF+gO/SK8saQBAAAAAIQwRuoAAAAAIIQR6gAAAAAghBHqAAAAACCEEeoAAAAAIIQR6gAAAAAghBHqAAAAACCEEeoAAIOeMaYxyOf7H2PMlCCf86vGmPhgnhMAEB5Ypw4AMOgZYxqttYkDeLzIzkV3g8YYY+T//66vh/37JBVaa6uDWRcAIPQxUgcACEnGmExjzHPGmE2djwWd2+cYY9YZY7YaY94zxkzs3H6bMeYFY8xbkt40xhQZY1YbY1YYY3YZY57sDF7q3F7Y+bzRGPNjY8x2Y8x6Y0x25/aCztcfGGN+1N1oojFmrDHmY2PMHyV9KGm0Mea3xphiY8wOY8wPOtvdK2mkpFXGmFWd267o/BxbjDHPGmMGLNQCAMILoQ4AEKp+Iem/rbWzJd0g6X86t++StNBaO1PS9yX9vxPec4GkG621l3S+ninpq5KmSMqXtKCb8yRIWm+tPV/SWkl3nXD+X1hrp0mq6KXO8ZJ+Y62daq3dL+m71tpCSdMlXWKMmW6t/aWkSkmLrLWLjDEZkv5N0mXW2gskFUv6Wt++FgDAUBPpdAEAAJylyyRN6Rxck6TkztGsFElPGGPGS7KSok54z9+ttUdPeL3RWlshScaYbZLGSnrnlPO0S3qp8/lmSZd3Pp8n6dOdz5+S9HAPde7//9u7e9UqgjgM489rIaKotZ1CEMuACgGvwEpsJOnEQtIoErwHwTqVlZXegtjEJiRd8BRegVYWGkSiiPwtdtQlJH5EQ5ic5wcLszs7H6daXs4MU1Xro/sbSW4zfIPPMATKybY2c+35avt9R4G1XfqXJE05Q50kqVdHgLmq+jR+mGQZWKmq60nOAi9G1R+39fF5VP7Kzt/FL/VzA/pu7/zKjzGTnAPuA5er6l2Sx8CxHdqEIYAu/OVYkqQp5PJLSVKvngN3vt8kmW3F08CbVr65j+OvMyz7BJj/wzanGELeZtubd3VU9wE4Oer7SpIZgCQnkpz/9ylLkg4jQ50kqQfHk7weXUvAXeBSkkmSV8Bie/ch8CDJBvu7IuUesJRkAswAm79rUFUvgQ2GfX9PgNVR9SPgWZKVqnrLEEiftv7XgAv/d/qSpMPCIw0kSdqDdqbcVlVVknlgoaquHfS8JEnTxz11kiTtzUVguR2D8B64dcDzkSRNKf+pkyRJkqSOuadOkiRJkjpmqJMkSZKkjhnqJEmSJKljhjpJkiRJ6pihTpIkSZI6ZqiTJEmSpI59A5DiV5dgU2mRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax, lr = lr_finder.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(model=seq2seq,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                train_loader=train_iterator,\n",
    "                valid_loader=valid_iterator,\n",
    "                field=EN,\n",
    "                n_epochs=N_EPOCHS,\n",
    "                grad_clip=GRAD_CLIP,\n",
    "                tf_ratio=TF_RATIO,\n",
    "                last_improv=0,\n",
    "                model_name=MODEL_NAME,\n",
    "                device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(history['loss'], label='train')\n",
    "axes[0].plot(history['val_loss'], label='valid')\n",
    "axes[0].set_title('Loss history')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "axes[1].plot(history['acc'], label='train')\n",
    "axes[1].plot(history['val_acc'], label='valid')\n",
    "axes[1].plot(np.array(history['bleu4']) * 100., label='BLEU-4')\n",
    "axes[1].set_title('Top-5 Accuracy & BLEU-4 history')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy & BLEU-4 (%)')\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq.load_state_dict(torch.load(f'./checkpoint/BEST_{MODEL_NAME}.pt').get('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, beam_size, src_field, dest_field, max_len, device):\n",
    "    references, hypotheses, sources = [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, data in pbar:\n",
    "            (src_sequences, src_lengths) = data.src[0], data.src[1]\n",
    "            (dest_sequences, dest_lengths) = data.dest[0], data.dest[1]\n",
    "            \n",
    "            batch_size = src_sequences.shape[1]\n",
    "            for j in range(batch_size): # We evaluate sentence by sentence\n",
    "                src_sequence = src_sequences[:, j].unsqueeze(1) # [seq_len, 1]\n",
    "                dest_sequence = dest_sequences[:, j].unsqueeze(1) # [seq_len, 1]\n",
    "                src_length, dest_length = src_lengths[j, None], dest_lengths[j, None] # [1,]\n",
    "                \n",
    "                # Encoding\n",
    "                _, h_state, c_state = model.encoder(input_sequences=src_sequence,\n",
    "                                                    sequence_lengths=src_length)\n",
    "                \n",
    "                # Init hidden and memory states\n",
    "                h_state = model.init_h0(h_state.permute(1, 2, 0)) # [batch_size, hidden_size, n_layers]\n",
    "                c_state = model.init_c0(c_state.permute(1, 2, 0)) # [batch_size, hidden_size, n_layers]\n",
    "                h_state = h_state.permute(2, 0, 1) # [n_layers, batch_size, hidden_size]\n",
    "                c_state = c_state.permute(2, 0, 1) # [n_layers, batch_size, hidden_size]\n",
    "                \n",
    "                # Decoding\n",
    "                tree = [[Node(\n",
    "                    token=torch.LongTensor([\n",
    "                        dest_field.vocab.stoi[dest_field.init_token]\n",
    "                    ]).to(device),\n",
    "                    states=(h_state, c_state)\n",
    "                )]]\n",
    "                \n",
    "                for _ in range(max_len):\n",
    "                    next_nodes = []\n",
    "                    for node in tree[-1]:\n",
    "                        # Skip eos token\n",
    "                        if node.eos:\n",
    "                            continue\n",
    "                        # Decode\n",
    "                        logit, h_state, c_state = model.decoder(\n",
    "                            input_word_index=node.token, \n",
    "                            h_state=node.states[0].contiguous(),\n",
    "                            c_state=node.states[1].contiguous()\n",
    "                        )\n",
    "                        # logit: [1, vocab_size]\n",
    "                        # h_state: [n_layers, 1, hidden_size]\n",
    "                        # c_state: [n_layers, 1, hidden_size]\n",
    "\n",
    "                        # Get scores\n",
    "                        logp = F.log_softmax(logit, dim=1).squeeze(dim=0) # [vocab_size]\n",
    "\n",
    "                        # Get top k tokens & logps\n",
    "                        topk_logps, topk_tokens = torch.topk(logp, beam_size)\n",
    "\n",
    "                        for k in range(beam_size):\n",
    "                            next_nodes.append(Node(\n",
    "                                token=topk_tokens[k, None],\n",
    "                                states=(h_state, c_state),\n",
    "                                logp=topk_logps[k, None].cpu().item(),\n",
    "                                parent=node,\n",
    "                                eos=topk_tokens[k].cpu().item() == dest_field.vocab[dest_field.eos_token]\n",
    "                            ))\n",
    "                    \n",
    "                    if len(next_nodes) == 0:\n",
    "                        break\n",
    "                    \n",
    "                    # Sort next_nodes to get the best\n",
    "                    next_nodes = sorted(next_nodes,\n",
    "                                        key=lambda node: node.logps,\n",
    "                                        reverse=True)\n",
    "                    # Update the tree\n",
    "                    tree.append(next_nodes[:beam_size])\n",
    "                \n",
    "                # Find the best path of the tree\n",
    "                best_path = find_best_path(tree)\n",
    "                \n",
    "                # Get the translation\n",
    "                pred_translated = [*map(lambda node: dest_field.vocab.itos[node.token], best_path)]\n",
    "                pred_translated = [*filter(lambda word: word not in [\n",
    "                    dest_field.init_token, dest_field.eos_token\n",
    "                ], pred_translated[::-1])]\n",
    "                \n",
    "                # Update hypotheses\n",
    "                hypotheses.append(pred_translated)\n",
    "                \n",
    "                # Update references\n",
    "                references.append([[\n",
    "                    dest_field.vocab.itos[indice] \n",
    "                    for indice in dest_sequence \n",
    "                    if indice not in (\n",
    "                        dest_field.vocab.stoi[dest_field.init_token],\n",
    "                        dest_field.vocab.stoi[dest_field.eos_token],\n",
    "                        dest_field.vocab.stoi[dest_field.pad_token]\n",
    "                    )\n",
    "                ]])\n",
    "                \n",
    "                # Update sources\n",
    "                sources.append([\n",
    "                    src_field.vocab.itos[indice] \n",
    "                    for indice in src_sequence \n",
    "                    if indice not in (\n",
    "                        src_field.vocab.stoi[src_field.init_token],\n",
    "                        src_field.vocab.stoi[src_field.eos_token],\n",
    "                        src_field.vocab.stoi[src_field.pad_token]\n",
    "                    )\n",
    "                ])\n",
    "    \n",
    "        # Calculate BLEU-4 score\n",
    "        assert len(hypotheses) == len(references) == len(sources)\n",
    "        bleu4 = bleu_score(hypotheses, references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    \n",
    "    return hypotheses, references, sources, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, bleu4 = evaluate(seq2seq.to(DEVICE),\n",
    "                          loader=test_iterator,\n",
    "                          beam_size=1,\n",
    "                          src_field=FR,\n",
    "                          dest_field=EN,\n",
    "                          max_len=MAX_LENGTH,\n",
    "                          device=DEVICE)\n",
    "print(f'BLEU-4: {bleu4*100:.3f}% with beam_size=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, bleu4 = evaluate(seq2seq.to(DEVICE),\n",
    "                          loader=test_iterator,\n",
    "                          beam_size=5,\n",
    "                          src_field=FR,\n",
    "                          dest_field=EN,\n",
    "                          max_len=MAX_LENGTH,\n",
    "                          device=DEVICE)\n",
    "print(f'BLEU-4: {bleu4*100:.3f}% with beam_size=5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentences, model, beam_size, src_field, dest_field, max_len, device):\n",
    "    if isinstance(sentences, list):\n",
    "        sentences = [*map(src_field.preprocess, sentences)]\n",
    "        targets = None\n",
    "    if isinstance(sentences, Dataset):\n",
    "        targets = [*map(lambda example: ' '.join(example.dest), sentences.examples)]\n",
    "        sentences = [*map(lambda example: example.src, sentences.examples)]\n",
    "    data = [*map(lambda word_list: src_field.process([word_list]), sentences)]\n",
    "    \n",
    "    translated_sentences = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(data), total=len(data))\n",
    "        for i, (src_sequence, src_length) in pbar:\n",
    "            src_sequence, src_length = src_sequence.to(device), src_length.to(device)\n",
    "                \n",
    "            # Encoding\n",
    "            _, h_state, c_state = model.encoder(input_sequences=src_sequence,\n",
    "                                                    sequence_lengths=src_length)\n",
    "                \n",
    "            # Init hidden and memory states\n",
    "            h_state = model.init_h0(h_state.permute(1, 2, 0)) # [batch_size, hidden_size, n_layers]\n",
    "            c_state = model.init_c0(c_state.permute(1, 2, 0)) # [batch_size, hidden_size, n_layers]\n",
    "            h_state = h_state.permute(2, 0, 1) # [n_layers, batch_size, hidden_size]\n",
    "            c_state = c_state.permute(2, 0, 1) # [n_layers, batch_size, hidden_size]\n",
    "                \n",
    "            # Decoding\n",
    "            tree = [[Node(\n",
    "                token=torch.LongTensor([\n",
    "                    dest_field.vocab.stoi[dest_field.init_token]\n",
    "                ]).to(device),\n",
    "                states=(h_state, c_state)\n",
    "            )]]\n",
    "               \n",
    "            # Generate tokens\n",
    "            for _ in range(max_len):\n",
    "                next_nodes = []\n",
    "                for node in tree[-1]:\n",
    "                    # Skip eos token\n",
    "                    if node.eos:\n",
    "                        continue\n",
    "                    # Decode\n",
    "                    logit, h_state, c_state = model.decoder(\n",
    "                        input_word_index=node.token, \n",
    "                        h_state=node.states[0].contiguous(),\n",
    "                        c_state=node.states[1].contiguous()\n",
    "                    )\n",
    "                    # logit: [1, vocab_size]\n",
    "                    # h_state: [n_layers, 1, hidden_size]\n",
    "                    # c_state: [n_layers, 1, hidden_size]\n",
    "\n",
    "                    # Get scores\n",
    "                    logp = F.log_softmax(logit, dim=1).squeeze(dim=0) # [vocab_size]\n",
    "\n",
    "                    # Get top k tokens & logps\n",
    "                    topk_logps, topk_tokens = torch.topk(logp, beam_size)\n",
    "\n",
    "                    for k in range(beam_size):\n",
    "                        next_nodes.append(Node(\n",
    "                            token=topk_tokens[k, None],\n",
    "                            states=(h_state, c_state),\n",
    "                            logp=topk_logps[k, None].cpu().item(),\n",
    "                            parent=node,\n",
    "                            eos=topk_tokens[k].cpu().item() == dest_field.vocab[dest_field.eos_token]\n",
    "                        ))\n",
    "                \n",
    "                if len(next_nodes) == 0:\n",
    "                    break\n",
    "\n",
    "                # Sort next_nodes to get the best\n",
    "                next_nodes = sorted(next_nodes,\n",
    "                                    key=lambda node: node.logps,\n",
    "                                    reverse=True)\n",
    "                # Update the tree\n",
    "                tree.append(next_nodes[:beam_size])\n",
    "                \n",
    "            # Find the best path of the tree\n",
    "            best_path = find_best_path(tree)\n",
    "\n",
    "            # Get the translation\n",
    "            pred_translated = [*map(lambda node: dest_field.vocab.itos[node.token], best_path)]\n",
    "            pred_translated = [*filter(lambda word: word not in [\n",
    "                dest_field.init_token, dest_field.eos_token\n",
    "            ], pred_translated[::-1])]\n",
    "                \n",
    "            translated_sentences.append(' '.join(pred_translated))\n",
    "            \n",
    "        sentences = [*map(lambda sentence: ' '.join(sentence), sentences)]\n",
    "    return sentences, translated_sentences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, translated_sentences, dest_sentences = translate(sentences=test_data,\n",
    "                                                            model=seq2seq,\n",
    "                                                            beam_size=5,\n",
    "                                                            src_field=FR,\n",
    "                                                            dest_field=EN,\n",
    "                                                            max_len=MAX_LENGTH,\n",
    "                                                            device=DEVICE)\n",
    "\n",
    "indexes = np.random.choice(len(test_data.examples), size=20, replace=False)\n",
    "print(indexes)\n",
    "print()\n",
    "for i in indexes:\n",
    "    html = f'<p><span style=\"color:blue\"><b>Source:</b> {sentences[i]}</span><br />'\n",
    "    html += f'<span style=\"color:green\"><b>Ground truth translation:</b> {dest_sentences[i]}</span><br />'\n",
    "    html += f'<span style=\"color:red\"><b>Predicted translation:</b> {translated_sentences[i]}</span></p>'\n",
    "    display(HTML(html))\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logp(model, loader, src_field, dest_field, max_len, device):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
