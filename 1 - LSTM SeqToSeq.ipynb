{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug  9 17:15:50 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 26%   28C    P8     6W / 180W |      1MiB / 16278MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Example, Field, Dataset\n",
    "from torchtext.data.iterator import BucketIterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import warnings\n",
    "\n",
    "import utils\n",
    "import torch_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "SEED = 781\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2,007,723\n",
      "CPU times: user 3.42 s, sys: 1.01 s, total: 4.42 s\n",
      "Wall time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairs = [*zip(\n",
    "    utils.read_file('./data/europarl-v7.fr-en.fr'),\n",
    "    utils.read_file('./data/europarl-v7.fr-en.en')\n",
    ")]\n",
    "pairs = [*map(lambda x: {'fr': x[0], 'en': x[1]}, pairs)]\n",
    "print(f'Number of examples: {len(pairs):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples after sampling: 200,000\n",
      "Example:\n",
      "\tFR => Les procédures par le biais desquelles de tels produits entrent et sortent de l'Union européenne doivent être ouvertes, transparentes et, par dessus tout, sûres.\n",
      "\n",
      "\tEN => The procedures whereby such products come in and out of the European Union have to be open, transparent and, above all, safe.\n",
      "\n",
      "CPU times: user 1.29 s, sys: 49.1 ms, total: 1.34 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairs = np.random.choice(pairs, size=200_000, replace=False)\n",
    "pairs = [*map(\n",
    "    lambda pair: {k: v for k, v in pair.items()}, # utils.normalize_string(v)\n",
    "    pairs\n",
    ")]\n",
    "print(f'Number of examples after sampling: {len(pairs):,}')\n",
    "print(f'Example:\\n\\tFR => {pairs[0][\"fr\"]}\\n\\tEN => {pairs[0][\"en\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_fr = [*map(lambda pair: len(pair['fr'].split()), pairs)]\n",
    "len_en = [*map(lambda pair: len(pair['en'].split()), pairs)]\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].hist([*filter(lambda x: x < 50, len_fr)])\n",
    "axes[0].grid(True)\n",
    "axes[1].hist([*filter(lambda x: x < 50, len_en)])\n",
    "axes[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [01:22<00:00, 2410.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples after filtering: 39,389\n"
     ]
    }
   ],
   "source": [
    "FR = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='fr',\n",
    "           include_lengths=True)\n",
    "EN = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='en',\n",
    "           include_lengths=True)\n",
    "\n",
    "MIN_LENGTH, MAX_LENGTH = 20, 30\n",
    "\n",
    "examples = [Example.fromdict(\n",
    "    data=pair,\n",
    "    fields={\n",
    "        'fr': ('src', FR),\n",
    "        'en': ('dest', EN)\n",
    "    }\n",
    ") for pair in tqdm.tqdm(pairs)]\n",
    "examples = [*filter(\n",
    "    lambda example: MIN_LENGTH <= len(example.src) and len(example.src) <= MAX_LENGTH \\\n",
    "        and MIN_LENGTH <= len(example.dest) and len(example.dest) <= MAX_LENGTH,\n",
    "    examples\n",
    ")]\n",
    "print(f'Number of examples after filtering: {len(examples):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 31,511\n",
      "valid set size: 3,939\n",
      "test set size: 3,939\n",
      "{'src': [\"d'\", 'un', 'point', 'de', 'vue', 'psychologique', ',', 'la', 'destruction', 'des', 'infrastructures', 'critiques', 'entraîne', 'une', 'diminution', 'de', 'la', 'confiance', 'de', 'la', 'population', 'en', \"l'\", 'union', 'européenne', '.'], 'dest': ['from', 'a', 'psychological', 'point', 'of', 'view', ',', 'the', 'destruction', 'of', 'critical', 'infrastructures', 'leads', 'to', 'a', 'loss', 'of', 'public', 'confidence', 'in', 'the', 'european', 'union', '.']}\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(examples, fields={'src': FR, 'dest': EN})\n",
    "train_data, valid_data, test_data = data.split(split_ratio=[0.8, 0.1, 0.1])\n",
    "print(f'train set size: {len(train_data.examples):,}')\n",
    "print(f'valid set size: {len(valid_data.examples):,}')\n",
    "print(f'test set size: {len(test_data.examples):,}')\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of FR vocabulary: 7,988\n",
      "Length of EN vocabulary: 6,589\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 5\n",
    "FR.build_vocab(train_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "EN.build_vocab(train_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "print(f'Length of FR vocabulary: {len(FR.vocab):,}')\n",
    "print(f'Length of EN vocabulary: {len(EN.vocab):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        vocab_size,\n",
    "        hidden_size,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "        recurrent_dropout\n",
    "    ):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            bidirectional=True,\n",
    "                            dropout=(recurrent_dropout if n_layers > 1 else 0))\n",
    "    \n",
    "    def forward(self, input_sequences, sequence_lengths):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_sequences: Tensor[seq_len, batch_size]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            \n",
    "        :return\n",
    "            outputs: Tensor[seq_len, batch_size, 2 * hidden_size]\n",
    "            hn: Tensor[n_layers * 2, batch_size, hidden_size]\n",
    "            cn: Tensor[n_layers * 2, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_sequences)\n",
    "        embedded = self.dropout(embedded)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, sequence_lengths)\n",
    "        outputs, (hn, cn) = self.lstm(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        return outputs, hn, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoder():\n",
    "    batch_size = 128\n",
    "    train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                              batch_size=batch_size,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True)\n",
    "    encoder = EncoderLSTM(\n",
    "        embedding_size=300,\n",
    "        vocab_size=len(FR.vocab),\n",
    "        hidden_size=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.25,\n",
    "        recurrent_dropout=0.25\n",
    "    )\n",
    "    for data in train_iterator:\n",
    "        outputs, hn, cn = encoder(\n",
    "            input_sequences=data.src[0],\n",
    "            sequence_lengths=data.src[1]\n",
    "        )\n",
    "        seq_len = data.src[0].size(0)\n",
    "        assert outputs.size() == torch.Size([seq_len, batch_size, 2 * 256]), outputs.size()\n",
    "        assert hn.size() == torch.Size([4 * 2, batch_size, 256]), hn.size()\n",
    "        assert cn.size() == torch.Size([4 * 2, batch_size, 256]), cn.size()\n",
    "        break\n",
    "    \n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        vocab_size,\n",
    "        hidden_size,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "        recurrent_dropout\n",
    "    ):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=(recurrent_dropout if n_layers > 1 else 0))\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input_word_index, h_state, c_state):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_word_index: Tensor[batch_size,]\n",
    "            h_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            c_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            \n",
    "        :return\n",
    "            logit: Tensor[batch_size, vocab_size]\n",
    "            h_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            c_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_word_index.unsqueeze(0))\n",
    "        embedded = self.dropout(embedded)\n",
    "        outputs, (h_state, c_state) = self.lstm(embedded, (h_state, c_state))\n",
    "        logit = self.fc(outputs.squeeze(0))\n",
    "        return logit, h_state, c_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decoder():\n",
    "    batch_size = 128\n",
    "    train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                              batch_size=batch_size,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True)\n",
    "    decoder = DecoderLSTM(\n",
    "        embedding_size=300,\n",
    "        vocab_size=len(EN.vocab),\n",
    "        hidden_size=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.25,\n",
    "        recurrent_dropout=0.25\n",
    "    )\n",
    "    for data in train_iterator:\n",
    "        logit, h_state, c_state = decoder(\n",
    "            input_word_index=data.dest[0][0],\n",
    "            h_state=torch.rand(4, batch_size, 256),\n",
    "            c_state=torch.rand(4, batch_size, 256)\n",
    "        )\n",
    "        assert logit.size() == torch.Size([batch_size, len(EN.vocab)]), logit.size()\n",
    "        assert h_state.size() == torch.Size([4, batch_size, 256]), h_state.size()\n",
    "        assert c_state.size() == torch.Size([4, batch_size, 256]), c_state.size()\n",
    "        break\n",
    "        \n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToSeqLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            'Encoder and Decoder must have the same number of reccurent layers'\n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            'Encoder and Decoder must have the same number of reccurrent hidden units'\n",
    "        \n",
    "        super(SeqToSeqLSTM, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src_sequences, src_lengths, dest_sequences, dest_lengths, tf_ratio):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            src_sequences: Tensor[seq_len, batch_size]\n",
    "            src_lengths: Tensor[batch_size,]\n",
    "            dest_sequences: Tensor[seq_len, batch_size]\n",
    "            dest_lengths: Tensor[batch_size,]\n",
    "            tf_ratio: float\n",
    "            \n",
    "        :return\n",
    "            logits: Tensor[max(decode_lengths), batch_size, vocab_size]\n",
    "            sorted_dest_sequences: Tensor[seq_len, batch_size]\n",
    "            sorted_decode_lengths: Tensor[batch_size,]\n",
    "            sorted_indices: Tensor[batch_size,]\n",
    "        \"\"\"\n",
    "        # Encoding\n",
    "        _, h_state, c_state = self.encoder(\n",
    "            input_sequences=src_sequences,\n",
    "            sequence_lengths=src_lengths\n",
    "        )\n",
    "        # h_state: [n_layers * 2, batch_size, hidden_size]\n",
    "        # c_state: [n_layers * 2, batch_size, hidden_size]\n",
    "        \n",
    "        # Sort the batch (dest) by decreasing lengths\n",
    "        sorted_dest_lengths, sorted_indices = torch.sort(dest_lengths, dim=0, descending=True)\n",
    "        sorted_dest_sequences = dest_sequences[:, sorted_indices]\n",
    "        \n",
    "        # Init hidden and memory states\n",
    "        h_state = h_state[:self.decoder.n_layers] \\\n",
    "            + h_state[self.decoder.n_layers:] # [n_layers, batch_size, hidden_size]\n",
    "        c_state = c_state[:self.decoder.n_layers] \\\n",
    "            + c_state[self.decoder.n_layers:] # [n_layers, batch_size, hidden_size]\n",
    "        \n",
    "        # We won't decode at the <eos> position, since we've finished generating as soon as we generate <eos>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        sorted_decode_lengths = (sorted_dest_lengths - 1).tolist()\n",
    "        \n",
    "        # Decoding\n",
    "        batch_size, last = h_state.size(1), None\n",
    "        logits = torch.zeros(max(sorted_decode_lengths), batch_size, self.decoder.vocab_size).to(self.device)\n",
    "        for t in range(max(sorted_decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in sorted_decode_lengths])\n",
    "            if last is not None:\n",
    "                if random.random() < tf_ratio:\n",
    "                    in_ = last[:batch_size_t]\n",
    "                else:\n",
    "                    in_ = sorted_dest_sequences[t, :batch_size_t]\n",
    "            else:\n",
    "                in_ = sorted_dest_sequences[t, :batch_size_t]\n",
    "            # in_ [batch_size,]\n",
    "            logit, h_state, c_state = self.decoder(\n",
    "                in_, \n",
    "                h_state[:, :batch_size_t, :],\n",
    "                c_state[:, :batch_size_t, :]\n",
    "            )\n",
    "            # logit: [batch_size, vocab_size]\n",
    "            # h_state: [num_layers, batch_size, hidden_size]\n",
    "            # c_state: [num_layers, batch_size, hidden_size]\n",
    "            logits[t, :batch_size_t, :] = logit\n",
    "            last = torch.argmax(F.softmax(logit, dim=1), dim=1) # [batch_size,]\n",
    "        \n",
    "        return logits, sorted_dest_sequences, sorted_decode_lengths, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_seq2seq():\n",
    "    batch_size = 128\n",
    "    train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                              batch_size=batch_size,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True)\n",
    "    encoder = EncoderLSTM(\n",
    "        embedding_size=300,\n",
    "        vocab_size=len(FR.vocab),\n",
    "        hidden_size=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.25,\n",
    "        recurrent_dropout=0.25\n",
    "    )\n",
    "    decoder = DecoderLSTM(\n",
    "        embedding_size=300,\n",
    "        vocab_size=len(EN.vocab),\n",
    "        hidden_size=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.25,\n",
    "        recurrent_dropout=0.25\n",
    "    )\n",
    "    model = SeqToSeqLSTM(encoder, decoder, device='cpu')\n",
    "    for data in train_iterator:\n",
    "        logits, sorted_dest_sequences, sorted_decode_lengths, sorted_indices = \\\n",
    "            model(\n",
    "                src_sequences=data.src[0], \n",
    "                src_lengths=data.src[1],\n",
    "                dest_sequences=data.dest[0],\n",
    "                dest_lengths=data.dest[1],\n",
    "                tf_ratio=0.\n",
    "            )\n",
    "        assert logits.size() == torch.Size([\n",
    "            max(sorted_decode_lengths),\n",
    "            batch_size,\n",
    "            len(EN.vocab)\n",
    "        ]), logits.size()\n",
    "        assert sorted_dest_sequences.size() == torch.Size([\n",
    "            data.dest[0].shape[0],\n",
    "            batch_size\n",
    "        ]), sorted_dest_sequences.size()\n",
    "        assert len(sorted_decode_lengths) == batch_size, len(sorted_decode_lengths)\n",
    "        assert sorted_indices.size() == torch.Size([\n",
    "            batch_size,\n",
    "        ]), sorted_indices.size()\n",
    "        break\n",
    "        \n",
    "test_seq2seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, criterion, loader, epoch, grad_clip, tf_ratio, device):\n",
    "    loss_tracker, acc_tracker = utils.AverageMeter(), utils.AverageMeter()\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "    for i, data in pbar:\n",
    "        # Forward prop.\n",
    "        logits, sorted_dest_sequences, sorted_decode_lengths, sorted_indices = \\\n",
    "            model(*data.src, *data.dest, tf_ratio=tf_ratio)\n",
    "        # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "        sorted_dest_sequences = sorted_dest_sequences[1:, :]\n",
    "        # Remove paddings\n",
    "        logits = nn.utils.rnn.pack_padded_sequence(\n",
    "            logits,\n",
    "            sorted_decode_lengths\n",
    "        ).data\n",
    "        sorted_dest_sequences = nn.utils.rnn.pack_padded_sequence(\n",
    "            sorted_dest_sequences,\n",
    "            sorted_decode_lengths\n",
    "        ).data\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, sorted_dest_sequences)\n",
    "        # Back prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if grad_clip is not None:\n",
    "            torch_utils.clip_gradient(optimizer, grad_clip)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Track metrics\n",
    "        loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "        acc_tracker.update(\n",
    "            torch_utils.accuracy(logits, sorted_dest_sequences, 5),\n",
    "            sum(sorted_decode_lengths)\n",
    "        )\n",
    "        # Update progressbar description\n",
    "        pbar.set_description(f'Epoch: {epoch + 1:03d} - loss: {loss_tracker.average:.3f} - acc: {acc_tracker.average:.3f}%')\n",
    "    return loss_tracker.average, acc_tracker.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, loader, field, epoch, device):\n",
    "    references, hypotheses = [], []\n",
    "    loss_tracker, acc_tracker = utils.AverageMeter(), utils.AverageMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, data in pbar: \n",
    "            # Forward prop.\n",
    "            logits, sorted_dest_sequences, sorted_decode_lengths, sorted_indices = \\\n",
    "                model(*data.src, *data.dest, tf_ratio=tf_ratio)\n",
    "            # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "            sorted_dest_sequences = sorted_dest_sequences[1:, :]\n",
    "            # Remove paddings\n",
    "            logits_copy = logits.clone()\n",
    "            logits = nn.utils.rnn.pack_padded_sequence(\n",
    "                logits,\n",
    "                sorted_decode_lengths\n",
    "            ).data\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, sorted_dest_sequences)\n",
    "            # Track metrics\n",
    "            loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "            acc_tracker.update(\n",
    "                torch_utils.accuracy(logits, sorted_dest_sequences, 5),\n",
    "                sum(sorted_decode_lengths)\n",
    "            )\n",
    "            # Update references\n",
    "            target_sequences = data.dest[0][:, sorted_indices]\n",
    "            for j in range(target_sequences.size(1)):\n",
    "                target_sequence = target_sequences[:, j].tolist()\n",
    "                reference = [\n",
    "                    field.vocab.itos[indice] \n",
    "                    for indice in target_sequence \n",
    "                    if indice not in (\n",
    "                        field.vocab.stoi[field.init_token],\n",
    "                        field.vocab.stoi[field.pad_token]\n",
    "                    )\n",
    "                ]\n",
    "                references.append([reference])\n",
    "            # Update hypotheses\n",
    "            _, preds = torch.max(logits_copy, dim=2)\n",
    "            preds, temp_preds = preds.t().tolist(), []\n",
    "            for j, p in enumerate(preds):\n",
    "                temp_preds.append([*map(\n",
    "                    lambda w: field.vocab.itos[w],\n",
    "                    preds[j][:sorted_decode_lengths[j]])]) # Remove padding\n",
    "                hypotheses.extend(temp_preds)\n",
    "            # Update progressbar description\n",
    "            pbar.set_description(f'Epoch: {epoch + 1:03d} - val_loss: {loss_tracker.average:.3f} - val_acc: {acc_tracker.average:.3f}%')\n",
    "        # Calculate BLEU-4 score\n",
    "        bleu4 = bleu_score(hypotheses, references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    return loss_tracker.average, acc_tracker.average, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, valid_loader, field, n_epochs, grad_clip, tf_ratio, device):\n",
    "    history, best_bleu = {\n",
    "        'acc': [],\n",
    "        'loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_loss': [],\n",
    "        'bleu4': []\n",
    "    }, 0.\n",
    "    for epoch in range(n_epochs):\n",
    "         # Stop training if no improvment since last 4 epochs\n",
    "        if last_improv == 4:\n",
    "            print('Training Finished - The model has stopped improving since last 4 epochs')\n",
    "            break\n",
    "        # Decay LR if no improvment\n",
    "        if last_improv > 0:\n",
    "            torch_utils.adjust_lr(optimizer, 0.8)\n",
    "        # Train step\n",
    "        loss, acc = train_step(model=model,\n",
    "                               optimizer=optimizer,\n",
    "                               criterion=criterion,\n",
    "                               loader=train_loader,\n",
    "                               epoch=epoch,\n",
    "                               grad_clip=grad_clip, \n",
    "                               tf_ratio=tf_ratio,\n",
    "                               device=device)\n",
    "        # Validation step\n",
    "        val_loss, val_acc, bleu4 = validate(model=model,\n",
    "                                            criterion=criterion,\n",
    "                                            loader=valid_loader,\n",
    "                                            field=field,\n",
    "                                            epoch=epoch,\n",
    "                                            device=device)\n",
    "        # Update history dict\n",
    "        history['acc'].append(acc)\n",
    "        history['loss'].append(loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['bleu4'].append(bleu4)\n",
    "        # Print BLEU score\n",
    "        text = f'BLEU-4: {bleu4*100:.3f}%'\n",
    "        if best_bleu > bleu4:\n",
    "            last_improv += 1\n",
    "            text += f' - Last improvement since {last_improv} epoch(s)'\n",
    "        else:\n",
    "            best_bleu, last_improv = bleu4, 0\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(model=model,\n",
    "                        optimizer=optimizer,\n",
    "                        data_name=model_name,\n",
    "                        epoch=epoch,\n",
    "                        last_improv=last_improv,\n",
    "                        bleu4=bleu4,\n",
    "                        is_best=bleu4 >= best_bleu)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'seq2seq-lstm'\n",
    "HIDDEN_SIZE = 256\n",
    "EMBEDDING_SIZE = 300\n",
    "DROPOUT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-5\n",
    "GRAD_CLIP = 5.\n",
    "TF_RATIO = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
