{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  7 19:31:02 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   38C    P0    43W / 180W |      1MiB / 16278MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Example, Field, Dataset\n",
    "from torchtext.data.iterator import BucketIterator\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 781\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, mode='rt', encoding='utf-8') as file:\n",
    "            content = file.readlines()\n",
    "        return content\n",
    "    except:\n",
    "        raise Error(f'File {filepath} doesn\\'t exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    # NFD => Normal Form Decompose\n",
    "    # Mn => Non Marking Space\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) \\\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    # Transform accented characters into unaccented ones\n",
    "    s = unicode_to_ascii(s.strip())\n",
    "    # Remove a sequence of whitespace characters\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2,007,723\n",
      "CPU times: user 3.39 s, sys: 980 ms, total: 4.37 s\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairs = [*zip(read_file('./data/europarl-v7.fr-en.fr'),\n",
    "              read_file('./data/europarl-v7.fr-en.en'))]\n",
    "pairs = [*map(lambda x: {'fr': x[0], 'en': x[1]}, pairs)]\n",
    "print(f'Number of examples: {len(pairs):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples after sampling: 50,000\n",
      "Example:\n",
      "\tFR => Les procedures par le biais desquelles de tels produits entrent et sortent de l'Union europeenne doivent etre ouvertes, transparentes et, par dessus tout, sures.\n",
      "\tEN => The procedures whereby such products come in and out of the European Union have to be open, transparent and, above all, safe.\n",
      "CPU times: user 5.64 s, sys: 81.1 ms, total: 5.72 s\n",
      "Wall time: 5.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairs = np.random.choice(pairs, size=50000, replace=False)\n",
    "pairs = [*map(lambda pair: {k: normalize_string(v) for k, v in pair.items()},\n",
    "              pairs)]\n",
    "print(f'Number of examples after sampling: {len(pairs):,}')\n",
    "print(f'Example:\\n\\tFR => {pairs[0][\"fr\"]}\\n\\tEN => {pairs[0][\"en\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:16<00:00, 3049.27it/s]\n"
     ]
    }
   ],
   "source": [
    "FR = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='fr',\n",
    "           include_lengths=True) # For pack_padded_sequence\n",
    "EN = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='en')\n",
    "\n",
    "examples = [Example.fromdict(data=pair, fields={'fr': ('src', FR),\n",
    "                                                'en': ('dest', EN)})\n",
    "            for pair in tqdm.tqdm(pairs)]\n",
    "data = Dataset(examples, fields={'src': FR, 'dest': EN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 40000\n",
      "valid set size: 5000\n",
      "test set size: 5000\n",
      "{'src': ['je', 'demande', 'a', 'la', 'banque', 'mondiale', 'de', 'reorienter', 'sa', 'strategie', 'actuelle', 'qui', 'est', 'actuellement', 'un', 'modele', 'energetique', 'a', 'grande', 'echelle', 'et', 'axe', 'sur', \"l'\", 'exportation', 'pour', 'privilegier', 'des', 'projets', 'energetiques', 'decentralises', 'a', 'petite', 'echelle', 'qui', 'repondent', 'plus', 'clairement', 'aux', 'besoins', 'fondamentaux', 'des', 'zones', 'rurales', '.'], 'dest': ['i', 'call', 'on', 'the', 'world', 'bank', 'to', 'reorient', 'its', 'current', 'strategy', 'from', 'a', 'large', '-', 'scale', ',', 'export', '-', 'oriented', 'energy', 'model', 'to', 'small', '-', 'scale', ',', 'decentralised', 'energy', 'projects', 'that', 'respond', 'more', 'clearly', 'to', 'basic', 'needs', 'in', 'rural', 'areas', '.']}\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = data.split(split_ratio=[0.8, 0.1, 0.1])\n",
    "print(f'train set size: {len(train_data.examples)}')\n",
    "print(f'valid set size: {len(valid_data.examples)}')\n",
    "print(f'test set size: {len(test_data.examples)}')\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of FR vocabulary: 17,426\n",
      "Length of EN vocabulary: 13,638\n",
      "CPU times: user 715 ms, sys: 7.66 ms, total: 723 ms\n",
      "Wall time: 721 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FR.build_vocab(train_data,\n",
    "               min_freq=2,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "EN.build_vocab(train_data,\n",
    "               min_freq=2,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "\n",
    "print(f'Length of FR vocabulary: {len(FR.vocab):,}')\n",
    "print(f'Length of EN vocabulary: {len(EN.vocab):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, latent_size, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, latent_size)\n",
    "        \n",
    "    def forward(self, input_sequences, sequence_lengths):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_sequences: Tensor[seq_len, batch_size]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            \n",
    "        :return\n",
    "            outputs: Tensor[seq_len, batch_size, hidden_size * num_directions=2]\n",
    "            hn: Tensor[batch_size, latent_size]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_sequences)\n",
    "        embedded = self.dropout(embedded)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, sequence_lengths)\n",
    "        outputs, hn = self.gru(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # hn: [n_layers=1 * num_directions=2, batch_size, hidden_size]\n",
    "        hn = torch.cat((hn[-2, :, :], hn[-1, :, :]), dim=1)\n",
    "        # hn: [batch_size, hidden_size * num_directions=2]\n",
    "        hn = torch.tanh(self.fc(hn))\n",
    "        return outputs, hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, latent_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2 + latent_size, latent_size)\n",
    "        self.V = nn.Linear(latent_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, h_state, enc_outputs, mask):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            h_state: Tensor[batch_size, latent_size]\n",
    "            enc_outputs: Tensor[seq_len, batch_size, hidden_size * 2]\n",
    "            mask: Tensor[batch_size, seq_len]\n",
    "            \n",
    "        :return\n",
    "            attn_weights: Tensor[batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        seq_len, _, _ = enc_outputs.size()\n",
    "        h_state = h_state.unsqueeze(0).repeat(seq_len, 1, 1)\n",
    "        # h_state: [seq_len, batch_size, latent_size]\n",
    "        concat = torch.cat((enc_outputs, h_state), dim=2)\n",
    "        # concat: [seq_len, batch_size, hidden_state * 2 + latent_size]\n",
    "        attn_energies = torch.tanh(self.attn(concat))\n",
    "        # attn_energies: [seq_len, batch_size, latent_size]\n",
    "        attn = self.V(attn_energies).squeeze(2).permute(1, 0)\n",
    "        # attn: [batch_size, seq_len]\n",
    "        attn = attn.masked_fill(mask == 0, -1e10)\n",
    "        attn_weights = F.softmax(attn, dim=1)\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, enc_hidden_size, attention, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.attention = attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(enc_hidden_size * 2 + embedding_size, hidden_size)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2 + hidden_size + embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input_word_index, h_state, enc_outputs, mask):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_word_index: Tensor[batch_size,]\n",
    "            h_state: Tensor[batch_size, hidden_size]\n",
    "            enc_outputs: Tensor[seq_len, batch_size, enc_hidden_size * 2]\n",
    "            mask: Tensor[batch_size, seq_len]\n",
    "        \n",
    "        :return\n",
    "            prediction: Tensor[batch_size, vocab_size]\n",
    "            h_state: Tensor[batch_size, hidden_size]\n",
    "            attn_weights: Tensor[batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_word_index.unsqueeze(0))\n",
    "        # embedded: [1, batch_size, embedding_size]\n",
    "        \n",
    "        attn_weights = self.attention(h_state, enc_outputs, mask)\n",
    "        # attn_weights: [batch_size, seq_len]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs.permute(1, 0, 2))\n",
    "        # [batch_size, 1, seq_len] * [batch_size, seq_len, enc_hidden_size * 2]\n",
    "        # context: [batch_size, 1, enc_hidden_size * 2]\n",
    "        context = context.permute(1, 0, 2)\n",
    "        # context: [1, batch_size, enc_hidden_size * 2]\n",
    "        concat = torch.cat((embedded, context), dim=2)\n",
    "        # concat: [1, batch_size, embedding_size + enc_hidden_size * 2]\n",
    "        \n",
    "        output, h_state = self.gru(concat, h_state.unsqueeze(0))\n",
    "        # output: [1, batch_size, hidden_size]\n",
    "        # h_state: [n_layers=1 * num_directions=1, batch_size, hidden_size]\n",
    "        # assert (output == h_state).all()\n",
    "        \n",
    "        concat = torch.cat((context, output, embedded), dim=2)\n",
    "        # concat: [1, batch_size, enc_hidden_size * 2 + hidden_size + embedding_size]\n",
    "        prediction = self.fc(concat.squeeze(0))\n",
    "        \n",
    "        return prediction, h_state.squeeze(0), attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToSeqNet(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, pad_index, device):\n",
    "        super(SeqToSeqNet, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_index = pad_index\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, input_sequences):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_sequences: Tensor[seq_len, batch_size]\n",
    "            \n",
    "        :return\n",
    "            mask: Tensor[batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        mask = (input_sequences != self.pad_index).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, input_sequences, sequence_lengths, target_sequences, tf_ratio):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_sequences: Tensor[seq_len, batch_size]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            target_sequences: Tensor[seq_len, batch_size]\n",
    "            tf_ratio: float\n",
    "            \n",
    "        :return\n",
    "            outputs: Tensor[seq_len - 1, batch_size, vocab_size]\n",
    "                Since we ignore the SOS_TOKEN\n",
    "        \"\"\"\n",
    "        mask = self.create_mask(input_sequences)\n",
    "        \n",
    "        enc_outputs, h_state = self.encoder(input_sequences, sequence_lengths)\n",
    "        \n",
    "        seq_len, batch_size = target_sequences.size()\n",
    "        outputs = torch.zeros(seq_len - 1, batch_size, self.decoder.vocab_size).to(self.device)\n",
    "        word_index = target_sequences[0, :]\n",
    "        \n",
    "        for t, idx in enumerate(range(1, seq_len)):\n",
    "            output, h_state, attn_weights = self.decoder(word_index, h_state, enc_outputs, mask)\n",
    "            # output: [batch_size, vocab_size]\n",
    "            outputs[t] = output\n",
    "            \n",
    "            if random.random() < tf_ratio:\n",
    "                word_index = target_sequences[idx, :]\n",
    "            else:\n",
    "                word_index = output.argmax(dim=1)\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, loss_func, tf_ratio, data_it, grad_clip, epoch_text):\n",
    "    epoch_loss = 0.\n",
    "    pbar = tqdm.tqdm(enumerate(data_it), total=len(data_it))\n",
    "    model.train()\n",
    "    for i, data in pbar:\n",
    "        opt.zero_grad()\n",
    "        outputs = model(*data.src, data.dest, tf_ratio)\n",
    "        # *data.src: unpack input_sequences and sequence_lengths\n",
    "        loss = loss_func(outputs.view(-1, model.decoder.vocab_size), data.dest[1:].view(-1))\n",
    "        loss.backward()\n",
    "        # plot_gradient_flow(model.named_parameters())\n",
    "        if grad_clip:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_description(epoch_text + f'loss:     {epoch_loss / (i + 1):.3f} - ppl:     {np.exp(epoch_loss / (i + 1)):7.3f}')\n",
    "    # plt.show() # Show the gradient flow\n",
    "    return epoch_loss / len(data_it), np.exp(epoch_loss / len(data_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, loss_func, data_it, epoch_text):\n",
    "    epoch_loss = 0.\n",
    "    pbar = tqdm.tqdm(enumerate(data_it), total=len(data_it))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in pbar:\n",
    "            outputs = model(*data.src, data.dest, 0) # Turn-off the teacher forcing\n",
    "            loss = loss_func(outputs.view(-1, model.decoder.vocab_size), data.dest[1:].view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_description(epoch_text + f'val_loss: {epoch_loss / (i + 1):.3f} - val_ppl: {np.exp(epoch_loss / (i + 1)):7.3f}')\n",
    "    return epoch_loss / len(data_it), np.exp(epoch_loss / len(data_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, loss_func, train_it, valid_it, tf_ratio, n_epochs, grad_clip, save_to, filename):\n",
    "    assert callable(loss_func)\n",
    "    if not os.path.exists(save_to):\n",
    "        !mkdir {save_to}\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'val_loss': [],\n",
    "        'ppl': [],\n",
    "        'val_ppl': []\n",
    "    }\n",
    "    best_loss = [float('inf')]\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_text = f'Epoch: {epoch + 1:02d} - '\n",
    "        loss, ppl = train_epoch(model=model, opt=opt, loss_func=loss_func, tf_ratio=tf_ratio,\n",
    "                                data_it=train_it, grad_clip=grad_clip, epoch_text=epoch_text)\n",
    "        val_loss, val_ppl = valid_epoch(model=model, loss_func=loss_func, data_it=valid_it, epoch_text=epoch_text)\n",
    "        \n",
    "        history['loss'].append(loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['ppl'].append(ppl)\n",
    "        history['val_ppl'].append(val_ppl)\n",
    "        \n",
    "        # Saving\n",
    "        if val_loss < best_loss[-1]:\n",
    "            best_loss.append(val_loss)\n",
    "            torch.save({\n",
    "                'model': model.state_dict()\n",
    "            }, f=os.path.join(save_to, filename))\n",
    "            \n",
    "        # Stop training\n",
    "        try:\n",
    "            if val_loss > best_loss[-2]:\n",
    "                print('Stop training because the loss is increasing!')\n",
    "                break\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_SIZE = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.25\n",
    "GRAD_CLIP = 1.0\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 15\n",
    "TF_RATIO = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True, # For pack_padded_sequence\n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model: 41,008,238\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(embedding_size=EMBEDDING_DIM,\n",
    "                  vocab_size=len(FR.vocab),\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  latent_size=HIDDEN_SIZE,\n",
    "                  dropout=DROPOUT).to(device)\n",
    "attention = Attention(hidden_size=HIDDEN_SIZE,\n",
    "                      latent_size=HIDDEN_SIZE).to(device)\n",
    "decoder = Decoder(embedding_size=EMBEDDING_DIM,\n",
    "                  vocab_size=len(EN.vocab),\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  enc_hidden_size=HIDDEN_SIZE,\n",
    "                  attention=attention,\n",
    "                  dropout=DROPOUT).to(device)\n",
    "seq2seq = SeqToSeqNet(encoder=encoder,\n",
    "                      decoder=decoder,\n",
    "                      pad_index=FR.vocab.stoi[FR.pad_token],\n",
    "                      device=device).to(device)\n",
    "seq2seq.apply(init_weights)\n",
    "opt_adam = optim.Adam(seq2seq.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=EN.vocab.stoi[EN.pad_token])\n",
    "print(f'Number of parameters of the model: {count_parameters(seq2seq):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 01 - loss:     5.520 - ppl:     249.569: 100%|██████████| 2500/2500 [11:23<00:00,  3.66it/s]\n",
      "Epoch: 01 - val_loss: 4.799 - val_ppl: 121.431: 100%|██████████| 313/313 [00:22<00:00, 14.09it/s]\n",
      "Epoch: 02 - loss:     4.507 - ppl:      90.647: 100%|██████████| 2500/2500 [11:24<00:00,  3.65it/s]\n",
      "Epoch: 02 - val_loss: 4.393 - val_ppl:  80.871: 100%|██████████| 313/313 [00:22<00:00, 13.94it/s]\n",
      "Epoch: 03 - loss:     3.947 - ppl:      51.784: 100%|██████████| 2500/2500 [11:26<00:00,  3.64it/s]\n",
      "Epoch: 03 - val_loss: 4.313 - val_ppl:  74.693: 100%|██████████| 313/313 [00:22<00:00, 13.88it/s]\n",
      "Epoch: 04 - loss:     3.565 - ppl:      35.352: 100%|██████████| 2500/2500 [11:28<00:00,  3.63it/s]\n",
      "Epoch: 04 - val_loss: 4.333 - val_ppl:  76.181: 100%|██████████| 313/313 [00:22<00:00, 14.04it/s]\n",
      "Epoch: 05 - loss:     3.336 - ppl:      28.107: 100%|██████████| 2500/2500 [11:24<00:00,  3.65it/s]\n",
      "Epoch: 05 - val_loss: 4.389 - val_ppl:  80.565: 100%|██████████| 313/313 [00:22<00:00, 13.94it/s]\n",
      "Epoch: 06 - loss:     3.181 - ppl:      24.071: 100%|██████████| 2500/2500 [11:26<00:00,  3.64it/s]\n",
      "Epoch: 06 - val_loss: 4.437 - val_ppl:  84.533: 100%|██████████| 313/313 [00:22<00:00, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because the loss is increasing!\n",
      "CPU times: user 59min 10s, sys: 19min 56s, total: 1h 19min 6s\n",
      "Wall time: 1h 10min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train(model=seq2seq,\n",
    "                opt=opt_adam,\n",
    "                loss_func=criterion,\n",
    "                train_it=train_iterator,\n",
    "                valid_it=valid_iterator,\n",
    "                tf_ratio=TF_RATIO,\n",
    "                n_epochs=N_EPOCHS,\n",
    "                grad_clip=GRAD_CLIP,\n",
    "                save_to='./saved_models',\n",
    "                filename='seq2seq-attn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
