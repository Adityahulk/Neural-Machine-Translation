{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install & import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext --upgrade > /dev/null 2>&1\n",
    "!pip install spacy > /dev/null 2>&1\n",
    "!python -m spacy download fr > /dev/null 2>&1\n",
    "!python -m spacy download en > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Example, Field, Dataset\n",
    "from torchtext.data.iterator import BucketIterator\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 781\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data'):\n",
    "    !mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!wget --no-check-certificate \\\n",
    "    http://www.statmt.org/europarl/v7/fr-en.tgz \\\n",
    "    -O ./data/fr-en.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!tar -xzvf ./data/fr-en.tgz -C ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, mode='rt', encoding='utf-8') as file:\n",
    "            content = file.readlines()\n",
    "        return content\n",
    "    except:\n",
    "        raise Error(f'File {filepath} doesn\\'t exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    # NFD => Normal Form Decompose\n",
    "    # Mn => Non Marking Space\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) \\\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    # Transform accented characters into unaccented ones\n",
    "    s = unicode_to_ascii(s.strip())\n",
    "    # Replace any of '.', '!', '?' by ' .', ' !', ' ?'. \\1 means the 1st bracked group. r is to not consider \\1\n",
    "    s = re.sub(r'([,.!?0-9])', r' \\1', s)\n",
    "    # Remove any character which is not in [^a-zA-Z0-9,.!?]\n",
    "    s = re.sub(r'[^a-zA-Z0-9,.!?]', r' ', s)\n",
    "    # Remove a sequence of whitespace characters\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_string('L\\'année 2019 était fabuleuse!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pairs = [*zip(read_file('./data/europarl-v7.fr-en.fr'),\n",
    "              read_file('./data/europarl-v7.fr-en.en'))]\n",
    "pairs = [*map(lambda x: {'fr': x[0], 'en': x[1]}, pairs)]\n",
    "print(f'Number of examples: {len(pairs):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not build models the entire dataset, since is very large. Instead, I sample a subset of 100,000 sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pairs = np.random.choice(pairs, size=200000, replace=False)\n",
    "pairs = [*map(lambda pair: {k: normalize_string(v) for k, v in pair.items()},\n",
    "              pairs)]\n",
    "print(f'Number of examples after sampling: {len(pairs):,}')\n",
    "print(f'Example:\\n\\tFR => {pairs[0][\"fr\"]}\\n\\tEN => {pairs[0][\"en\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split data in train, valid and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           preprocessing=lambda x: x[::-1],\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='fr',\n",
    "           include_lengths=True) # For pack_padded_sequence\n",
    "EN = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='en')\n",
    "\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "examples = [Example.fromdict(data=pair, fields={'fr': ('src', FR),\n",
    "                                                'en': ('dest', EN)})\n",
    "            for pair in tqdm.tqdm(pairs)]\n",
    "examples = [*filter(lambda example: len(example.src) <= MAX_LENGTH and len(example.dest) <= MAX_LENGTH, examples)]\n",
    "print(f'Number of examples after filtering: {len(examples):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(examples, fields={'src': FR, 'dest': EN})\n",
    "train_data, valid_data, test_data = data.split(split_ratio=[0.8, 0.1, 0.1])\n",
    "print(f'train set size: {len(train_data.examples)}')\n",
    "print(f'valid set size: {len(valid_data.examples)}')\n",
    "print(f'test set size: {len(test_data.examples)}')\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model understands only number, we need to transform text sequences into sequence of numbers where each numbers represents a unique token. To do this, we build a vocabulary for each language that map words to indexes and vice versa. the vocabulary id built from train set only in order to prevent data leakage. We also add some special tokens:\n",
    "- `<sos>`: for start of sentence.\n",
    "- `<unk>`: for unknown or less frequent words.\n",
    "- `<eos>`: for end of sentence. \n",
    "- `<pad>`: for padding (make all sentences in a batch the same size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MIN_COUNT = 5\n",
    "FR.build_vocab(train_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "EN.build_vocab(train_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "\n",
    "print(f'Length of FR vocabulary: {len(FR.vocab):,}')\n",
    "print(f'Length of EN vocabulary: {len(EN.vocab):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "The goal is to find the best english sentence $y$ that maximize the likelihood given a french sentence $x$, $P(y|x)$. To do this, we trained a neural probabilistic sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The part of model map the source sequence to hidden vector (encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, n_layers, dropout, recurrent_dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=(recurrent_dropout if n_layers > 1 else 0))\n",
    "        \n",
    "    def forward(self, input_sequences, sequence_lengths):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_sequences: Tensor[seq_len, batch_size]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            \n",
    "        :return\n",
    "            outputs: Tensor[seq_len, batch_size, hidden_size]\n",
    "            hn: Tensor[num_layers, batch_size, hidden_size]\n",
    "            cn: Tensor[num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_sequences)\n",
    "        embedded = self.dropout(embedded)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, sequence_lengths)\n",
    "        outputs, (hn, cn) = self.lstm(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        return outputs, hn, cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The part of model performs a conditional language modeling given the hidden vector outputs by the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, n_layers, dropout, recurrent_dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=(recurrent_dropout if n_layers > 1 else 0))\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input_word_index, h_state, c_state):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_word_index: Tensor[batch_size,]\n",
    "            h_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            c_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            \n",
    "        :return\n",
    "            logit: Tensor[batch_size, vocab_size]\n",
    "            h_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            c_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_word_index.unsqueeze(0))\n",
    "        embedded = self.dropout(embedded)\n",
    "        outputs, (h_state, c_state) = self.lstm(embedded, (h_state, c_state))\n",
    "        # outputs: [1, batch_size, hidden_size]\n",
    "        logit = self.fc(outputs.squeeze(0))\n",
    "        return logit, h_state, c_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to sequence model\n",
    "\n",
    "This puts encoder and decoder together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToSeqNet(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        assert encoder.n_layers == decoder.n_layers, 'Encoder and Decoder must have the same number of reccurent layers'\n",
    "        assert encoder.hidden_size == decoder.hidden_size, 'Encoder and Decoder must have the same number of reccurrent hidden units'\n",
    "\n",
    "        super(SeqToSeqNet, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, input_sequences, sequence_lengths, target_sequences, tf_ratio):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_sequences: Tensor[seq_len, batch_size]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            target_sequences: Tensor[seq_len, batch_size]\n",
    "            tf_ratio: float\n",
    "            \n",
    "        :return\n",
    "            outputs: Tensor[seq_len - 1, batch_size, vocab_size]\n",
    "                Since we ignore the SOS_TOKEN\n",
    "        \"\"\"\n",
    "        _, h_state, c_state = self.encoder(input_sequences, sequence_lengths)\n",
    "        \n",
    "        seq_len, batch_size = target_sequences.size()\n",
    "        outputs = torch.zeros(seq_len - 1, batch_size, self.decoder.vocab_size).to(self.device)\n",
    "        word_index = target_sequences[0, :]\n",
    "        \n",
    "        for t, idx in enumerate(range(1, seq_len)):\n",
    "            output, h_state, c_state = self.decoder(word_index, h_state, c_state)\n",
    "            # output: [batch_size, vocab_size]\n",
    "            outputs[t] = output\n",
    "            \n",
    "            if random.random() < tf_ratio:\n",
    "                word_index = target_sequences[idx, :]\n",
    "            else:\n",
    "                word_index = output.argmax(dim=1)\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient_flow(named_parameters):\n",
    "    grad_mean, layers = [], []\n",
    "    for name, param in named_parameters:\n",
    "        if param.requires_grad and 'bias' not in name:\n",
    "            layers.append(name)\n",
    "            grad_mean.append(param.grad.abs().mean())\n",
    "    plt.plot(grad_mean, alpha=0.3, color='b')\n",
    "    plt.hlines(0, 0, len(grad_mean) + 1, linewidth=1, color='k' )\n",
    "    plt.xticks(range(0, len(grad_mean), 1), layers, rotation='vertical')\n",
    "    plt.xlim(xmin=0, xmax=len(grad_mean))\n",
    "    # plt.ylim(bottom=-0.001, top=0.02) # Zoom on lower gradients\n",
    "    plt.xlabel('Layers')\n",
    "    plt.ylabel('Mean of gradients')\n",
    "    plt.title('Gradient Flow')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, loss_func, tf_ratio, data_it, grad_clip, epoch_text):\n",
    "    epoch_loss = 0.\n",
    "    pbar = tqdm.tqdm(enumerate(data_it), total=len(data_it))\n",
    "    model.train()\n",
    "    for i, data in pbar:\n",
    "        opt.zero_grad()\n",
    "        outputs = model(*data.src, data.dest, tf_ratio)\n",
    "        # *data.src: unpack input_sequences and sequence_lengths\n",
    "        loss = loss_func(outputs.view(-1, model.decoder.vocab_size), data.dest[1:].view(-1))\n",
    "        loss.backward()\n",
    "        # plot_gradient_flow(model.named_parameters())\n",
    "        if grad_clip:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_description(epoch_text + f'loss:     {epoch_loss / (i + 1):.3f} - ppl:     {np.exp(epoch_loss / (i + 1)):7.3f}')\n",
    "    # plt.show() # Show the gradient flow\n",
    "    return epoch_loss / len(data_it), np.exp(epoch_loss / len(data_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, loss_func, data_it, epoch_text):\n",
    "    epoch_loss = 0.\n",
    "    pbar = tqdm.tqdm(enumerate(data_it), total=len(data_it))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in pbar:\n",
    "            outputs = model(*data.src, data.dest, 0) # Turn-off the teacher forcing\n",
    "            loss = loss_func(outputs.view(-1, model.decoder.vocab_size), data.dest[1:].view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_description(epoch_text + f'val_loss: {epoch_loss / (i + 1):.3f} - val_ppl: {np.exp(epoch_loss / (i + 1)):7.3f}')\n",
    "    return epoch_loss / len(data_it), np.exp(epoch_loss / len(data_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, loss_func, train_it, valid_it, tf_ratio, n_epochs, grad_clip, save_to, filename):\n",
    "    assert callable(loss_func)\n",
    "    if not os.path.exists(save_to):\n",
    "        !mkdir {save_to}\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'val_loss': [],\n",
    "        'ppl': [],\n",
    "        'val_ppl': []\n",
    "    }\n",
    "    best_loss = [float('inf')]\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_text = f'Epoch: {epoch + 1:02d} - '\n",
    "        loss, ppl = train_epoch(model=model, opt=opt, loss_func=loss_func, tf_ratio=tf_ratio,\n",
    "                                data_it=train_it, grad_clip=grad_clip, epoch_text=epoch_text)\n",
    "        val_loss, val_ppl = valid_epoch(model=model, loss_func=loss_func, data_it=valid_it, epoch_text=epoch_text)\n",
    "        \n",
    "        history['loss'].append(loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['ppl'].append(ppl)\n",
    "        history['val_ppl'].append(val_ppl)\n",
    "        \n",
    "        # Reduce tf_ratio\n",
    "        tf_ratio = tf_ratio - tf_ratio / (epoch + 2)\n",
    "        \n",
    "        # Saving\n",
    "        if val_loss < best_loss[-1]:\n",
    "            best_loss.append(val_loss)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': opt.state_dict()\n",
    "            }, f=os.path.join(save_to, filename))\n",
    "            \n",
    "        # Stop training\n",
    "        try:\n",
    "            if val_loss > best_loss[-2]:\n",
    "                print('Stop training because the loss is increasing!')\n",
    "                break\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 256\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.15\n",
    "RECURRENT_DROPOUT = 0.35\n",
    "GRAD_CLIP = 1.0\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 15\n",
    "TF_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True, # For pack_padded_sequence\n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embedding_size=EMBEDDING_DIM,\n",
    "                  vocab_size=len(FR.vocab),\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  n_layers=N_LAYERS,\n",
    "                  dropout=DROPOUT,\n",
    "                  recurrent_dropout=RECURRENT_DROPOUT).to(device)\n",
    "decoder = Decoder(embedding_size=EMBEDDING_DIM,\n",
    "                  vocab_size=len(EN.vocab),\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  n_layers=N_LAYERS,\n",
    "                  dropout=DROPOUT,\n",
    "                  recurrent_dropout=RECURRENT_DROPOUT).to(device)\n",
    "seq2seq = SeqToSeqNet(encoder=encoder,\n",
    "                      decoder=decoder,\n",
    "                      device=device).to(device)\n",
    "opt_adam = optim.Adam(seq2seq.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=EN.vocab.stoi[EN.pad_token])\n",
    "print(f'Number of parameters of the model: {count_parameters(seq2seq):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = train(model=seq2seq,\n",
    "                opt=opt_adam,\n",
    "                loss_func=criterion,\n",
    "                train_it=train_iterator,\n",
    "                valid_it=valid_iterator,\n",
    "                tf_ratio=TF_RATIO,\n",
    "                n_epochs=N_EPOCHS,\n",
    "                grad_clip=GRAD_CLIP,\n",
    "                save_to='./saved_models',\n",
    "                filename='seq2seq-baseline-lstm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(history['loss'], label='train')\n",
    "axes[0].plot(history['val_loss'], label='valid')\n",
    "axes[0].set_title('Loss history')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(history['ppl'], label='train')\n",
    "axes[1].plot(history['val_ppl'], label='valid')\n",
    "axes[1].set_title('Perplexity history')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Perplexity')\n",
    "axes[1].grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq.load_state_dict(torch.load('./saved_models/seq2seq-baseline-lstm.pt').get('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_ppl = valid_epoch(model=seq2seq,\n",
    "                                loss_func=criterion,\n",
    "                                data_it=test_iterator,\n",
    "                                epoch_text='Evaluation on test set - ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    \n",
    "    def __init__(self, model, src_field, dest_field, normalizer, max_length, device):\n",
    "        assert callable(normalizer)\n",
    "        \n",
    "        self.model = model\n",
    "        self.src_field = src_field\n",
    "        self.dest_field = dest_field\n",
    "        self.normalizer = normalizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        sentence = self.normalizer(sentence)\n",
    "        sentence = self.src_field.preprocess(sentence)\n",
    "        input_sequence, sequence_length = self.src_field.process([sentence])\n",
    "        input_sequence = input_sequence.to(self.device)\n",
    "        sequence_length = sequence_length.to(self.device)\n",
    "        _, h_state, c_state = self.model.encoder(input_sequence, sequence_length)\n",
    "        return _, h_state, c_state\n",
    "    \n",
    "    def greedy(self, sentence):\n",
    "        \"\"\"\n",
    "        Translate using Greedy method\n",
    "        \n",
    "        :param\n",
    "            sentence: str\n",
    "            \n",
    "        :return \n",
    "        \"\"\"\n",
    "        _, h_state, c_state = self.encode(sentence)\n",
    "        outputs, logp = [], 0\n",
    "        word_index = torch.tensor([self.dest_field.vocab.stoi[self.dest_field.init_token]], device=self.device)\n",
    "        for _ in range(self.max_length):\n",
    "            output, h_state, c_state = self.model.decoder(word_index, h_state, c_state)\n",
    "            probas = F.softmax(output, dim=1)\n",
    "            proba, word_index = torch.topk(probas, k=1, dim=1)\n",
    "            word_index = word_index.squeeze(0)\n",
    "            if word_index.detach().item() == self.dest_field.vocab.stoi[self.dest_field.eos_token]:\n",
    "                break\n",
    "            outputs.append(word_index.detach().item())\n",
    "            logp +=  np.log(proba.detach().item())\n",
    "        return ' '.join([*map(self.dest_field.vocab.itos.__getitem__, outputs)]), logp\n",
    "    \n",
    "    def beam_search(self, sentence, beam_size):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Inference(model=seq2seq,\n",
    "                       src_field=FR,\n",
    "                       dest_field=EN,\n",
    "                       normalizer=normalize_string,\n",
    "                       max_length=MAX_LENGTH,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.greedy('Le parlement européen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.random.choice(len(test_data.examples), 10)\n",
    "print(indexes)\n",
    "print()\n",
    "for i in indexes:\n",
    "    s = ' '.join(test_data.examples[i].src[::-1])\n",
    "    d = ' '.join(test_data.examples[i].dest)\n",
    "    print(s)\n",
    "    print('-'*100)\n",
    "    print(translator.greedy(s))\n",
    "    print('-'*100)\n",
    "    print(d)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
