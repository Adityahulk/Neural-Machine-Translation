{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 15 17:04:17 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 26%   30C    P8     6W / 180W |      1MiB / 16278MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install & import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext --upgrade > /dev/null 2>&1\n",
    "!pip install spacy > /dev/null 2>&1\n",
    "!python -m spacy download fr > /dev/null 2>&1\n",
    "!python -m spacy download en > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Example, Field, Dataset\n",
    "from torchtext.data.iterator import BucketIterator\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 781\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data'):\n",
    "    !mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-06 16:33:35--  http://www.statmt.org/europarl/v7/fr-en.tgz\n",
      "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
      "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 202718517 (193M) [application/x-gzip]\n",
      "Saving to: ‘./data/fr-en.tgz’\n",
      "\n",
      "./data/fr-en.tgz    100%[===================>] 193.33M  32.5KB/s    in 25m 0s  \n",
      "\n",
      "2020-04-06 16:58:35 (132 KB/s) - ‘./data/fr-en.tgz’ saved [202718517/202718517]\n",
      "\n",
      "CPU times: user 23.2 s, sys: 5.91 s, total: 29.1 s\n",
      "Wall time: 25min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wget --no-check-certificate \\\n",
    "    http://www.statmt.org/europarl/v7/fr-en.tgz \\\n",
    "    -O ./data/fr-en.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europarl-v7.fr-en.en\n",
      "europarl-v7.fr-en.fr\n",
      "CPU times: user 51.3 ms, sys: 42 ms, total: 93.3 ms\n",
      "Wall time: 5.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!tar -xzvf ./data/fr-en.tgz -C ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, mode='rt', encoding='utf-8') as file:\n",
    "            content = file.readlines()\n",
    "        return content\n",
    "    except:\n",
    "        raise Error(f'File {filepath} doesn\\'t exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    # NFD => Normal Form Decompose\n",
    "    # Mn => Non Marking Space\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) \\\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    # Transform accented characters into unaccented ones\n",
    "    s = unicode_to_ascii(s.strip())\n",
    "    # Replace any of '.', '!', '?' by ' .', ' !', ' ?'. \\1 means the 1st bracked group. r is to not consider \\1\n",
    "    s = re.sub(r'([,.!?0-9])', r' \\1', s)\n",
    "    # Remove any character which is not in [^a-zA-Z0-9,.!?]\n",
    "    s = re.sub(r'[^a-zA-Z0-9,.!?]', r' ', s)\n",
    "    # Remove a sequence of whitespace characters\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L annee 2 0 1 9 etait fabuleuse !'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_string('L\\'année 2019 était fabuleuse!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2,007,723\n",
      "CPU times: user 3.49 s, sys: 734 ms, total: 4.22 s\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairs = [*zip(read_file('./data/europarl-v7.fr-en.fr'),\n",
    "              read_file('./data/europarl-v7.fr-en.en'))]\n",
    "pairs = [*map(lambda x: {'fr': x[0], 'en': x[1]}, pairs)]\n",
    "print(f'Number of examples: {len(pairs):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not build models the entire dataset, since is very large. Instead, I sample a subset of 100,000 sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples after sampling: 100,000\n",
      "Example:\n",
      "\tFR => Il est tendancieux et ne reflete pas la situation de menace reelle associee au trafic illegal de substances nucleaires au sein de l Union europeenne .\n",
      "\tEN => It is tendentious and does not match up to the real threat posed by illicit traffic in nuclear materials in the European Union .\n",
      "CPU times: user 13.9 s, sys: 59.1 ms, total: 14 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairs = np.random.choice(pairs, size=100000, replace=False)\n",
    "pairs = [*map(lambda pair: {k: normalize_string(v) for k, v in pair.items()},\n",
    "              pairs)]\n",
    "print(f'Number of examples after sampling: {len(pairs):,}')\n",
    "print(f'Example:\\n\\tFR => {pairs[0][\"fr\"]}\\n\\tEN => {pairs[0][\"en\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split data in train, valid and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:30<00:00, 3318.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples after filtering: 56,089\n"
     ]
    }
   ],
   "source": [
    "FR = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           preprocessing=lambda x: x[::-1],\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='fr',\n",
    "           include_lengths=True) # For pack_padded_sequence\n",
    "EN = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='en')\n",
    "\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "examples = [Example.fromdict(data=pair, fields={'fr': ('src', FR),\n",
    "                                                'en': ('dest', EN)})\n",
    "            for pair in tqdm.tqdm(pairs)]\n",
    "examples = [*filter(lambda example: len(example.src) <= MAX_LENGTH and len(example.dest) <= MAX_LENGTH, examples)]\n",
    "print(f'Number of examples after filtering: {len(examples):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 44871\n",
      "valid set size: 5609\n",
      "test set size: 5609\n",
      "{'src': ['.', 'europeenne', 'integration', 'd', 'processus', 'au', 'peu', 'tres', 'identifient', 's', 'europeens', 'citoyens', 'les', 'que', 'fait', 'le', 'sur', 'porte', 'longuement', 'a', 'debat', 'le'], 'dest': ['much', 'was', 'said', 'during', 'the', 'debate', 'on', 'the', 'programme', 'about', 'how', 'little', 'the', 'citizens', 'identify', 'with', 'the', 'process', 'of', 'european', 'integration', '.']}\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(examples, fields={'src': FR, 'dest': EN})\n",
    "train_data, valid_data, test_data = data.split(split_ratio=[0.8, 0.1, 0.1])\n",
    "print(f'train set size: {len(train_data.examples)}')\n",
    "print(f'valid set size: {len(valid_data.examples)}')\n",
    "print(f'test set size: {len(test_data.examples)}')\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model understands only number, we need to transform text sequences into sequence of numbers where each numbers represents a unique token. To do this, we build a vocabulary for each language that map words to indexes and vice versa. the vocabulary id built from train set only in order to prevent data leakage. We also add some special tokens:\n",
    "- `<sos>`: for start of sentence.\n",
    "- `<unk>`: for unknown or less frequent words.\n",
    "- `<eos>`: for end of sentence. \n",
    "- `<pad>`: for padding (make all sentences in a batch the same size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of FR vocabulary: 7,726\n",
      "Length of EN vocabulary: 6,468\n",
      "CPU times: user 616 ms, sys: 0 ns, total: 616 ms\n",
      "Wall time: 616 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MIN_COUNT = 5\n",
    "FR.build_vocab(train_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "EN.build_vocab(train_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "\n",
    "print(f'Length of FR vocabulary: {len(FR.vocab):,}')\n",
    "print(f'Length of EN vocabulary: {len(EN.vocab):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "The goal is to find the best english sentence $y$ that maximize the likelihood given a french sentence $x$, $P(y|x)$. To do this, we trained a neural probabilistic sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The part of model map the source sequence to hidden vector (encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, n_layers, dropout, recurrent_dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=(recurrent_dropout if n_layers > 1 else 0))\n",
    "        \n",
    "    def forward(self, input_sequences, sequence_lengths):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_sequences: Tensor[seq_len, batch_size]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            \n",
    "        :return\n",
    "            outputs: Tensor[seq_len, batch_size, hidden_size]\n",
    "            hn: Tensor[num_layers, batch_size, hidden_size]\n",
    "            cn: Tensor[num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_sequences)\n",
    "        embedded = self.dropout(embedded)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, sequence_lengths)\n",
    "        outputs, (hn, cn) = self.lstm(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        return outputs, hn, cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The part of model performs a conditional language modeling given the hidden vector outputs by the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size, hidden_size, n_layers, dropout, recurrent_dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=(recurrent_dropout if n_layers > 1 else 0))\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input_word_index, h_state, c_state):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_word_index: Tensor[batch_size,]\n",
    "            h_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            c_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            \n",
    "        :return\n",
    "            logit: Tensor[batch_size, vocab_size]\n",
    "            h_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "            c_state: Tensor[num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_word_index.unsqueeze(0))\n",
    "        embedded = self.dropout(embedded)\n",
    "        outputs, (h_state, c_state) = self.lstm(embedded, (h_state, c_state))\n",
    "        # outputs: [1, batch_size, hidden_size]\n",
    "        logit = self.fc(outputs.squeeze(0))\n",
    "        return logit, h_state, c_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to sequence model\n",
    "\n",
    "This puts encoder and decoder together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToSeqNet(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        assert encoder.n_layers == decoder.n_layers, 'Encoder and Decoder must have the same number of reccurent layers'\n",
    "        assert encoder.hidden_size == decoder.hidden_size, 'Encoder and Decoder must have the same number of reccurrent hidden units'\n",
    "\n",
    "        super(SeqToSeqNet, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, input_sequences, sequence_lengths, target_sequences, tf_ratio):\n",
    "        \"\"\"\n",
    "        :params\n",
    "            input_sequences: Tensor[seq_len, batch_size]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            target_sequences: Tensor[seq_len, batch_size]\n",
    "            tf_ratio: float\n",
    "            \n",
    "        :return\n",
    "            outputs: Tensor[seq_len - 1, batch_size, vocab_size]\n",
    "                Since we ignore the SOS_TOKEN\n",
    "        \"\"\"\n",
    "        _, h_state, c_state = self.encoder(input_sequences, sequence_lengths)\n",
    "        \n",
    "        seq_len, batch_size = target_sequences.size()\n",
    "        outputs = torch.zeros(seq_len - 1, batch_size, self.decoder.vocab_size).to(self.device)\n",
    "        word_index = target_sequences[0, :]\n",
    "        \n",
    "        for t, idx in enumerate(range(1, seq_len)):\n",
    "            output, h_state, c_state = self.decoder(word_index, h_state, c_state)\n",
    "            # output: [batch_size, vocab_size]\n",
    "            outputs[t] = output\n",
    "            \n",
    "            if random.random() < tf_ratio:\n",
    "                word_index = target_sequences[idx, :]\n",
    "            else:\n",
    "                word_index = output.argmax(dim=1)\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient_flow(named_parameters):\n",
    "    grad_mean, layers = [], []\n",
    "    for name, param in named_parameters:\n",
    "        if param.requires_grad and 'bias' not in name:\n",
    "            layers.append(name)\n",
    "            grad_mean.append(param.grad.abs().mean())\n",
    "    plt.plot(grad_mean, alpha=0.3, color='b')\n",
    "    plt.hlines(0, 0, len(grad_mean) + 1, linewidth=1, color='k' )\n",
    "    plt.xticks(range(0, len(grad_mean), 1), layers, rotation='vertical')\n",
    "    plt.xlim(xmin=0, xmax=len(grad_mean))\n",
    "    # plt.ylim(bottom=-0.001, top=0.02) # Zoom on lower gradients\n",
    "    plt.xlabel('Layers')\n",
    "    plt.ylabel('Mean of gradients')\n",
    "    plt.title('Gradient Flow')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, loss_func, tf_ratio, data_it, grad_clip, epoch_text):\n",
    "    epoch_loss = 0.\n",
    "    pbar = tqdm.tqdm(enumerate(data_it), total=len(data_it))\n",
    "    model.train()\n",
    "    for i, data in pbar:\n",
    "        opt.zero_grad()\n",
    "        outputs = model(*data.src, data.dest, tf_ratio)\n",
    "        # *data.src: unpack input_sequences and sequence_lengths\n",
    "        loss = loss_func(outputs.view(-1, model.decoder.vocab_size), data.dest[1:].view(-1))\n",
    "        loss.backward()\n",
    "        # plot_gradient_flow(model.named_parameters())\n",
    "        if grad_clip:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_description(epoch_text + f'loss:     {epoch_loss / (i + 1):.3f} - ppl:     {np.exp(epoch_loss / (i + 1)):7.3f}')\n",
    "    # plt.show() # Show the gradient flow\n",
    "    return epoch_loss / len(data_it), np.exp(epoch_loss / len(data_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, loss_func, data_it, epoch_text):\n",
    "    epoch_loss = 0.\n",
    "    pbar = tqdm.tqdm(enumerate(data_it), total=len(data_it))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in pbar:\n",
    "            outputs = model(*data.src, data.dest, 0) # Turn-off the teacher forcing\n",
    "            loss = loss_func(outputs.view(-1, model.decoder.vocab_size), data.dest[1:].view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_description(epoch_text + f'val_loss: {epoch_loss / (i + 1):.3f} - val_ppl: {np.exp(epoch_loss / (i + 1)):7.3f}')\n",
    "    return epoch_loss / len(data_it), np.exp(epoch_loss / len(data_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, loss_func, train_it, valid_it, tf_ratio, n_epochs, grad_clip, save_to, filename):\n",
    "    assert callable(loss_func)\n",
    "    if not os.path.exists(save_to):\n",
    "        !mkdir {save_to}\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'val_loss': [],\n",
    "        'ppl': [],\n",
    "        'val_ppl': []\n",
    "    }\n",
    "    best_loss = [float('inf')]\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_text = f'Epoch: {epoch + 1:02d} - '\n",
    "        loss, ppl = train_epoch(model=model, opt=opt, loss_func=loss_func, tf_ratio=tf_ratio,\n",
    "                                data_it=train_it, grad_clip=grad_clip, epoch_text=epoch_text)\n",
    "        val_loss, val_ppl = valid_epoch(model=model, loss_func=loss_func, data_it=valid_it, epoch_text=epoch_text)\n",
    "        \n",
    "        history['loss'].append(loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['ppl'].append(ppl)\n",
    "        history['val_ppl'].append(val_ppl)\n",
    "        \n",
    "        # Reduce tf_ratio\n",
    "        tf_ratio = tf_ratio - tf_ratio / (epoch + 2)\n",
    "        \n",
    "        # Saving\n",
    "        if val_loss < best_loss[-1]:\n",
    "            best_loss.append(val_loss)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': opt.state_dict()\n",
    "            }, f=os.path.join(save_to, filename))\n",
    "            \n",
    "        # Stop training\n",
    "        try:\n",
    "            if val_loss > best_loss[-2]:\n",
    "                print('Stop training because the loss is increasing!')\n",
    "                break\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.15\n",
    "RECURRENT_DROPOUT = 0.35\n",
    "GRAD_CLIP = 1.0\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 15\n",
    "TF_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True, # For pack_padded_sequence\n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model: 5,322,308\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(embedding_size=EMBEDDING_DIM,\n",
    "                  vocab_size=len(FR.vocab),\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  n_layers=N_LAYERS,\n",
    "                  dropout=DROPOUT,\n",
    "                  recurrent_dropout=RECURRENT_DROPOUT).to(device)\n",
    "decoder = Decoder(embedding_size=EMBEDDING_DIM,\n",
    "                  vocab_size=len(EN.vocab),\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  n_layers=N_LAYERS,\n",
    "                  dropout=DROPOUT,\n",
    "                  recurrent_dropout=RECURRENT_DROPOUT).to(device)\n",
    "seq2seq = SeqToSeqNet(encoder=encoder,\n",
    "                      decoder=decoder,\n",
    "                      device=device).to(device)\n",
    "opt_adam = optim.Adam(seq2seq.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=EN.vocab.stoi[EN.pad_token])\n",
    "print(f'Number of parameters of the model: {count_parameters(seq2seq):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 01 - loss:     5.563 - ppl:     260.527: 100%|██████████| 702/702 [00:46<00:00, 15.12it/s]\n",
      "Epoch: 01 - val_loss: 5.393 - val_ppl: 219.772: 100%|██████████| 88/88 [00:02<00:00, 33.68it/s]\n",
      "Epoch: 02 - loss:     5.087 - ppl:     161.980: 100%|██████████| 702/702 [00:46<00:00, 15.06it/s]\n",
      "Epoch: 02 - val_loss: 5.057 - val_ppl: 157.169: 100%|██████████| 88/88 [00:02<00:00, 31.33it/s]\n",
      "Epoch: 03 - loss:     4.899 - ppl:     134.106: 100%|██████████| 702/702 [00:46<00:00, 15.06it/s]\n",
      "Epoch: 03 - val_loss: 4.919 - val_ppl: 136.814: 100%|██████████| 88/88 [00:02<00:00, 31.66it/s]\n",
      "Epoch: 04 - loss:     4.764 - ppl:     117.231: 100%|██████████| 702/702 [00:46<00:00, 15.21it/s]\n",
      "Epoch: 04 - val_loss: 4.793 - val_ppl: 120.659: 100%|██████████| 88/88 [00:02<00:00, 34.79it/s]\n",
      "Epoch: 05 - loss:     4.654 - ppl:     104.960: 100%|██████████| 702/702 [00:46<00:00, 15.00it/s]\n",
      "Epoch: 05 - val_loss: 4.719 - val_ppl: 112.024: 100%|██████████| 88/88 [00:02<00:00, 31.37it/s]\n",
      "Epoch: 06 - loss:     4.561 - ppl:      95.710: 100%|██████████| 702/702 [00:46<00:00, 15.24it/s]\n",
      "Epoch: 06 - val_loss: 4.654 - val_ppl: 104.958: 100%|██████████| 88/88 [00:02<00:00, 31.37it/s]\n",
      "Epoch: 07 - loss:     4.470 - ppl:      87.372: 100%|██████████| 702/702 [00:45<00:00, 15.33it/s]\n",
      "Epoch: 07 - val_loss: 4.605 - val_ppl:  99.973: 100%|██████████| 88/88 [00:02<00:00, 31.90it/s]\n",
      "Epoch: 08 - loss:     4.389 - ppl:      80.540: 100%|██████████| 702/702 [00:45<00:00, 15.54it/s]\n",
      "Epoch: 08 - val_loss: 4.563 - val_ppl:  95.835: 100%|██████████| 88/88 [00:02<00:00, 34.32it/s]\n",
      "Epoch: 09 - loss:     4.314 - ppl:      74.712: 100%|██████████| 702/702 [00:45<00:00, 15.39it/s]\n",
      "Epoch: 09 - val_loss: 4.535 - val_ppl:  93.202: 100%|██████████| 88/88 [00:02<00:00, 33.93it/s]\n",
      "Epoch: 10 - loss:     4.245 - ppl:      69.746: 100%|██████████| 702/702 [00:45<00:00, 15.49it/s]\n",
      "Epoch: 10 - val_loss: 4.513 - val_ppl:  91.205: 100%|██████████| 88/88 [00:02<00:00, 34.37it/s]\n",
      "Epoch: 11 - loss:     4.179 - ppl:      65.273: 100%|██████████| 702/702 [00:46<00:00, 15.11it/s]\n",
      "Epoch: 11 - val_loss: 4.486 - val_ppl:  88.759: 100%|██████████| 88/88 [00:02<00:00, 31.71it/s]\n",
      "Epoch: 12 - loss:     4.122 - ppl:      61.652: 100%|██████████| 702/702 [00:46<00:00, 15.09it/s]\n",
      "Epoch: 12 - val_loss: 4.478 - val_ppl:  88.028: 100%|██████████| 88/88 [00:02<00:00, 31.43it/s]\n",
      "Epoch: 13 - loss:     4.061 - ppl:      58.011: 100%|██████████| 702/702 [00:47<00:00, 14.84it/s]\n",
      "Epoch: 13 - val_loss: 4.480 - val_ppl:  88.256: 100%|██████████| 88/88 [00:02<00:00, 32.04it/s]\n",
      "Epoch: 14 - loss:     4.008 - ppl:      55.031: 100%|██████████| 702/702 [00:46<00:00, 15.18it/s]\n",
      "Epoch: 14 - val_loss: 4.463 - val_ppl:  86.753: 100%|██████████| 88/88 [00:02<00:00, 30.38it/s]\n",
      "Epoch: 15 - loss:     3.957 - ppl:      52.321: 100%|██████████| 702/702 [00:46<00:00, 15.07it/s]\n",
      "Epoch: 15 - val_loss: 4.456 - val_ppl:  86.163: 100%|██████████| 88/88 [00:02<00:00, 30.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 24s, sys: 5min 2s, total: 20min 27s\n",
      "Wall time: 12min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train(model=seq2seq,\n",
    "                opt=opt_adam,\n",
    "                loss_func=criterion,\n",
    "                train_it=train_iterator,\n",
    "                valid_it=valid_iterator,\n",
    "                tf_ratio=TF_RATIO,\n",
    "                n_epochs=N_EPOCHS,\n",
    "                grad_clip=GRAD_CLIP,\n",
    "                save_to='./saved_models',\n",
    "                filename='seq2seq-baseline-lstm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(history['loss'], label='train')\n",
    "axes[0].plot(history['val_loss'], label='valid')\n",
    "axes[0].set_title('Loss history')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(history['ppl'], label='train')\n",
    "axes[1].plot(history['val_ppl'], label='valid')\n",
    "axes[1].set_title('Perplexity history')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Perplexity')\n",
    "axes[1].grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.load_state_dict(torch.load('./saved_models/seq2seq-baseline-lstm.pt').get('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/88 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.649 - val_ppl: 104.510:   0%|          | 0/88 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.207 - val_ppl:  24.695:   0%|          | 0/88 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 2.972 - val_ppl:  19.522:   0%|          | 0/88 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 2.913 - val_ppl:  18.415:   0%|          | 0/88 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 2.913 - val_ppl:  18.415:   5%|▍         | 4/88 [00:00<00:02, 33.35it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 2.950 - val_ppl:  19.110:   5%|▍         | 4/88 [00:00<00:02, 33.35it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.048 - val_ppl:  21.080:   5%|▍         | 4/88 [00:00<00:02, 33.35it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.089 - val_ppl:  21.952:   5%|▍         | 4/88 [00:00<00:02, 33.35it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.169 - val_ppl:  23.779:   5%|▍         | 4/88 [00:00<00:02, 33.35it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.217 - val_ppl:  24.959:   5%|▍         | 4/88 [00:00<00:02, 33.35it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.217 - val_ppl:  24.959:  10%|█         | 9/88 [00:00<00:02, 36.83it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.289 - val_ppl:  26.817:  10%|█         | 9/88 [00:00<00:02, 36.83it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.327 - val_ppl:  27.858:  10%|█         | 9/88 [00:00<00:02, 36.83it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.360 - val_ppl:  28.778:  10%|█         | 9/88 [00:00<00:02, 36.83it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.384 - val_ppl:  29.482:  10%|█         | 9/88 [00:00<00:02, 36.83it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.412 - val_ppl:  30.319:  10%|█         | 9/88 [00:00<00:02, 36.83it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.412 - val_ppl:  30.319:  16%|█▌        | 14/88 [00:00<00:01, 39.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.445 - val_ppl:  31.348:  16%|█▌        | 14/88 [00:00<00:01, 39.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.495 - val_ppl:  32.937:  16%|█▌        | 14/88 [00:00<00:01, 39.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.521 - val_ppl:  33.832:  16%|█▌        | 14/88 [00:00<00:01, 39.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.556 - val_ppl:  35.017:  16%|█▌        | 14/88 [00:00<00:01, 39.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.580 - val_ppl:  35.882:  16%|█▌        | 14/88 [00:00<00:01, 39.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.580 - val_ppl:  35.882:  22%|██▏       | 19/88 [00:00<00:01, 39.79it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.600 - val_ppl:  36.595:  22%|██▏       | 19/88 [00:00<00:01, 39.79it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.646 - val_ppl:  38.329:  22%|██▏       | 19/88 [00:00<00:01, 39.79it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.669 - val_ppl:  39.214:  22%|██▏       | 19/88 [00:00<00:01, 39.79it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.703 - val_ppl:  40.552:  22%|██▏       | 19/88 [00:00<00:01, 39.79it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.703 - val_ppl:  40.552:  26%|██▌       | 23/88 [00:00<00:01, 36.76it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.727 - val_ppl:  41.549:  26%|██▌       | 23/88 [00:00<00:01, 36.76it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.747 - val_ppl:  42.389:  26%|██▌       | 23/88 [00:00<00:01, 36.76it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.782 - val_ppl:  43.888:  26%|██▌       | 23/88 [00:00<00:01, 36.76it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.806 - val_ppl:  44.964:  26%|██▌       | 23/88 [00:00<00:01, 36.76it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.806 - val_ppl:  44.964:  31%|███       | 27/88 [00:00<00:01, 34.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.827 - val_ppl:  45.945:  31%|███       | 27/88 [00:00<00:01, 34.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.852 - val_ppl:  47.082:  31%|███       | 27/88 [00:00<00:01, 34.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.868 - val_ppl:  47.828:  31%|███       | 27/88 [00:00<00:01, 34.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.887 - val_ppl:  48.785:  31%|███       | 27/88 [00:00<00:01, 34.45it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.887 - val_ppl:  48.785:  35%|███▌      | 31/88 [00:00<00:01, 33.09it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.896 - val_ppl:  49.213:  35%|███▌      | 31/88 [00:00<00:01, 33.09it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.915 - val_ppl:  50.138:  35%|███▌      | 31/88 [00:00<00:01, 33.09it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.933 - val_ppl:  51.071:  35%|███▌      | 31/88 [00:00<00:01, 33.09it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.948 - val_ppl:  51.810:  35%|███▌      | 31/88 [00:00<00:01, 33.09it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.948 - val_ppl:  51.810:  40%|███▉      | 35/88 [00:00<00:01, 31.69it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.964 - val_ppl:  52.651:  40%|███▉      | 35/88 [00:01<00:01, 31.69it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.980 - val_ppl:  53.510:  40%|███▉      | 35/88 [00:01<00:01, 31.69it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 3.993 - val_ppl:  54.223:  40%|███▉      | 35/88 [00:01<00:01, 31.69it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.003 - val_ppl:  54.760:  40%|███▉      | 35/88 [00:01<00:01, 31.69it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.003 - val_ppl:  54.760:  44%|████▍     | 39/88 [00:01<00:01, 30.84it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.017 - val_ppl:  55.548:  44%|████▍     | 39/88 [00:01<00:01, 30.84it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.031 - val_ppl:  56.299:  44%|████▍     | 39/88 [00:01<00:01, 30.84it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.044 - val_ppl:  57.043:  44%|████▍     | 39/88 [00:01<00:01, 30.84it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.052 - val_ppl:  57.505:  44%|████▍     | 39/88 [00:01<00:01, 30.84it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.052 - val_ppl:  57.505:  49%|████▉     | 43/88 [00:01<00:01, 30.78it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.069 - val_ppl:  58.515:  49%|████▉     | 43/88 [00:01<00:01, 30.78it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.084 - val_ppl:  59.402:  49%|████▉     | 43/88 [00:01<00:01, 30.78it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.095 - val_ppl:  60.027:  49%|████▉     | 43/88 [00:01<00:01, 30.78it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.106 - val_ppl:  60.681:  49%|████▉     | 43/88 [00:01<00:01, 30.78it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.106 - val_ppl:  60.681:  53%|█████▎    | 47/88 [00:01<00:01, 30.27it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.121 - val_ppl:  61.638:  53%|█████▎    | 47/88 [00:01<00:01, 30.27it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.133 - val_ppl:  62.348:  53%|█████▎    | 47/88 [00:01<00:01, 30.27it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.143 - val_ppl:  63.017:  53%|█████▎    | 47/88 [00:01<00:01, 30.27it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.143 - val_ppl:  63.017:  57%|█████▋    | 50/88 [00:01<00:01, 29.58it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.155 - val_ppl:  63.748:  57%|█████▋    | 50/88 [00:01<00:01, 29.58it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.165 - val_ppl:  64.367:  57%|█████▋    | 50/88 [00:01<00:01, 29.58it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.174 - val_ppl:  64.970:  57%|█████▋    | 50/88 [00:01<00:01, 29.58it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.174 - val_ppl:  64.970:  60%|██████    | 53/88 [00:01<00:01, 29.06it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.187 - val_ppl:  65.826:  60%|██████    | 53/88 [00:01<00:01, 29.06it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.204 - val_ppl:  66.978:  60%|██████    | 53/88 [00:01<00:01, 29.06it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.215 - val_ppl:  67.673:  60%|██████    | 53/88 [00:01<00:01, 29.06it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.215 - val_ppl:  67.673:  64%|██████▎   | 56/88 [00:01<00:01, 28.20it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.224 - val_ppl:  68.300:  64%|██████▎   | 56/88 [00:01<00:01, 28.20it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.235 - val_ppl:  69.076:  64%|██████▎   | 56/88 [00:01<00:01, 28.20it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.247 - val_ppl:  69.886:  64%|██████▎   | 56/88 [00:01<00:01, 28.20it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.247 - val_ppl:  69.886:  67%|██████▋   | 59/88 [00:01<00:01, 27.89it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.258 - val_ppl:  70.642:  67%|██████▋   | 59/88 [00:01<00:01, 27.89it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.268 - val_ppl:  71.361:  67%|██████▋   | 59/88 [00:01<00:01, 27.89it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.275 - val_ppl:  71.869:  67%|██████▋   | 59/88 [00:01<00:01, 27.89it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.275 - val_ppl:  71.869:  70%|███████   | 62/88 [00:01<00:00, 26.69it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.288 - val_ppl:  72.818:  70%|███████   | 62/88 [00:01<00:00, 26.69it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.299 - val_ppl:  73.639:  70%|███████   | 62/88 [00:02<00:00, 26.69it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.309 - val_ppl:  74.401:  70%|███████   | 62/88 [00:02<00:00, 26.69it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.309 - val_ppl:  74.401:  74%|███████▍  | 65/88 [00:02<00:00, 26.09it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.319 - val_ppl:  75.090:  74%|███████▍  | 65/88 [00:02<00:00, 26.09it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.327 - val_ppl:  75.699:  74%|███████▍  | 65/88 [00:02<00:00, 26.09it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.336 - val_ppl:  76.408:  74%|███████▍  | 65/88 [00:02<00:00, 26.09it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.336 - val_ppl:  76.408:  77%|███████▋  | 68/88 [00:02<00:00, 25.58it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.342 - val_ppl:  76.898:  77%|███████▋  | 68/88 [00:02<00:00, 25.58it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.351 - val_ppl:  77.569:  77%|███████▋  | 68/88 [00:02<00:00, 25.58it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.359 - val_ppl:  78.174:  77%|███████▋  | 68/88 [00:02<00:00, 25.58it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.359 - val_ppl:  78.174:  81%|████████  | 71/88 [00:02<00:00, 25.42it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.367 - val_ppl:  78.803:  81%|████████  | 71/88 [00:02<00:00, 25.42it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.375 - val_ppl:  79.407:  81%|████████  | 71/88 [00:02<00:00, 25.42it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.382 - val_ppl:  80.029:  81%|████████  | 71/88 [00:02<00:00, 25.42it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.382 - val_ppl:  80.029:  84%|████████▍ | 74/88 [00:02<00:00, 25.08it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.387 - val_ppl:  80.384:  84%|████████▍ | 74/88 [00:02<00:00, 25.08it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.392 - val_ppl:  80.816:  84%|████████▍ | 74/88 [00:02<00:00, 25.08it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.398 - val_ppl:  81.305:  84%|████████▍ | 74/88 [00:02<00:00, 25.08it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.398 - val_ppl:  81.305:  88%|████████▊ | 77/88 [00:02<00:00, 25.00it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.406 - val_ppl:  81.935:  88%|████████▊ | 77/88 [00:02<00:00, 25.00it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.413 - val_ppl:  82.509:  88%|████████▊ | 77/88 [00:02<00:00, 25.00it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.420 - val_ppl:  83.081:  88%|████████▊ | 77/88 [00:02<00:00, 25.00it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.420 - val_ppl:  83.081:  91%|█████████ | 80/88 [00:02<00:00, 24.74it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.426 - val_ppl:  83.636:  91%|█████████ | 80/88 [00:02<00:00, 24.74it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.433 - val_ppl:  84.183:  91%|█████████ | 80/88 [00:02<00:00, 24.74it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.441 - val_ppl:  84.845:  91%|█████████ | 80/88 [00:02<00:00, 24.74it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.441 - val_ppl:  84.845:  94%|█████████▍| 83/88 [00:02<00:00, 24.74it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.447 - val_ppl:  85.359:  94%|█████████▍| 83/88 [00:02<00:00, 24.74it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.452 - val_ppl:  85.835:  94%|█████████▍| 83/88 [00:02<00:00, 24.74it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.460 - val_ppl:  86.501:  94%|█████████▍| 83/88 [00:02<00:00, 24.74it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.460 - val_ppl:  86.501:  98%|█████████▊| 86/88 [00:02<00:00, 24.65it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.467 - val_ppl:  87.072:  98%|█████████▊| 86/88 [00:02<00:00, 24.65it/s]\u001b[A\n",
      "Evaluation on test set - val_loss: 4.472 - val_ppl:  87.490: 100%|██████████| 88/88 [00:03<00:00, 29.29it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_ppl = valid_epoch(model=seq2seq,\n",
    "                                loss_func=criterion,\n",
    "                                data_it=test_iterator,\n",
    "                                epoch_text='Evaluation on test set - ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    \n",
    "    def __init__(self, model, src_field, dest_field, normalizer, max_length, device):\n",
    "        assert callable(normalizer)\n",
    "        \n",
    "        self.model = model\n",
    "        self.src_field = src_field\n",
    "        self.dest_field = dest_field\n",
    "        self.normalizer = normalizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "    \n",
    "    def encoder(self, sentence):\n",
    "        sentence = self.normalizer(sentence)\n",
    "        sentence = self.src_field.preprocess(sentence)\n",
    "        input_sequence, sequence_length = self.src_field.process([sentence])\n",
    "        input_sequence = input_sequence.to(self.device)\n",
    "        sequence_length = sequence_length.to(self.device)\n",
    "        _, h_state, c_state = self.model.encoder(input_sequence, sequence_length)\n",
    "        return _, h_state, c_state\n",
    "    \n",
    "    def greedy(self, sentence):\n",
    "        \"\"\"\n",
    "        Translate using Greedy method\n",
    "        \n",
    "        :param\n",
    "            sentence: str\n",
    "            \n",
    "        :return \n",
    "        \"\"\"\n",
    "        _, h_state, c_state = self.encode(sentence)\n",
    "        outputs, logp = [], 0\n",
    "        word_index = torch.tensor([self.dest_field.vocab.stoi[self.dest_field.init_token]], device=self.device)\n",
    "        for _ in range(self.max_length):\n",
    "            output, h_state, c_state = self.model.decoder(word_index, h_state, c_state)\n",
    "            probas = F.softmax(output, dim=1)\n",
    "            proba, word_index = torch.topk(probas, k=1, dim=1)\n",
    "            word_index = word_index.squeeze(0)\n",
    "            if word_index.detach().item() == self.dest_field.vocab.stoi[self.dest_field.eos_token]:\n",
    "                break\n",
    "            outputs.append(word_index.detach().item())\n",
    "            logp +=  np.log(proba.detach().item())\n",
    "        return ' '.join([*map(self.dest_field.vocab.itos.__getitem__, outputs)]), logp\n",
    "    \n",
    "    def beam_search(self, sentence, beam_size):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Inference(model=seq2seq,\n",
    "                       src_field=FR,\n",
    "                       dest_field=EN,\n",
    "                       normalizer=normalize_string,\n",
    "                       max_length=MAX_LENGTH,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('parliament european parliament', -1.594740785338368)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.greedy('Le parlement européen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je soutiens le rapport klinz afin de contribuer au processus de reforme des agences de notation .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('i welcome the report report the the the the the the of of . . .', -26.211226817460478)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "i support the klinz report in order to advance the process of reforming the rating agencies .\n",
      "====================================================================================================\n",
      "il ne faut pas seulement le proclamer , mais se donner les moyens de le mettre en ? uvre .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('we must not be to but but but but the to to the . .', -25.623090723833517)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "we must not only talk about it we must give ourselves the means to implement it .\n",
      "====================================================================================================\n",
      "il n y a pas eu de position commune avant juin cette annee mais a present , ils s en debarrassent aupres du parlement comme d un dossier brulant .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('there is not been the of of parliament parliament , , , , , , , , , , , this in . . . . . .', -59.92127452165231)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "there has been no common position until june of this year but now they pass it to parliament like a hot potato .\n",
      "====================================================================================================\n",
      "mais si nous nous plaignons de prescriptions trop compliquees , nous ne devons pas omettre d etudier leurs causes plus profondes .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('but we we , , , , , we we we we to to to to to to to . . .', -46.109859728275055)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "if however we complain about excessively complicated regulations we should not lose sight of the deeper reasons for them .\n",
      "====================================================================================================\n",
      "nous avons maintenant une agence europeenne de la securite aerienne qui est d ailleurs reconnue comme ayant l autorite voulue , y compris par la faa americaine .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('we have a a european of the the , , , , , , , , , , , , , , <unk> . .', -53.83375838952148)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "we now have a european aviation safety agency , which is recognised as having the desired authority , including by the us faa .\n",
      "====================================================================================================\n",
      "developper des reseaux de jeunes et d enfants pour qu ils servent de plateformes durables permettant de consulter les enfants ,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('competition for women and and and and and and and and and and and', -28.56613575912405)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "build up existing youth and children s networks as sustainable platforms for consulting children\n",
      "====================================================================================================\n",
      "nous avons besoin d une reforme de notre politique structurelle pour faire en sorte que l ue reste forte .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('we need a common of of policy policy a a eu the the the . .', -32.400797295777814)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "we need reform of our structural policy to ensure that the eu remains strong .\n",
      "====================================================================================================\n",
      "cette strategie ne fait du reste que des gagnants .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('this is is is only the of of . . .', -21.089413441715354)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "it is also a win win strategy .\n",
      "====================================================================================================\n",
      "il est important de se rappeler que la cooperation interetatique intervient entre des gouvernements qui sont controles par les parlements nationaux , lesquels reclament aussi des responsabilites .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('it is important to that the the of the the the the the the the the the the the the . . . . . .', -53.20705762174079)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "it is important to remember that intergovernmental cooperation takes place between governments controlled by the national parliaments , which also demand accountability .\n",
      "====================================================================================================\n",
      "le 8 octobre sera une date decisive . c est en effet a cette date que doit se reunir le prochain conseil affaires generales .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('the year 0 will be be a a a a a the the the the the the the the council council council the council .', -49.443836393036335)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "the defining moment will be on 8 october when the next general affairs council is due to take place .\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.choice(len(test_data.examples), 10):\n",
    "    s = ' '.join(test_data.examples[i].src[::-1])\n",
    "    d = ' '.join(test_data.examples[i].dest)\n",
    "    print(s)\n",
    "    print('-'*100)\n",
    "    print(translator.greedy(s))\n",
    "    print('-'*100)\n",
    "    print(d)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
