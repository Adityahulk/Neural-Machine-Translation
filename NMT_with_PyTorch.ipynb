{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT with PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNB/48tiipf5TpE0HbuRlJk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dksifoua/Neural-Machine-Translation/blob/master/NMT_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Sc0ST76twF",
        "colab_type": "code",
        "outputId": "46134b1d-c42b-4e54-aae5-6dacc58d59ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "!pip install torchtext --upgrade\n",
        "!python -m spacy download fr\n",
        "!python -m spacy download en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torchtext in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext) (0.1.85)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied: fr_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz#egg=fr_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM7QNn0xxCaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import tqdm\n",
        "import random\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import spacy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Example, Field, Dataset\n",
        "from torchtext.data.iterator import BucketIterator\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bidq94KZ36Xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SEED = 781\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXBFwfmcA0LY",
        "colab_type": "code",
        "outputId": "41a836a6-bdee-422e-82fc-eceba6e86ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "if not os.path.exists('./data'):\n",
        "    !mkdir ./data\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    http://www.statmt.org/europarl/v7/fr-en.tgz \\\n",
        "    -O ./data/fr-en.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-17 04:07:17--  http://www.statmt.org/europarl/v7/fr-en.tgz\n",
            "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
            "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 202718517 (193M) [application/x-gzip]\n",
            "Saving to: ‘./data/fr-en.tgz’\n",
            "\n",
            "./data/fr-en.tgz     85%[================>   ] 165.20M  1.18MB/s    eta 20s    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0VvuklFVG-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xzvf ./data/fr-en.tgz -C ./data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95zqRUW_BEH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(filepath):\n",
        "    try:\n",
        "        with open(filepath, mode='rt', encoding='utf-8') as file:\n",
        "            content = file.readlines()\n",
        "        return content\n",
        "    except:\n",
        "        raise NotImplementedError(f'File {filepath} doesn\\'t exist')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwD1a8eHWIB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    # NFD => Normal Form Decompose\n",
        "    # Mn => Non Marking Space\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) \\\n",
        "                    if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z1-9!.?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w1KLG09WgPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "pairs = [*zip(read_file('./data/europarl-v7.fr-en.fr'),\n",
        "             read_file('./data/europarl-v7.fr-en.en'))]\n",
        "pairs = [*map(lambda x: {'fr': x[0], 'en': x[1]}, pairs)]\n",
        "print('Number of examples:', len(pairs))\n",
        "pairs = np.random.choice(pairs, size=30000, replace=False)\n",
        "pairs = [*map(lambda pair: {k: normalize_string(v) for k, v in pair.items()},\n",
        "              pairs)]\n",
        "print('Number of examples after sampling:', len(pairs))\n",
        "print('Example:', pairs[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiIYkYqE5ARI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "FR = Field(init_token='<sos>',\n",
        "           eos_token='<eos>',\n",
        "           pad_token='<pad>',\n",
        "           unk_token='<unk>',\n",
        "           lower=True,\n",
        "           tokenize='spacy',\n",
        "           tokenizer_language='fr',\n",
        "           preprocessing=lambda x: x[::-1])\n",
        "EN = Field(init_token='<sos>',\n",
        "           eos_token='<eos>',\n",
        "           pad_token='<pad>',\n",
        "           unk_token='<unk>',\n",
        "           lower=True,\n",
        "           tokenize='spacy',\n",
        "           tokenizer_language='en')\n",
        "\n",
        "examples = [Example.fromdict(data=pair, fields={'fr': ('src', FR),\n",
        "                                                'en': ('dest', EN)})\n",
        "            for pair in tqdm.tqdm(pairs)]\n",
        "data = Dataset(examples, fields={'src': FR, 'dest': EN})\n",
        "train_data, valid_data, test_data = data.split(split_ratio=[0.7, 0.2, 0.1])\n",
        "print('train size:', len(train_data.examples))\n",
        "print('valid size:', len(valid_data.examples))\n",
        "print('test size:', len(test_data.examples))\n",
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWqpg9Azsp1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FR.build_vocab(train_data, min_freq=5,\n",
        "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
        "EN.build_vocab(train_data, min_freq=5,\n",
        "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nig1u2ebMwVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Length of FR vocabulary:', len(FR.vocab))\n",
        "print('Length of EN vocabulary:', len(EN.vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDaNHsaAMxWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_it, valid_it, test_it = BucketIterator.splits((train_data, valid_data,\n",
        "                                                     test_data),\n",
        "                                                    batch_size=BATCH_SIZE,\n",
        "                                                    device=DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17GWbzdnPZ4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim,\n",
        "                 hidden_units, n_layers, dropout, bi=True):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_units,\n",
        "                            num_layers=n_layers,\n",
        "                            bidirectional=bi,\n",
        "                            dropout=dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        embedded = self.dropout(embedded)\n",
        "        outputs, (h_state, c_state) = self.lstm(embedded)\n",
        "        d, _, _ = h_state.size()\n",
        "        h_state = h_state[:d//2, :, :] + h_state[d//2:, :, :]\n",
        "        c_state = c_state[:d//2, :, :] + c_state[d//2:, :, :]\n",
        "        return outputs, (h_state, c_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN5JwPAvIGSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder = Encoder(vocab_size=len(FR.vocab), embedding_dim=300,\n",
        "#                   hidden_units=128, n_layers=4, dropout=0.25)\n",
        "# encoder.to(DEVICE)\n",
        "# for batch in train_it:\n",
        "#     print('inputs shape', batch.src.shape)\n",
        "#     outputs, (h_state, c_state) = encoder(batch.src)\n",
        "#     print('outputs shape:', outputs.shape)\n",
        "#     print('h_state shape:', h_state.shape)\n",
        "#     print('c_state shape:', c_state.shape)\n",
        "#     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_wAlelsKC6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim,\n",
        "                 hidden_units, n_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_units,\n",
        "                            num_layers=n_layers,\n",
        "                            dropout=dropout)\n",
        "        self.linear = nn.Linear(hidden_units, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, h_state, c_state):\n",
        "        embedded = self.embedding(inputs)\n",
        "        embedded = self.dropout(embedded)\n",
        "        outputs, (h_state, c_state) = self.lstm(embedded, (h_state, c_state))\n",
        "        logits = self.linear(outputs.squeeze(0))\n",
        "        return logits, (h_state, c_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2uDv0AlBNHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decoder = Decoder(vocab_size=len(EN.vocab), embedding_dim=300,\n",
        "#                   hidden_units=128, n_layers=4, dropout=0.25)\n",
        "# decoder.to(DEVICE)\n",
        "# for i, batch in enumerate(train_it):\n",
        "#     print('inputs shape', batch.dest[i].unsqueeze(0).shape)\n",
        "#     outputs, (h_state, c_state) = decoder(batch.dest[i].unsqueeze(0), h_state, c_state)\n",
        "#     print('outputs shape:', outputs.shape)\n",
        "#     print('h_state shape:', h_state.shape)\n",
        "#     print('c_state shape:', c_state.shape)\n",
        "#     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK1uCzTUMubJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeqToSeqNet(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(SeqToSeqNet, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        enc_out, (h_state, c_state) = self.encoder(inputs)\n",
        "        target = targets[0, :].unsqueeze(0)\n",
        "        logits = []\n",
        "        for t in range(1, targets.size(0)):\n",
        "            logit, (h_state, c_state) = self.decoder(target, h_state, c_state)\n",
        "            target = logit.argmax(1).unsqueeze(0)\n",
        "            logits.append(logit)\n",
        "        return torch.stack(logits, dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKS2vScrL_oQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = SeqToSeqNet(encoder, decoder)\n",
        "# model.to(DEVICE)\n",
        "# for i, batch in enumerate(train_it):\n",
        "#     print('inputs shape', batch.dest.shape)\n",
        "#     outputs = model(batch.src, batch.dest)\n",
        "#     print('outputs shape:', outputs.shape)\n",
        "#     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctoLP8JzBi11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(model: nn.Module):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.uniform_(param.data, a=-0.08, b=0.08)\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRkzCVrnQCB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    grad_mean, layers = [], []\n",
        "    for name, param in named_parameters:\n",
        "        if param.requires_grad and 'bias' not in name:\n",
        "            layers.append(name)\n",
        "            grad_mean.append(param.grad.abs().mean())\n",
        "    plt.plot(grad_mean, alpha=0.3, color='b')\n",
        "    plt.hlines(0, 0, len(grad_mean) + 1, linewidth=1, color='k' )\n",
        "    plt.xticks(range(0, len(grad_mean), 1), layers, rotation='vertical')\n",
        "    plt.xlim(xmin=0, xmax=len(grad_mean))\n",
        "    plt.ylim(bottom=-0.001, top=0.02)\n",
        "    plt.xlabel('Layers')\n",
        "    plt.ylabel('Mean of gradients')\n",
        "    plt.title('Gradient Flow')\n",
        "    plt.grid(True)\n",
        "\n",
        "def train_step(model, opt, loss_func, data_it, grad_clip, epoch_text=''):\n",
        "    epoch_loss = 0.\n",
        "    pbar = tqdm.tqdm_notebook(enumerate(data_it), total=len(data_it))\n",
        "    model.train()\n",
        "    for i, data in pbar:\n",
        "        opt.zero_grad()\n",
        "        logits = model(data.src, data.dest)\n",
        "        d = logits.size(-1)\n",
        "        loss = loss_func(logits.view(-1, d), data.dest[1:, :].view(-1))\n",
        "        loss.backward()\n",
        "        plot_grad_flow(model.named_parameters())\n",
        "        if grad_clip:\n",
        "            nn.utils.clip_grad_norm(model.parameters(), grad_clip)\n",
        "        opt.step()\n",
        "        epoch_loss += loss.item()\n",
        "        pbar.set_description(epoch_text + f'Train Loss: {epoch_loss/(i+1):.3f}')\n",
        "    plt.show() # Show the gradient flow\n",
        "    return epoch_loss / len(data_it)\n",
        "\n",
        "def evaluate(model, loss_func, data_it):\n",
        "    epoch_loss = 0.\n",
        "    pbar = tqdm.tqdm_notebook(enumerate(data_it), total=len(data_it))\n",
        "    model.eval()\n",
        "    for i, data in pbar:\n",
        "        with torch.no_grad():\n",
        "            logits = model(data.src, data.dest)\n",
        "            d = logits.size(-1)\n",
        "            loss = loss_func(logits.view(-1, d), data.dest[1:, :].view(-1))\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_description(f'Valid Loss: {epoch_loss / (i + 1):.3f}')\n",
        "    return epoch_loss / len(data_it)\n",
        "\n",
        "def train(model, optimizer, loss_function, train_it, valid_it, n_epochs,\n",
        "          grad_clip=None, save_to='./saved_models', filename='seq2seq.pt'):\n",
        "    assert callable(loss_function)\n",
        "    if not os.path.exists(save_to):\n",
        "        !mkdir {save_to}\n",
        "\n",
        "    history = {'loss': [], 'val_loss': []}\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_text = f'Epoch: {epoch + 1:02d} - '\n",
        "        loss = train_step(model, optimizer, loss_function,\n",
        "                          train_it, grad_clip, epoch_text)\n",
        "        val_loss = evaluate(model, loss_function, valid_it)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            torch.save({'model': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict()},\n",
        "                       f=f'./saved_model/{os.path.join(save_to, filename)}')\n",
        "\n",
        "        history['loss'].append(loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "    return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2wIgawGgaXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_FR = len(FR.vocab)\n",
        "VOCAB_SIZE_EN = len(EN.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_UNITS = 512\n",
        "N_LAYERS = 1\n",
        "DROPOUT = 0\n",
        "LR = 1e-3\n",
        "N_EPOCHS = 5\n",
        "\n",
        "encoder = Encoder(vocab_size=VOCAB_SIZE_FR,\n",
        "                  embedding_dim=EMBEDDING_DIM,\n",
        "                  hidden_units=HIDDEN_UNITS,\n",
        "                  n_layers=N_LAYERS,\n",
        "                  dropout=DROPOUT)\n",
        "decoder = Decoder(vocab_size=VOCAB_SIZE_EN,\n",
        "                  embedding_dim=EMBEDDING_DIM,\n",
        "                  hidden_units=HIDDEN_UNITS,\n",
        "                  n_layers=N_LAYERS,\n",
        "                  dropout=DROPOUT)\n",
        "torch.cuda.reset_max_memory_allocated(device=DEVICE)\n",
        "torch.cuda.reset_max_memory_cached(device=DEVICE)\n",
        "model = SeqToSeqNet(encoder, decoder)\n",
        "model.to(DEVICE)\n",
        "model.apply(init_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=EN.vocab.stoi[EN.pad_token])\n",
        "\n",
        "print(model)\n",
        "print('Number of parameters of the model:', count_parameters(model))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-woYQDxsCJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = train(model, optimizer, criterion, train_it, valid_it, N_EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGI8KZyYSl1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.memory_reserved(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQcnVt4aCkRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history['loss'], label='train')\n",
        "plt.plot(history['val_loss'], label='valid')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7EUXJHwEM41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('./saved_model/seq2seq.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfbKli0hIPLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}