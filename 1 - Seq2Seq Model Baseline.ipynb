{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wEd7p3ARFcyr"
   },
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gttqmxRIFSUa",
    "outputId": "a57491fa-3286-4201-9bc3-22e222adc11a"
   },
   "outputs": [],
   "source": [
    "!pip install torchtext --upgrade > /dev/null 2>&1\n",
    "!python -m spacy download fr > /dev/null 2>&1\n",
    "!python -m spacy download en > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-A0mVf7GNix"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Example, Field, Dataset\n",
    "from torchtext.data.iterator import BucketIterator\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0iX1ZuNG0wH"
   },
   "outputs": [],
   "source": [
    "seed = 781\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2BPfwqcFk4h"
   },
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gc5EcEA1FnCw"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./data'):\n",
    "    !mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "wmajgrxCHwsw",
    "outputId": "abcd63e9-f6f2-4eab-bb4e-72616f57de64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-25 22:47:40--  http://www.statmt.org/europarl/v7/fr-en.tgz\n",
      "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
      "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 202718517 (193M) [application/x-gzip]\n",
      "Saving to: ‘./data/fr-en.tgz’\n",
      "\n",
      "./data/fr-en.tgz    100%[===================>] 193.33M   131KB/s    in 25m 59s \n",
      "\n",
      "2020-03-25 23:13:38 (127 KB/s) - ‘./data/fr-en.tgz’ saved [202718517/202718517]\n",
      "\n",
      "CPU times: user 25 s, sys: 7.49 s, total: 32.5 s\n",
      "Wall time: 25min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wget --no-check-certificate \\\n",
    "    http://www.statmt.org/europarl/v7/fr-en.tgz \\\n",
    "    -O ./data/fr-en.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "M2r79GZZH7Ip",
    "outputId": "838d4e4c-217d-4c93-afaf-1d4e2a22335d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europarl-v7.fr-en.en\n",
      "europarl-v7.fr-en.fr\n",
      "CPU times: user 108 ms, sys: 20 ms, total: 128 ms\n",
      "Wall time: 5.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!tar -xzvf ./data/fr-en.tgz -C ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tazMbPR6Hnjg"
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95Q9N_zBHpTr"
   },
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, mode='rt', encoding='utf-8') as file:\n",
    "            content = file.readlines()\n",
    "        return content\n",
    "    except:\n",
    "        raise NotImplementedError(f'File {filepath} doesn\\'t exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02KzJaPmIjI2"
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    # NFD => Normal Form Decompose\n",
    "    # Mn => Non Marking Space\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) \\\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    # Transform accented characters into unaccented ones\n",
    "    s = unicode_to_ascii(s.strip())\n",
    "    # Remove a sequence of whitespace characters\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "wlJtKDpMJF-1",
    "outputId": "27290cb2-4a77-4200-b790-819142f587c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2,007,723\n",
      "CPU times: user 3.26 s, sys: 1 s, total: 4.26 s\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairs = [*zip(read_file('./data/europarl-v7.fr-en.fr'),\n",
    "              read_file('./data/europarl-v7.fr-en.en'))]\n",
    "pairs = [*map(lambda x: {'fr': x[0], 'en': x[1]}, pairs)]\n",
    "print(f'Number of examples: {len(pairs):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not build models the entire dataset, since is very large. Instead, I sample a subset of 30,000 sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples after sampling: 30,000\n",
      "Example:\n",
      "\tFR => Les procedures par le biais desquelles de tels produits entrent et sortent de l'Union europeenne doivent etre ouvertes, transparentes et, par dessus tout, sures.\n",
      "\tEN => The procedures whereby such products come in and out of the European Union have to be open, transparent and, above all, safe.\n",
      "CPU times: user 4.4 s, sys: 84 ms, total: 4.48 s\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairs = np.random.choice(pairs, size=30000, replace=False)\n",
    "pairs = [*map(lambda pair: {k: normalize_string(v) for k, v in pair.items()},\n",
    "              pairs)]\n",
    "print(f'Number of examples after sampling: {len(pairs):,}')\n",
    "print(f'Example:\\n\\tFR => {pairs[0][\"fr\"]}\\n\\tEN => {pairs[0][\"en\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I build the train/valid/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "BAq2uyDyJ6KD",
    "outputId": "64369eb2-8c31-44bf-ecbb-abb0de8122d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:58<00:00, 512.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 540 ms, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FR = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           preprocessing=lambda x: x[::-1],\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='fr',\n",
    "           include_lengths=True) # For pack_padded_sequence\n",
    "EN = Field(init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='en')\n",
    "\n",
    "examples = [Example.fromdict(data=pair, fields={'fr': ('src', FR),\n",
    "                                                'en': ('dest', EN)})\n",
    "            for pair in tqdm.tqdm(pairs)]\n",
    "data = Dataset(examples, fields={'src': FR, 'dest': EN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 27000\n",
      "valid size: 1500\n",
      "test size: 1500\n",
      "{'src': ['.', 'europeenne', 'communaute', 'la', 'de', 'niveau', 'au', 'micro-gestion', 'une', 'a', 'proceder', 'de', 'tentation', 'la', 'a', 'resister', 'de', 'que', 'ainsi', ',', 'terrain', 'de', 'acteurs', 'les', 'par', 'fournies', 'etre', 'peuvent', 'qui', 'competences', 'des', 'et', 'connaissances', 'des', 'ampleur', \"l'\", 'reconnaitre', 'de', 'important', 'est', 'il', 'et', ',', 'propre', 'specificite', 'sa', 'possede', 'europeennes', 'mers', 'des', 'chacune'], 'dest': ['each', 'of', 'the', 'seas', 'in', 'europe', 'has', 'its', 'own', 'specificity', ',', 'and', 'it', 'is', 'important', 'to', 'recognise', 'the', 'level', 'of', 'knowledge', 'and', 'expertise', 'that', 'can', 'be', 'provided', 'by', 'the', 'stakeholders', 'on', 'the', 'ground', 'and', 'to', 'resist', 'the', 'temptation', 'to', 'micro', '-', 'manage', 'on', 'an', 'eu', 'level', '.']}\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = data.split(split_ratio=[0.9, 0.05, 0.05])\n",
    "print(f'train size: {len(train_data.examples)}')\n",
    "print(f'valid size: {len(valid_data.examples)}')\n",
    "print(f'test size: {len(test_data.examples)}')\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model understands only number, we need to transform text sequences into sequence of numbers where each numbers represents an unique word. To do this, we build a vocabulary for each language that map words to indexes and vice versa. the vocabulary id built from train set only in order to prevent data leakage. We also add some special tokens:\n",
    "- `<sos>`: for start of sentence.\n",
    "- `<unk>`: for unknown or less frequent words.\n",
    "- `<eos>`: for end of sentence. \n",
    "- `<pad>`: for padding (make all sentences in a batch the same size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "JycVMjsRLmoB",
    "outputId": "1e833ee2-cf9e-47e2-fc21-1b06f9be07f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of FR vocabulary: 14508\n",
      "Length of EN vocabulary: 11499\n",
      "CPU times: user 492 ms, sys: 0 ns, total: 492 ms\n",
      "Wall time: 491 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FR.build_vocab(train_data,\n",
    "               min_freq=2,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "EN.build_vocab(train_data,\n",
    "               min_freq=2,\n",
    "               specials=['<sos>', '<eos>', '<unk>', '<pad>'])\n",
    "\n",
    "print(f'Length of FR vocabulary: {len(FR.vocab)}')\n",
    "print(f'Length of EN vocabulary: {len(EN.vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKKRGB9cIbn8"
   },
   "source": [
    "# Modeling\n",
    "\n",
    "We used a neural and probabilistic framework to generate english translations of french sentences. The goal is to maximizing the likelihood of a generated english translate given a french sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yT-6GZgfMXIu"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The part of model map th source sequence to hidden vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJL7MVwAMVtR"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size,\n",
    "                 n_layers=1, dropout=0, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=(dropout if n_layers > 1 else 0),\n",
    "                            bidirectional=bidirectional)\n",
    "    \n",
    "    def forward(self, in_, seq_len):\n",
    "        \"\"\"\n",
    "        inputs\n",
    "            in_: (seq_len, batch_size)\n",
    "            seq_len: (batch_size)\n",
    "\n",
    "        outputs\n",
    "            out: (seq_len, batch_size, num_directions * hidden_size)\n",
    "            hn: (num_layers * num_directions, batch_size, hidden_size)\n",
    "            cn: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(in_)\n",
    "        embedded = self.dropout(embedded)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, seq_len)\n",
    "        out, (hn, cn) = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out)\n",
    "        return out, hn, cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cFeoEJpMYgE"
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The part of model performs language modeling given the hidden vector outputs by the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "deAfHkCcIdzj"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size,\n",
    "                 n_layers=1, dropout=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=(dropout if n_layers > 1 else 0))\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, in_, h0, c0):\n",
    "        \"\"\"\n",
    "        inputs\n",
    "            in_: (1, batch_size) => seq_len = 1, a word\n",
    "            h0: (num_layers, batch_size, hidden_size)\n",
    "            c0: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        embed = self.embedding(_in) \n",
    "        # embedded: (1, batch_size, embed_size)\n",
    "        out, hn, cn = self.lstm(embedded)\n",
    "        # out: (1, batch_size, hidden_size)\n",
    "        # hn: (num_layers, batch_size, hidden_size)\n",
    "        # cn: (num_layers, batch_size, hidden_size)\n",
    "        logit = self.fc(out.squeeze(0))\n",
    "        # logit: (batch_size, vocab_size)\n",
    "\n",
    "        outputs: logit, hn, cn\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(in_)\n",
    "        embedded = self.dropout(embedded)\n",
    "        out, (hn, cn) = self.lstm(embedded, (h0, c0))\n",
    "        logit = self.fc(out.squeeze(0))\n",
    "        return logit, hn, cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZwOJLkTMbCt"
   },
   "source": [
    "## Sequence to sequence model\n",
    "\n",
    "This puts encoder and decoder together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5Wd0iNaMgp1"
   },
   "outputs": [],
   "source": [
    "class SeqToSeqNet(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, device=device):\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "        'Encoder and Decoder have to have the same number of reccurent layers'\n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "        'Encoder and Decoder have to have the same number of reccurent hidden units'\n",
    "\n",
    "        super(SeqToSeqNet, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def encode(self, in_, seq_len):\n",
    "        _, hn, cn = self.encoder(in_, seq_len)\n",
    "        # Sum the two directional encoder hn state\n",
    "        if self.encoder.bidirectional:\n",
    "            hn = hn[:self.encoder.n_layers, :, :] + \\\n",
    "                    hn[self.encoder.n_layers:, :, :]\n",
    "            cn = cn[:self.encoder.n_layers, :, :] + \\\n",
    "                    cn[self.encoder.n_layers:, :, :]\n",
    "        return hn, cn\n",
    "\n",
    "    def decode(self, h_state, c_state, target, sos_index, teacher_forcing, ratio):\n",
    "        target_len, batch_size = target.size()\n",
    "        out = torch.zeros((target_len, batch_size, self.decoder.vocab_size),\n",
    "                           device=self.device)\n",
    "        in_ = target[0, :].unsqueeze(0)\n",
    "        for t in range(1, target_len):\n",
    "            logit, h_state, c_state = self.decoder(in_, h_state, c_state)\n",
    "            out[t] = logit # (batch_size, vocab_size)\n",
    "            if teacher_forcing and random.random() < ratio:\n",
    "                in_ = logit.argmax(1).unsqueeze(0) # (1, batch_size)\n",
    "            else:\n",
    "                in_ = target[t, :].unsqueeze(0)\n",
    "        return out\n",
    "\n",
    "    def forward(self, in_, seq_len, target, sos_index,\n",
    "                teacher_forcing=True, ratio=.5):\n",
    "        \"\"\"\n",
    "        inputs\n",
    "            in_: (seq_len, batch_size)\n",
    "            seq_len: (batch_size)\n",
    "            target: (seq_len, batch_size)\n",
    "            sos_index: int\n",
    "            eos_index: int\n",
    "\n",
    "        outputs\n",
    "            out: (seq_len, batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        hn, cn = self.encode(in_, seq_len)\n",
    "        out = self.decode(hn, cn, target, sos_index, teacher_forcing, ratio)\n",
    "        return out\n",
    "\n",
    "    def infer(self, h_state, c_state, sos_index, eos_index, max_len, sample):\n",
    "        \"\"\"\n",
    "        Infer a sequence. Not a batch.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        in_ = torch.ones((1, 1), device=self.device,\n",
    "                         dtype=torch.int64) * sos_index\n",
    "        for _ in range(max_len):\n",
    "            logit, h_state, c_state = self.decoder(in_, h_state, c_state)\n",
    "            if sample:\n",
    "                probs = F.softmax(logit, dim=1) # (1, vocab_size)\n",
    "                next_ = torch.multinomial(probs, 1) # (1, 1)\n",
    "            else:\n",
    "                in_, next_ = logit.topk(1, dim=1) # (batch_size=1, 1)\n",
    "            next_idx = next_.squeeze().cpu().item()\n",
    "            if next_idx == eos_index:\n",
    "                break\n",
    "            out.append(next_idx)\n",
    "            in_ = torch.ones((1, 1), device=self.device,\n",
    "                             dtype=torch.int64) * next_idx # (1, 1)\n",
    "        return out\n",
    "\n",
    "    def inference(self, in_, seq_len, sos_index, eos_index, max_len, sample=True):\n",
    "        \"\"\"\n",
    "        Inferring a sentence or a batch of sentences.\n",
    "        inputs\n",
    "            in_: (seq_len, batch_size)\n",
    "            seq_len: (batch_size)\n",
    "            sos_index: int\n",
    "            eos_index: int\n",
    "            max_len: int\n",
    "            sample: bool\n",
    "\n",
    "        outputs\n",
    "            out: list(seq_len*, batch_size)\n",
    "        \"\"\"\n",
    "        hn, cn = self.encode(in_, seq_len)\n",
    "        # hn: (num_layers, batch_size, hidden_size)\n",
    "        # cn: (num_layers, batch_size, hidden_size)\n",
    "        _, batch_size = in_.size()\n",
    "        out = []\n",
    "        for i in range(batch_size):\n",
    "            out.append(\n",
    "                self.infer(hn[:, i, :].unsqueeze(1),\n",
    "                           cn[:, i, :].unsqueeze(1),\n",
    "                           sos_index, eos_index, max_len, sample))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCLUmCfFMjGV"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_aldUUdZ5dv"
   },
   "outputs": [],
   "source": [
    "def init_weights(model: nn.Module):\n",
    "    for name, param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data, a=-0.08, b=0.08)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J1teOdv5q7ns"
   },
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    grad_mean, layers = [], []\n",
    "    for name, param in named_parameters:\n",
    "        if param.requires_grad and 'bias' not in name:\n",
    "            layers.append(name)\n",
    "            grad_mean.append(param.grad.abs().mean())\n",
    "    plt.plot(grad_mean, alpha=0.3, color='b')\n",
    "    plt.hlines(0, 0, len(grad_mean) + 1, linewidth=1, color='k' )\n",
    "    plt.xticks(range(0, len(grad_mean), 1), layers, rotation='vertical')\n",
    "    plt.xlim(xmin=0, xmax=len(grad_mean))\n",
    "    # plt.ylim(bottom=-0.001, top=0.02) # Zoom on lower gradients\n",
    "    plt.xlabel('Layers')\n",
    "    plt.ylabel('Mean of gradients')\n",
    "    plt.title('Gradient Flow')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8QgblulqoWT"
   },
   "outputs": [],
   "source": [
    "def train_step(model, opt, loss_func, data_it, grad_clip, sos_index,\n",
    "               epoch_text=''):\n",
    "    epoch_loss = 0.\n",
    "    pbar = tqdm.tqdm(enumerate(data_it), total=len(data_it))\n",
    "    model.train()\n",
    "    for i, data in pbar:\n",
    "        opt.zero_grad()\n",
    "        logits = model(*data.src, data.dest, sos_index)\n",
    "        # *data.src: unpack in_ and seq_len\n",
    "        loss = loss_func(logits[1:].view(-1, logits.size(-1)),\n",
    "                         data.dest[1:].view(-1))\n",
    "        loss.backward()\n",
    "        # plot_grad_flow(model.named_parameters())\n",
    "        if grad_clip:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_description(epoch_text + f'Train Loss: {epoch_loss/(i+1):.3f}')\n",
    "    # plt.show() # Show the gradient flow\n",
    "    return epoch_loss / len(data_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYND0IGKrdJu"
   },
   "outputs": [],
   "source": [
    "def valid_step(model, loss_func, data_it, sos_index, epoch_text=''):\n",
    "    epoch_loss = 0.\n",
    "    pbar = tqdm.tqdm(enumerate(data_it), total=len(data_it))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in pbar:\n",
    "            logits = model(*data.src, data.dest, sos_index,\n",
    "                           teacher_forcing=False)\n",
    "            loss = loss_func(logits[1:].view(-1, logits.size(-1)),\n",
    "                             data.dest[1:].view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_description(epoch_text + f'Valid Loss: {epoch_loss/(i+1):.3f}')\n",
    "    return epoch_loss / len(data_it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ECEXh0oHwSFN"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_function, train_it, valid_it, n_epochs, sos_index,\n",
    "          grad_clip=None, save_to='./saved_models', filename='seq2seq-baseline.pt'):\n",
    "    assert callable(loss_function)\n",
    "    if not os.path.exists(save_to):\n",
    "        !mkdir {save_to}\n",
    "\n",
    "    history = {'loss': [], 'val_loss': []}\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_text = f'Epoch: {epoch + 1:02d} - '\n",
    "        loss = train_step(model, optimizer, loss_function, train_it, grad_clip,\n",
    "                          sos_index, epoch_text)\n",
    "        val_loss = valid_step(model, loss_function, valid_it, sos_index,\n",
    "                              epoch_text)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict()},\n",
    "                       f=os.path.join(save_to, filename))\n",
    "\n",
    "        history['loss'].append(loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZV_iUlh3CWo"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_SIZE = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "LR = 1e-3\n",
    "GRAD_CLIP = 1.0\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 30\n",
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBekgFKP3C3S"
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator =  \\\n",
    "        BucketIterator.splits((train_data, valid_data,\n",
    "                               test_data),\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              sort_key=lambda x: len(x.src),\n",
    "                              sort_within_batch=True, # For pack_padded_sequence\n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "aCWVfRdAxY6N",
    "outputId": "1d3a0be8-26c7-404c-ba6c-b1722e7d871d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model: 27,103,199\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(embed_size=EMBEDDING_DIM,\n",
    "                  vocab_size=len(FR.vocab),\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  n_layers=N_LAYERS,\n",
    "                  dropout=DROPOUT).to(device)\n",
    "decoder = Decoder(embed_size=EMBEDDING_DIM,\n",
    "                  vocab_size=len(EN.vocab),\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  n_layers=N_LAYERS,\n",
    "                  dropout=DROPOUT).to(device)\n",
    "seq2seq = SeqToSeqNet(encoder=encoder, decoder=decoder).to(device)\n",
    "seq2seq.apply(init_weights)\n",
    "optimizer = optim.Adam(seq2seq.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=EN.vocab.stoi[EN.pad_token])\n",
    "print(f'Number of parameters of the model: {count_parameters(seq2seq):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "R22m0tls06BS",
    "outputId": "d6f9779c-82ff-4e0b-9a4b-e8fdffe98111"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 01 - Train Loss: 6.134:  93%|█████████▎| 196/211 [02:36<00:12,  1.20it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train(seq2seq, optimizer, criterion, train_iterator, valid_iterator,\n",
    "                sos_index=EN.vocab.stoi[EN.init_token],\n",
    "                n_epochs=N_EPOCHS, grad_clip=GRAD_CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "PQ_klP1-6XyF",
    "outputId": "594ec77b-331d-4a6e-87eb-3da5088583a8"
   },
   "outputs": [],
   "source": [
    "plt.plot(history['loss'], label='train')\n",
    "plt.plot(history['val_loss'], label='valid')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DxDV_RX866Hp",
    "outputId": "6448b84f-af1d-4bc1-e7c7-7dc8cebaaae2"
   },
   "outputs": [],
   "source": [
    "seq2seq.load_state_dict(torch.load('./saved_models/seq2seq.pt').get('model'))\n",
    "test_loss = valid_step(seq2seq, criterion, test_iterator,\n",
    "                       sos_index=EN.vocab.stoi[EN.init_token],\n",
    "                       epoch_text='Test loss => ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kC8tYcPCLgz5"
   },
   "source": [
    "# Inference & BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "bCe-iET5mwf3",
    "outputId": "1d3ff501-f297-478b-f0a2-32e5957f637b"
   },
   "outputs": [],
   "source": [
    "help(SeqToSeqNet.inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "RdRIlUJjLeDj",
    "outputId": "97427a7c-8638-45cf-b7fd-a4a1f63b3cd4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "references = []\n",
    "targets_with_sample = []\n",
    "targets_without_sample  = []\n",
    "seq2seq.eval()\n",
    "with torch.no_grad():\n",
    "    for i, example in enumerate(test_data.examples):\n",
    "        in_ = torch.tensor([FR.vocab.stoi[token]\n",
    "                            for token in example.src],\n",
    "                        dtype=torch.int64, device=device).unsqueeze(1)\n",
    "        if in_.size(0) == 0:\n",
    "            print(f'skipped example {i}!')\n",
    "            continue\n",
    "        seq_len = torch.tensor([in_.size(0)], dtype=torch.int64, device=device)\n",
    "        with_ = seq2seq.inference(in_, seq_len, EN.vocab.stoi[EN.init_token],\n",
    "                                  EN.vocab.stoi[EN.eos_token], MAX_LEN)\n",
    "        without = seq2seq.inference(in_, seq_len,  EN.vocab.stoi[EN.init_token],\n",
    "                                    EN.vocab.stoi[EN.eos_token], MAX_LEN, False)\n",
    "        targets_with_sample.append([EN.vocab.itos[int(idx)] for idx in with_[0]])\n",
    "        targets_without_sample.append([EN.vocab.itos[int(idx)] for idx in without[0]])\n",
    "        references.append([example.dest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "yTolzzQaoAGd",
    "outputId": "aab5bd3b-b676-4d19-a4e5-e3fd153d5eec"
   },
   "outputs": [],
   "source": [
    "print(f'BLEU score with sampling: {bleu_score(targets_with_sample, references)}')\n",
    "print(f'BLEU score without sampling: {bleu_score(targets_without_sample, references)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "JV-mzrtbp_mp",
    "outputId": "fbbb9970-0adb-4d4f-b126-f16868147421"
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(' '.join(references[idx][0]))\n",
    "print('================================')\n",
    "print(' '.join(targets_with_sample[idx]))\n",
    "print('================================')\n",
    "print(' '.join(targets_without_sample[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hzyFn-Vqn3S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wEd7p3ARFcyr",
    "v2BPfwqcFk4h",
    "tazMbPR6Hnjg",
    "yT-6GZgfMXIu",
    "7cFeoEJpMYgE",
    "2ZwOJLkTMbCt"
   ],
   "name": "1 - Sequence to Sequence Model with RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
